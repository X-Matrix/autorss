{
  "title": "Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting",
  "link": "https://arxiv.org/abs/2602.17634v1",
  "published": "2026-02-19T18:48:08Z",
  "updated": "2026-02-19T18:48:08Z",
  "summary": "Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.",
  "id": "http://arxiv.org/abs/2602.17634v1",
  "authors": [
    "Xinghong Fu",
    "Yanhong Li",
    "Georgios Papaioannou",
    "Yoon Kim"
  ],
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.17634v1"
}