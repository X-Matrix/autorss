{
  "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
  "link": "https://arxiv.org/abs/2602.17598v1",
  "published": "2026-02-19T18:22:39Z",
  "updated": "2026-02-19T18:22:39Z",
  "summary": "Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($Îº{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.",
  "id": "http://arxiv.org/abs/2602.17598v1",
  "authors": [
    "Jayadev Billa"
  ],
  "categories": [
    "cs.CL",
    "cs.AI",
    "eess.AS"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.17598v1"
}