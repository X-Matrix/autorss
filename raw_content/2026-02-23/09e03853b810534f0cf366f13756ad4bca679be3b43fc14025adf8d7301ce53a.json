{
  "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees",
  "link": "https://arxiv.org/abs/2602.20065v1",
  "published": "2026-02-23T17:22:46Z",
  "updated": "2026-02-23T17:22:46Z",
  "summary": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.",
  "id": "http://arxiv.org/abs/2602.20065v1",
  "authors": [
    "Natalia Moskvina",
    "Raquel Montero",
    "Masaya Yoshida",
    "Ferdy Hubers",
    "Paolo Morosi",
    "Walid Irhaymi",
    "Jin Yan",
    "Tamara Serrano",
    "Elena Pagliarini",
    "Fritz GÃ¼nther",
    "Evelina Leivada"
  ],
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.20065v1"
}