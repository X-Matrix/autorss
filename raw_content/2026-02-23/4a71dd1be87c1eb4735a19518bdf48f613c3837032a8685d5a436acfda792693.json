{
  "title": "StyleStream: Real-Time Zero-Shot Voice Style Conversion",
  "link": "https://arxiv.org/abs/2602.20113v1",
  "published": "2026-02-23T18:32:59Z",
  "updated": "2026-02-23T18:32:59Z",
  "summary": "Voice style conversion aims to transform an input utterance to match a target speaker's timbre, accent, and emotion, with a central challenge being the disentanglement of linguistic content from style. While prior work has explored this problem, conversion quality remains limited, and real-time voice style conversion has not been addressed. We propose StyleStream, the first streamable zero-shot voice style conversion system that achieves state-of-the-art performance. StyleStream consists of two components: a Destylizer, which removes style attributes while preserving linguistic content, and a Stylizer, a diffusion transformer (DiT) that reintroduces target style conditioned on reference speech. Robust content-style disentanglement is enforced through text supervision and a highly constrained information bottleneck. This design enables a fully non-autoregressive architecture, achieving real-time voice style conversion with an end-to-end latency of 1 second. Samples and real-time demo: https://berkeley-speech-group.github.io/StyleStream/.",
  "id": "http://arxiv.org/abs/2602.20113v1",
  "authors": [
    "Yisi Liu",
    "Nicholas Lee",
    "Gopala Anumanchipalli"
  ],
  "categories": [
    "cs.SD",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.20113v1"
}