{
  "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming",
  "link": "https://arxiv.org/abs/2602.19948v1",
  "published": "2026-02-23T15:17:18Z",
  "updated": "2026-02-23T15:17:18Z",
  "summary": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes.   Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the \"black box\" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment.",
  "id": "http://arxiv.org/abs/2602.19948v1",
  "authors": [
    "Ian Steenstra",
    "Paola Pedrelli",
    "Weiyan Shi",
    "Stacy Marsella",
    "Timothy W. Bickmore"
  ],
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.CY",
    "cs.HC",
    "cs.MA"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.19948v1"
}