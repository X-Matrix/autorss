{
  "title": "Are LLMs Ready to Replace Bangla Annotators?",
  "link": "https://arxiv.org/abs/2602.16241v1",
  "published": "2026-02-18T07:36:41Z",
  "updated": "2026-02-18T07:36:41Z",
  "summary": "Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.",
  "id": "http://arxiv.org/abs/2602.16241v1",
  "authors": [
    "Md. Najib Hasan",
    "Touseef Hasan",
    "Souvika Sarkar"
  ],
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.16241v1"
}