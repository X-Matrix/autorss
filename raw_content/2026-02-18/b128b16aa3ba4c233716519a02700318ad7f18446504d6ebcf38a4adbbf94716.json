{
  "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
  "link": "https://arxiv.org/abs/2602.16671v1",
  "published": "2026-02-18T18:09:03Z",
  "updated": "2026-02-18T18:09:03Z",
  "summary": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
  "id": "http://arxiv.org/abs/2602.16671v1",
  "authors": [
    "Jaid Monwar Chowdhury",
    "Chi-An Fu",
    "Reyhaneh Jabbarvand"
  ],
  "categories": [
    "cs.SE",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.16671v1"
}