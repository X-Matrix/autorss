{
  "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models",
  "link": "https://arxiv.org/abs/2602.16626v1",
  "published": "2026-02-18T17:21:02Z",
  "updated": "2026-02-18T17:21:02Z",
  "summary": "Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.",
  "id": "http://arxiv.org/abs/2602.16626v1",
  "authors": [
    "SungJun Cho",
    "Chetan Gohil",
    "Rukuang Huang",
    "Oiwi Parker Jones",
    "Mark W. Woolrich"
  ],
  "categories": [
    "cs.LG",
    "cs.AI",
    "q-bio.NC"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.16626v1"
}