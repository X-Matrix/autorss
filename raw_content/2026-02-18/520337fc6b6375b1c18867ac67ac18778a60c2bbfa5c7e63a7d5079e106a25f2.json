{
  "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
  "link": "https://arxiv.org/abs/2602.16703v1",
  "published": "2026-02-18T18:51:28Z",
  "updated": "2026-02-18T18:51:28Z",
  "summary": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
  "id": "http://arxiv.org/abs/2602.16703v1",
  "authors": [
    "Shen Zhou Hong",
    "Alex Kleinman",
    "Alyssa Mathiowetz",
    "Adam Howes",
    "Julian Cohen",
    "Suveer Ganta",
    "Alex Letizia",
    "Dora Liao",
    "Deepika Pahari",
    "Xavier Roberts-Gaal",
    "Luca Righetti",
    "Joe Torres"
  ],
  "categories": [
    "cs.CY",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.16703v1"
}