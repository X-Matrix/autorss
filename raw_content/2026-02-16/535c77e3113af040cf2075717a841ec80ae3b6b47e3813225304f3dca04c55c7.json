{
  "title": "Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs",
  "link": "https://arxiv.org/abs/2602.14697v1",
  "published": "2026-02-16T12:34:27Z",
  "updated": "2026-02-16T12:34:27Z",
  "summary": "Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL",
  "id": "http://arxiv.org/abs/2602.14697v1",
  "authors": [
    "Lunjun Zhang",
    "Ryan Chen",
    "Bradly C. Stadie"
  ],
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.14697v1"
}