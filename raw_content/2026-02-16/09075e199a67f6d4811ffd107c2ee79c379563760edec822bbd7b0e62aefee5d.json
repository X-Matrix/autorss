{
  "title": "Scale redundancy and soft gauge fixing in positively homogeneous neural networks",
  "link": "https://arxiv.org/abs/2602.14729v1",
  "published": "2026-02-16T13:21:49Z",
  "updated": "2026-02-16T13:21:49Z",
  "summary": "Neural networks with positively homogeneous activations exhibit an exact continuous reparametrization symmetry: neuron-wise rescalings generate parameter-space orbits along which the input--output function is invariant. We interpret this symmetry as a gauge redundancy and introduce gauge-adapted coordinates that separate invariant and scale-imbalance directions. Inspired by gauge fixing in field theory, we introduce a soft orbit-selection (norm-balancing) functional acting only on redundant scale coordinates. We show analytically that it induces dissipative relaxation of imbalance modes to preserve the realized function. In controlled experiments, this orbit-selection penalty expands the stable learning-rate regime and suppresses scale drift without changing expressivity. These results establish a structural link between gauge-orbit geometry and optimization conditioning, providing a concrete connection between gauge-theoretic concepts and machine learning.",
  "id": "http://arxiv.org/abs/2602.14729v1",
  "authors": [
    "Rodrigo Carmo Terin"
  ],
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.14729v1"
}