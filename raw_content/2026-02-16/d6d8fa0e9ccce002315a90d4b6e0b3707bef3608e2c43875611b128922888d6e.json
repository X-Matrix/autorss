{
  "title": "On the Learning Dynamics of RLVR at the Edge of Competence",
  "link": "https://arxiv.org/abs/2602.14872v1",
  "published": "2026-02-16T16:03:08Z",
  "updated": "2026-02-16T16:03:08Z",
  "summary": "Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model's capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.",
  "id": "http://arxiv.org/abs/2602.14872v1",
  "authors": [
    "Yu Huang",
    "Zixin Wen",
    "Yuejie Chi",
    "Yuting Wei",
    "Aarti Singh",
    "Yingbin Liang",
    "Yuxin Chen"
  ],
  "categories": [
    "cs.LG",
    "cs.AI",
    "math.OC",
    "stat.ML"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.14872v1"
}