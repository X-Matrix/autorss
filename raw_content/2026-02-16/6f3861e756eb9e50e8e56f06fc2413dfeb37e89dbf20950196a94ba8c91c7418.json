{
  "title": "Lifted Relational Probabilistic Inference via Implicit Learning",
  "link": "https://arxiv.org/abs/2602.14890v1",
  "published": "2026-02-16T16:24:13Z",
  "updated": "2026-02-16T16:24:13Z",
  "summary": "Reconciling the tension between inductive learning and deductive reasoning in first-order relational domains is a longstanding challenge in AI. We study the problem of answering queries in a first-order relational probabilistic logic through a joint effort of learning and reasoning, without ever constructing an explicit model. Traditional lifted inference assumes access to a complete model and exploits symmetry to evaluate probabilistic queries; however, learning such models from partial, noisy observations is intractable in general. We reconcile these two challenges through implicit learning to reason and first-order relational probabilistic inference techniques. More specifically, we merge incomplete first-order axioms with independently sampled, partially observed examples into a bounded-degree fragment of the sum-of-squares (SOS) hierarchy in polynomial time. Our algorithm performs two lifts simultaneously: (i) grounding-lift, where renaming-equivalent ground moments share one variable, collapsing the domain of individuals; and (ii) world-lift, where all pseudo-models (partial world assignments) are enforced in parallel, producing a global bound that holds across all worlds consistent with the learned constraints. These innovations yield the first polynomial-time framework that implicitly learns a first-order probabilistic logic and performs lifted inference over both individuals and worlds.",
  "id": "http://arxiv.org/abs/2602.14890v1",
  "authors": [
    "Luise Ge",
    "Brendan Juba",
    "Kris Nilsson",
    "Alison Shao"
  ],
  "categories": [
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.14890v1"
}