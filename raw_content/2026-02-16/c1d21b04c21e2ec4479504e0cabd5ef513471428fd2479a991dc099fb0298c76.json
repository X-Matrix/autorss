{
  "title": "The Well-Tempered Classifier: Some Elementary Properties of Temperature Scaling",
  "link": "https://arxiv.org/abs/2602.14862v1",
  "published": "2026-02-16T15:54:52Z",
  "updated": "2026-02-16T15:54:52Z",
  "summary": "Temperature scaling is a simple method that allows to control the uncertainty of probabilistic models. It is mostly used in two contexts: improving the calibration of classifiers and tuning the stochasticity of large language models (LLMs). In both cases, temperature scaling is the most popular method for the job. Despite its popularity, a rigorous theoretical analysis of the properties of temperature scaling has remained elusive. We investigate here some of these properties. For classification, we show that increasing the temperature increases the uncertainty in the model in a very general sense (and in particular increases its entropy). However, for LLMs, we challenge the common claim that increasing temperature increases diversity. Furthermore, we introduce two new characterisations of temperature scaling. The first one is geometric: the tempered model is shown to be the information projection of the original model onto the set of models with a given entropy. The second characterisation clarifies the role of temperature scaling as a submodel of more general linear scalers such as matrix scaling and Dirichlet calibration: we show that temperature scaling is the only linear scaler that does not change the hard predictions of the model.",
  "id": "http://arxiv.org/abs/2602.14862v1",
  "authors": [
    "Pierre-Alexandre Mattei",
    "Bruno Loureiro"
  ],
  "categories": [
    "stat.ML",
    "cs.AI",
    "cs.IT",
    "cs.LG",
    "stat.ME"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.14862v1"
}