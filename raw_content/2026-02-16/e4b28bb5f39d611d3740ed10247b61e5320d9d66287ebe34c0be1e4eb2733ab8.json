{
  "title": "Residual Connections and the Causal Shift: Uncovering a Structural Misalignment in Transformers",
  "link": "https://arxiv.org/abs/2602.14760v1",
  "published": "2026-02-16T14:04:42Z",
  "updated": "2026-02-16T14:04:42Z",
  "summary": "Large Language Models (LLMs) are trained with next-token prediction, implemented in autoregressive Transformers via causal masking for parallelism. This creates a subtle misalignment: residual connections tie activations to the current token, while supervision targets the next token, potentially propagating mismatched information if the current token is not the most informative for prediction. In this work, we empirically localize this input-output alignment shift in pretrained LLMs, using decoding trajectories over tied embedding spaces and similarity-based metrics. Our experiments reveal that the hidden token representations switch from input alignment to output alignment deep within the network. Motivated by this observation, we propose a lightweight residual-path mitigation based on residual attenuation, implemented either as a fixed-layer intervention or as a learnable gating mechanism. Experiments on multiple benchmarks show that these strategies alleviate the representation misalignment and yield improvements, providing an efficient and general architectural enhancement for autoregressive Transformers.",
  "id": "http://arxiv.org/abs/2602.14760v1",
  "authors": [
    "Jonathan Lys",
    "Vincent Gripon",
    "Bastien Pasdeloup",
    "Lukas Mauch",
    "Fabien Cardinaux",
    "Ghouthi Boukli Hacene"
  ],
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.14760v1"
}