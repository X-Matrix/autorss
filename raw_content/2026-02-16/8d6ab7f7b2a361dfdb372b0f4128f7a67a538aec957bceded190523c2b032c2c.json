{
  "title": "VIPA: Visual Informative Part Attention for Referring Image Segmentation",
  "link": "https://arxiv.org/abs/2602.14788v1",
  "published": "2026-02-16T14:36:50Z",
  "updated": "2026-02-16T14:36:50Z",
  "summary": "Referring Image Segmentation (RIS) aims to segment a target object described by a natural language expression. Existing methods have evolved by leveraging the vision information into the language tokens. To more effectively exploit visual contexts for fine-grained segmentation, we propose a novel Visual Informative Part Attention (VIPA) framework for referring image segmentation. VIPA leverages the informative parts of visual contexts, called a visual expression, which can effectively provide the structural and semantic visual target information to the network. This design reduces high-variance cross-modal projection and enhances semantic consistency in an attention mechanism of the referring image segmentation. We also design a visual expression generator (VEG) module, which retrieves informative visual tokens via local-global linguistic context cues and refines the retrieved tokens for reducing noise information and sharing informative visual attributes. This module allows the visual expression to consider comprehensive contexts and capture semantic visual contexts of informative regions. In this way, our framework enables the network's attention to robustly align with the fine-grained regions of interest. Extensive experiments and visual analysis demonstrate the effectiveness of our approach. Our VIPA outperforms the existing state-of-the-art methods on four public RIS benchmarks.",
  "id": "http://arxiv.org/abs/2602.14788v1",
  "authors": [
    "Yubin Cho",
    "Hyunwoo Yu",
    "Kyeongbo Kong",
    "Kyomin Sohn",
    "Bongjoon Hyun",
    "Suk-Ju Kang"
  ],
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.14788v1"
}