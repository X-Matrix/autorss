{
  "title": "BFS-PO: Best-First Search for Large Reasoning Models",
  "link": "https://arxiv.org/abs/2602.14917v1",
  "published": "2026-02-16T16:53:41Z",
  "updated": "2026-02-16T16:53:41Z",
  "summary": "Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.",
  "id": "http://arxiv.org/abs/2602.14917v1",
  "authors": [
    "Fiorenzo Parascandolo",
    "Wenhui Tan",
    "Enver Sangineto",
    "Ruihua Song",
    "Rita Cucchiara"
  ],
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.14917v1"
}