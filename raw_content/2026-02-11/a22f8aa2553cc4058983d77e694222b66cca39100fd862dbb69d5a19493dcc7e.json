{
  "title": "Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics",
  "link": "https://arxiv.org/abs/2602.10885v1",
  "published": "2026-02-11T14:13:46Z",
  "updated": "2026-02-11T14:13:46Z",
  "summary": "Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \\textbf{RLCER} (\\textbf{R}einforcement \\textbf{L}earning with \\textbf{C}oT Supervision via Self-\\textbf{E}volving \\textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.",
  "id": "http://arxiv.org/abs/2602.10885v1",
  "authors": [
    "Leheng Sheng",
    "Wenchang Ma",
    "Ruixin Hong",
    "Xiang Wang",
    "An Zhang",
    "Tat-Seng Chua"
  ],
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.10885v1"
}