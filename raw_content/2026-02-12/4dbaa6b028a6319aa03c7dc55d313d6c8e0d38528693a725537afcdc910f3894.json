{
  "title": "Manifold-Aware Temporal Domain Generalization for Large Language Models",
  "link": "https://arxiv.org/abs/2602.11965v1",
  "published": "2026-02-12T14:00:44Z",
  "updated": "2026-02-12T14:00:44Z",
  "summary": "Temporal distribution shifts are pervasive in real-world deployments of Large Language Models (LLMs), where data evolves continuously over time. While Temporal Domain Generalization (TDG) seeks to model such structured evolution, existing approaches characterize model adaptation in the full parameter space. This formulation becomes computationally infeasible for modern LLMs. This paper introduces a geometric reformulation of TDG under parameter-efficient fine-tuning. We establish that the low-dimensional temporal structure underlying model evolution can be preserved under parameter-efficient reparameterization, enabling temporal modeling without operating in the ambient parameter space. Building on this principle, we propose Manifold-aware Temporal LoRA (MaT-LoRA), which constrains temporal updates to a shared low-dimensional manifold within a low-rank adaptation subspace, and models its evolution through a structured temporal core. This reparameterization dramatically reduces temporal modeling complexity while retaining expressive power. Extensive experiments on synthetic and real-world datasets, including scientific documents, news publishers, and review ratings, demonstrate that MaT-LoRA achieves superior temporal generalization performance with practical scalability for LLMs.",
  "id": "http://arxiv.org/abs/2602.11965v1",
  "authors": [
    "Yiheng Yao",
    "Zekun Cai",
    "Xinyuan Song",
    "Hiroki Hill Kobayashi",
    "Xuan Song",
    "Ryosuke Shibasaki",
    "Liang Zhao"
  ],
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.11965v1"
}