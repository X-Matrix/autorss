{
  "title": "Agentic Test-Time Scaling for WebAgents",
  "link": "https://arxiv.org/abs/2602.12276v1",
  "published": "2026-02-12T18:58:30Z",
  "updated": "2026-02-12T18:58:30Z",
  "summary": "Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.",
  "id": "http://arxiv.org/abs/2602.12276v1",
  "authors": [
    "Nicholas Lee",
    "Lutfi Eren Erdogan",
    "Chris Joseph John",
    "Surya Krishnapillai",
    "Michael W. Mahoney",
    "Kurt Keutzer",
    "Amir Gholami"
  ],
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.12276v1"
}