{
  "title": "Towards Performance-Enhanced Model-Contrastive Federated Learning using Historical Information in Heterogeneous Scenarios",
  "link": "https://arxiv.org/abs/2602.11945v1",
  "published": "2026-02-12T13:40:37Z",
  "updated": "2026-02-12T13:40:37Z",
  "summary": "Federated Learning (FL) enables multiple nodes to collaboratively train a model without sharing raw data. However, FL systems are usually deployed in heterogeneous scenarios, where nodes differ in both data distributions and participation frequencies, which undermines the FL performance. To tackle the above issue, this paper proposes PMFL, a performance-enhanced model-contrastive federated learning framework using historical training information. Specifically, on the node side, we design a novel model-contrastive term into the node optimization objective by incorporating historical local models to capture stable contrastive points, thereby improving the consistency of model updates in heterogeneous data distributions.   On the server side, we utilize the cumulative participation count of each node to adaptively adjust its aggregation weight, thereby correcting the bias in the global objective caused by different node participation frequencies. Furthermore, the updated global model incorporates historical global models to reduce its fluctuations in performance between adjacent rounds. Extensive experiments demonstrate that PMFL achieves superior performance compared with existing FL methods in heterogeneous scenarios.",
  "id": "http://arxiv.org/abs/2602.11945v1",
  "authors": [
    "Hongliang Zhang",
    "Jiguo Yu",
    "Guijuan Wang",
    "Wenshuo Ma",
    "Tianqing He",
    "Baobao Chai",
    "Chunqiang Hu"
  ],
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.11945v1"
}