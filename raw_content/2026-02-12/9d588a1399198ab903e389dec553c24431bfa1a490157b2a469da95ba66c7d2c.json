{
  "title": "GPT-4o Lacks Core Features of Theory of Mind",
  "link": "https://arxiv.org/abs/2602.12150v1",
  "published": "2026-02-12T16:33:58Z",
  "updated": "2026-02-12T16:33:58Z",
  "summary": "Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior -- regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.",
  "id": "http://arxiv.org/abs/2602.12150v1",
  "authors": [
    "John Muchovej",
    "Amanda Royka",
    "Shane Lee",
    "Julian Jara-Ettinger"
  ],
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.12150v1"
}