{
  "title": "An Empirical Study of the Imbalance Issue in Software Vulnerability Detection",
  "link": "https://arxiv.org/abs/2602.12038v1",
  "published": "2026-02-12T15:05:47Z",
  "updated": "2026-02-12T15:05:47Z",
  "summary": "Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerability detection remains in its early stages, with model performance exhibiting variability across datasets. Drawing insights from other well-explored application areas like computer vision, we conjecture that the imbalance issue (the number of vulnerable code is extremely small) is at the core of the phenomenon. To validate this, we conduct a comprehensive empirical study involving nine open-source datasets and two state-of-the-art DL models. The results confirm our conjecture. We also obtain insightful findings on how existing imbalance solutions perform in vulnerability detection. It turns out that these solutions perform differently as well across datasets and evaluation metrics. Specifically: 1) Focal loss is more suitable to improve the precision, 2) mean false error and class-balanced loss encourages the recall, and 3) random over-sampling facilitates the F1-measure. However, none of them excels across all metrics. To delve deeper, we explore external influences on these solutions and offer insights for developing new solutions.",
  "id": "http://arxiv.org/abs/2602.12038v1",
  "authors": [
    "Yuejun Guo",
    "Qiang Hu",
    "Qiang Tang",
    "Yves Le Traon"
  ],
  "categories": [
    "cs.SE",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.12038v1"
}