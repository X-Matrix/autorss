{
  "title": "Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings",
  "link": "https://arxiv.org/abs/2602.15791v1",
  "published": "2026-02-17T18:26:36Z",
  "updated": "2026-02-17T18:26:36Z",
  "summary": "Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.",
  "id": "http://arxiv.org/abs/2602.15791v1",
  "authors": [
    "Suhyung Jang",
    "Ghang Lee",
    "Jaekun Lee",
    "Hyunjun Lee"
  ],
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.15791v1"
}