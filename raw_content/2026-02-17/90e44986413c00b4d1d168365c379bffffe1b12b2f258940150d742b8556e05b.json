{
  "title": "Decision Quality Evaluation Framework at Pinterest",
  "link": "https://arxiv.org/abs/2602.15809v1",
  "published": "2026-02-17T18:45:55Z",
  "updated": "2026-02-17T18:45:55Z",
  "summary": "Online platforms require robust systems to enforce content safety policies at scale. A critical component of these systems is the ability to evaluate the quality of moderation decisions made by both human agents and Large Language Models (LLMs). However, this evaluation is challenging due to the inherent trade-offs between cost, scale, and trustworthiness, along with the complexity of evolving policies. To address this, we present a comprehensive Decision Quality Evaluation Framework developed and deployed at Pinterest. The framework is centered on a high-trust Golden Set (GDS) curated by subject matter experts (SMEs), which serves as a ground truth benchmark. We introduce an automated intelligent sampling pipeline that uses propensity scores to efficiently expand dataset coverage. We demonstrate the framework's practical application in several key areas: benchmarking the cost-performance trade-offs of various LLM agents, establishing a rigorous methodology for data-driven prompt optimization, managing complex policy evolution, and ensuring the integrity of policy content prevalence metrics via continuous validation. The framework enables a shift from subjective assessments to a data-driven and quantitative practice for managing content safety systems.",
  "id": "http://arxiv.org/abs/2602.15809v1",
  "authors": [
    "Yuqi Tian",
    "Robert Paine",
    "Attila Dobi",
    "Kevin O'Sullivan",
    "Aravindh Manickavasagam",
    "Faisal Farooq"
  ],
  "categories": [
    "stat.AP",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.15809v1"
}