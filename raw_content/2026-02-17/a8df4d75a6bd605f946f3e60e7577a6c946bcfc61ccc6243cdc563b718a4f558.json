{
  "title": "Avey-B",
  "link": "https://arxiv.org/abs/2602.15814v1",
  "published": "2026-02-17T18:50:40Z",
  "updated": "2026-02-17T18:50:40Z",
  "summary": "Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.",
  "id": "http://arxiv.org/abs/2602.15814v1",
  "authors": [
    "Devang Acharya",
    "Mohammad Hammoud"
  ],
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.15814v1"
}