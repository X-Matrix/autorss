{
  "title": "Relative Geometry of Neural Forecasters: Linking Accuracy and Alignment in Learned Latent Geometry",
  "link": "https://arxiv.org/abs/2602.15676v1",
  "published": "2026-02-17T16:00:08Z",
  "updated": "2026-02-17T16:00:08Z",
  "summary": "Neural networks can accurately forecast complex dynamical systems, yet how they internally represent underlying latent geometry remains poorly understood. We study neural forecasters through the lens of representational alignment, introducing anchor-based, geometry-agnostic relative embeddings that remove rotational and scaling ambiguities in latent spaces. Applying this framework across seven canonical dynamical systems - ranging from periodic to chaotic - we reveal reproducible family-level structure: multilayer perceptrons align with other MLPs, recurrent networks with RNNs, while transformers and echo-state networks achieve strong forecasts despite weaker alignment. Alignment generally correlates with forecasting accuracy, yet high accuracy can coexist with low alignment. Relative geometry thus provides a simple, reproducible foundation for comparing how model families internalize and represent dynamical structure.",
  "id": "http://arxiv.org/abs/2602.15676v1",
  "authors": [
    "Deniz Kucukahmetler",
    "Maximilian Jean Hemmann",
    "Julian Mosig von Aehrenfeld",
    "Maximilian Amthor",
    "Christian Deubel",
    "Nico Scherf",
    "Diaaeldin Taha"
  ],
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.15676v1"
}