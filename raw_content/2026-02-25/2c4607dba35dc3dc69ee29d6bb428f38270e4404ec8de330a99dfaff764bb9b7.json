{
  "title": "Distill and Align Decomposition for Enhanced Claim Verification",
  "link": "https://arxiv.org/abs/2602.21857v1",
  "published": "2026-02-25T12:32:04Z",
  "updated": "2026-02-25T12:32:04Z",
  "summary": "Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.",
  "id": "http://arxiv.org/abs/2602.21857v1",
  "authors": [
    "Jabez Magomere",
    "Elena Kochkina",
    "Samuel Mensah",
    "Simerjot Kaur",
    "Fernando Acero",
    "Arturo Oncevay",
    "Charese H. Smiley",
    "Xiaomo Liu",
    "Manuela Veloso"
  ],
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.21857v1"
}