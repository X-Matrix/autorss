{
  "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs",
  "link": "https://arxiv.org/abs/2602.21864v1",
  "published": "2026-02-25T12:45:45Z",
  "updated": "2026-02-25T12:45:45Z",
  "summary": "Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, resulting in inaccurate or over-lengthy responses to graph-related queries. To address this, we propose the $\\mbox{DynamicGTR}$ framework, which dynamically selects the optimal GTR for each query during inference, thereby enhancing the zero-shot graph QA capabilities of VLMs with a customizable accuracy and brevity trade-off. Extensive experiments show that DynamicGTR not only improves VLM-based graph algorithm QA performance but also successfully transfers the experience trained from synthetic graph algorithm tasks to real-world applications like link prediction and node classification, without any additional training. Additionally, DynamicGTR demonstrates strong transferability across tasks, domains, and models, suggesting its potential as a flexible solution for broad graph scenarios.",
  "id": "http://arxiv.org/abs/2602.21864v1",
  "authors": [
    "Yanbin Wei",
    "Jiangyue Yan",
    "Chun Kang",
    "Yang Chen",
    "Hua Liu",
    "James Kwok",
    "Yu Zhang"
  ],
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.CL",
    "cs.GR"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.21864v1"
}