{
  "title": "An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention",
  "link": "https://arxiv.org/abs/2602.21800v1",
  "published": "2026-02-25T11:27:34Z",
  "updated": "2026-02-25T11:27:34Z",
  "summary": "The rapid advancement of large language models (LLMs) has led to a significant increase in automated tools in the software engineering, capable of performing various code-related tasks such as code generation, completion, and translation. Despite these advancements, its effectiveness is constrained by fixed context lengths, limiting its ability to generalize across long, domain-specific code sequences. To address this challenge, we investigate zero-shot, inference-only methods aimed at improving position encodings and optimizing attention mechanisms. Our goal is to provide a thorough analysis of current approaches that facilitate context length extrapolation in code, particularly in the context of long code completion tasks.",
  "id": "http://arxiv.org/abs/2602.21800v1",
  "authors": [
    "Madhusudan Ghosh",
    "Rishabh Gupta"
  ],
  "categories": [
    "cs.SE",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.21800v1"
}