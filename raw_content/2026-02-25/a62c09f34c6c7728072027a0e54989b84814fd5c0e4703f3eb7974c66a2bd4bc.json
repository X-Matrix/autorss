{
  "title": "NESTOR: A Nested MOE-based Neural Operator for Large-Scale PDE Pre-Training",
  "link": "https://arxiv.org/abs/2602.22059v1",
  "published": "2026-02-25T16:08:46Z",
  "updated": "2026-02-25T16:08:46Z",
  "summary": "Neural operators have emerged as an efficient paradigm for solving PDEs, overcoming the limitations of traditional numerical methods and significantly improving computational efficiency. However, due to the diversity and complexity of PDE systems, existing neural operators typically rely on a single network architecture, which limits their capacity to fully capture heterogeneous features and complex system dependencies. This constraint poses a bottleneck for large-scale PDE pre-training based on neural operators. To address these challenges, we propose a large-scale PDE pre-trained neural operator based on a nested Mixture-of-Experts (MoE) framework. In particular, the image-level MoE is designed to capture global dependencies, while the token-level Sub-MoE focuses on local dependencies. Our model can selectively activate the most suitable expert networks for a given input, thereby enhancing generalization and transferability. We conduct large-scale pre-training on twelve PDE datasets from diverse sources and successfully transfer the model to downstream tasks. Extensive experiments demonstrate the effectiveness of our approach.",
  "id": "http://arxiv.org/abs/2602.22059v1",
  "authors": [
    "Dengdi Sun",
    "Xiaoya Zhou",
    "Xiao Wang",
    "Hao Si",
    "Wanli Lyu",
    "Jin Tang",
    "Bin Luo"
  ],
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.22059v1"
}