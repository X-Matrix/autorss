{
  "title": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments",
  "link": "https://arxiv.org/abs/2602.21939v1",
  "published": "2026-02-25T14:24:47Z",
  "updated": "2026-02-25T14:24:47Z",
  "summary": "How can researchers identify beliefs that large language models (LLMs) hide? As LLMs become more sophisticated and the prevalence of alignment faking increases, combined with their growing integration into high-stakes decision-making, responding to this challenge has become critical. This paper proposes that a list experiment, a simple method widely used in the social sciences, can be applied to study the hidden beliefs of LLMs. List experiments were originally developed to circumvent social desirability bias in human respondents, which closely parallels alignment faking in LLMs. The paper implements a list experiment on models developed by Anthropic, Google, and OpenAI and finds hidden approval of mass surveillance across all models, as well as some approval of torture, discrimination, and first nuclear strike. Importantly, a placebo treatment produces a null result, validating the method. The paper then compares list experiments with direct questioning and discusses the utility of the approach.",
  "id": "http://arxiv.org/abs/2602.21939v1",
  "authors": [
    "Maxim Chupilkin"
  ],
  "categories": [
    "cs.CY",
    "cs.AI"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.21939v1"
}