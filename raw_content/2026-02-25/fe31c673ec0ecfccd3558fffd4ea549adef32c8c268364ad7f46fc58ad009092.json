{
  "title": "Generalisation of RLHF under Reward Shift and Clipped KL Regularisation",
  "link": "https://arxiv.org/abs/2602.21765v1",
  "published": "2026-02-25T10:36:17Z",
  "updated": "2026-02-25T10:36:17Z",
  "summary": "Alignment and adaptation in large language models heavily rely on reinforcement learning from human feedback (RLHF); yet, theoretical understanding of its generalisability remains premature, especially when the learned reward could shift, and the KL control is estimated and clipped. To address this issue, we develop generalisation theory for RLHF that explicitly accounts for (1) \\emph{reward shift}: reward models are trained on preference data from earlier or mixed behaviour policies while RLHF optimises the current policy on its own rollouts; and (2) \\emph{clipped KL regularisation}: the KL regulariser is estimated from sampled log-probability ratios and then clipped for stabilisation, resulting in an error to RLHF. We present generalisation bounds for RLHF, suggesting that the generalisation error stems from a sampling error from prompts and rollouts, a reward shift error, and a KL clipping error. We also discuss special cases of (1) initialising RLHF parameters with a uniform prior over a finite space, and (2) training RLHF by stochastic gradient descent, as an Ornstein-Uhlenbeck process. The theory yields practical implications in (1) optimal KL clipping threshold, and (2) budget allocation in prompts, rollouts, and preference data.",
  "id": "http://arxiv.org/abs/2602.21765v1",
  "authors": [
    "Kenton Tang",
    "Yuzhu Chen",
    "Fengxiang He"
  ],
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.21765v1"
}