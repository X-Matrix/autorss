{
  "title": "Scattering Transform for Auditory Attention Decoding",
  "link": "https://arxiv.org/abs/2602.23003v1",
  "published": "2026-02-26T13:48:27Z",
  "updated": "2026-02-26T13:48:27Z",
  "summary": "The use of hearing aids will increase in the coming years due to demographic change. One open problem that remains to be solved by a new generation of hearing aids is the cocktail party problem. A possible solution is electroencephalography-based auditory attention decoding. This has been the subject of several studies in recent years, which have in common that they use the same preprocessing methods in most cases. In this work, in order to achieve an advantage, the use of a scattering transform is proposed as an alternative to these preprocessing methods. The two-layer scattering transform is compared with a regular filterbank, the synchrosqueezing short-time Fourier transform and the common preprocessing. To demonstrate the performance, the known and the proposed preprocessing methods are compared for different classification tasks on two widely used datasets, provided by the KU Leuven (KUL) and the Technical University of Denmark (DTU). Both established and new neural-network-based models, CNNs, LSTMs, and recent Transformer/graph-based models are used for classification. Various evaluation strategies were compared, with a focus on the task of classifying speakers who are unknown from the training. We show that the two-layer scattering transform can significantly improve the performance for subject-related conditions, especially on the KUL dataset. However, on the DTU dataset, this only applies to some of the models, or when larger amounts of training data are provided, as in 10-fold cross-validation. This suggests that the scattering transform is capable of extracting additional relevant information.",
  "id": "http://arxiv.org/abs/2602.23003v1",
  "authors": [
    "Ren√© Pallenberg",
    "Fabrice Katzberg",
    "Alfred Mertins",
    "Marco Maass"
  ],
  "categories": [
    "eess.SP",
    "cs.AI",
    "eess.AS"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.23003v1"
}