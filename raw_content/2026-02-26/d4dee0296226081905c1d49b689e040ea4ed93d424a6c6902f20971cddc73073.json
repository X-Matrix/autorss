{
  "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
  "link": "https://arxiv.org/abs/2602.22897v1",
  "published": "2026-02-26T11:35:04Z",
  "updated": "2026-02-26T11:35:04Z",
  "summary": "Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.",
  "id": "http://arxiv.org/abs/2602.22897v1",
  "authors": [
    "Xiaoxi Li",
    "Wenxiang Jiao",
    "Jiarui Jin",
    "Shijian Wang",
    "Guanting Dong",
    "Jiajie Jin",
    "Hao Wang",
    "Yinuo Wang",
    "Ji-Rong Wen",
    "Yuan Lu",
    "Zhicheng Dou"
  ],
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.CV",
    "cs.LG",
    "cs.MM"
  ],
  "pdf_link": "https://arxiv.org/pdf/2602.22897v1"
}