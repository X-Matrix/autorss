{
  "title": "Why do AI models use so many em-dashes?",
  "link": "https://seangoedecke.com/em-dashes/",
  "published": "Thu, 30 Oct 2025 00:00:00 GMT",
  "summary": "<p>If you asked most people to name a defining feature of AI-generated writing, they’d probably say the em-dash — like this. Language models use em-dashes so much that real humans who like em-dashes have <a href=\"https://www.reddit.com/r/OpenAI/comments/1mk62b1/comment/n7gnqpb/\">stopped using them</a> out of fear of being confused with AI. It’s also surprisingly hard to prompt models to avoid em-dashes: take this <a href=\"https://community.openai.com/t/cannot-get-responses-to-not-include-dashes-and-em-dashes/1023216/7\">thread</a> from the OpenAI forums where users share their unsuccessful attempts. Given all that, it’s kind of weird that <strong>we don’t really know why language models use the em-dash so much</strong>.</p>\n<h3>Explanations I don’t find convincing</h3>\n<p>One common explanation is that normal English text contains a lot of em-dashes, so it’s just learned behavior from the training data. I find this fairly unconvincing, for the reason that <em>everyone thinks AI uses a lot of em-dashes</em>. If em-dashes were as common in AI prose as human prose, they would be as unremarkable as the use of other punctuation marks.</p>\n<p>Another explanation I’m not convinced by is that AI models like em-dashes because they’re so versatile. When the model is trying to predict the next token, an em-dash keeps its options open: it could either continue on the same point or make a brand new point. Since models are just trying to pick the next most likely token, could they just be “playing it safe” by using em-dashes? I don’t think so. First, other punctuation marks are similarly flexible. Second, I’m not sure that “playing it safe” is a good idiom for thinking about how models generate text.</p>\n<p>Other people have <a href=\"https://msukhareva.substack.com/p/the-mystery-of-emdashes-part-two?\">argued</a> that AI models use em-dashes because model training explicitly biases for brevity, and em-dashes are very token-efficient. From what I can tell by playing with the OpenAI <a href=\"https://platform.openai.com/tokenizer\">tokenizer</a>, the em-dash itself isn’t inherently more efficient, but plausibly without it you’d have to write some connective tissue like ”, therefore”. Still, I don’t buy this. Many em-dashes (e.g. the common “it’s not X - it’s Y” pattern) could simply be replaced with a comma, which is equally brief<sup id=\"fnref-0\"><a class=\"footnote-ref\" href=\"#fn-0\">0</a></sup>. I also don’t think GPT-4o is so brevity-focused that it’s doing micro-optimizations around punctuation like this: if it wanted to use fewer tokens, it could simply waffle less<sup id=\"fnref-0.5\"><a class=\"footnote-ref\" href=\"#fn-0.5\">0.5</a></sup>.</p>\n<h3>Could em-dash use be RLHF-ed in from African English?</h3>\n<p>One theory I spent more time looking into was that <strong>em-dash use could reflect the local English dialect of the RLHF workers</strong>. The final stage of training a language model<sup id=\"fnref-1\"><a class=\"footnote-ref\" href=\"#fn-1\">1</a></sup> involves RLHF: reinforcement learning with human feedback. Essentially, hundreds of human testers are paid to interact with the model and grade model outputs, which are then fed back into the model to make it more helpful and friendly.</p>\n<p>The AI company paying for this work is incentivized to do it in countries that are low cost-of-living but have many fluent English speakers. For OpenAI, this meant African countries like Kenya and Nigeria. But one interesting consequence of this decision is that African English is subtly different from American or British English. For instance, African English uses the word “delve” more liberally, which is the <a href=\"https://www.theguardian.com/technology/2024/apr/16/techscape-ai-gadgest-humane-ai-pin-chatgpt\">explanation</a> for why GPT-4o really likes the word “delve” (and other flowery words like “explore” and “tapestry”)<sup id=\"fnref-2\"><a class=\"footnote-ref\" href=\"#fn-2\">2</a></sup>.</p>\n<p>Does African English use a lot of em-dashes, causing African RLHF workers to rate responses with em-dashes highly? This would be a neat explanation, but I don’t think it’s true. I pulled a <a href=\"https://varieng.helsinki.fi/CoRD/corpora/ICE-NIG/\">dataset</a> of Nigerian English text and measured the frequency of em-dashes per-word. Out of all words in the dataset, 0.022% of them were em-dashes. This <a href=\"https://www.researchgate.net/profile/Kun-Sun-5/publication/328512136_Frequency_Distributions_of_Punctuation_Marks_in_English_Evidence_from_Large-scale_Corpora/links/5f803541a6fdccfd7b521aac/Frequency-Distributions-of-Punctuation-Marks-in-English-Evidence-from-Large-scale-Corpora.pdf\">paper</a> about the frequency of punctuation marks in English text in general estimates general em-dash rates as between 0.25% and 0.275%:</p>\n<blockquote>\n<p>The use of the dash increased after 1750, then reached its peak (about 0.35%) in 1860, but afterwards continued to drop up until the 1950s before starting to fluctuate between 0.25% and 0.275%. The frequency of punctuation marks calculated in the current study is relative to word count in corpora.</p>\n</blockquote>\n<p>Remember that point about em-dash rates peaking in 1860 for later. But for now, it seems like Nigerian English, which is a good-enough stand-in for punctuation rates in African English, is actually <em>less</em> prone to use em-dashes. For that reason, I don’t think the overuse of em-dashes and “delve” are caused by the same mechanism.</p>\n<h3>Digitization of print media</h3>\n<p>One interesting observation about em-dashes is that <em>GPT-3.5 did not use them</em>. GPT-4o used ~10x more em-dashes than its predecessor, and GPT-4.1 was even worse. However, Anthropic and Google’s models do use em-dashes. Even the open-source Chinese models use em-dashes<sup id=\"fnref-3\"><a class=\"footnote-ref\" href=\"#fn-3\">3</a></sup>. What changed between November 2022 and July 2024?</p>\n<p>One thing that changed was the makeup of the training data. In 2022, OpenAI was almost certainly training on a mix of public internet data and pirated books from sites like LibGen. However, once the power of language models became apparent, AI labs quickly realized that they needed more high-quality training data, which meant scanning a lot of print books. Only OpenAI employees know when or if OpenAI started scanning books, but <a href=\"https://www.publishersweekly.com/pw/by-topic/digital/copyright/article/98089-federal-judge-rules-ai-training-is-fair-use-in-anthropic-copyright-case.html?utm_source=chatgpt.com\">court filings</a> have revealed that Anthropic started their process in February 2024. I think it’s reasonable to assume that OpenAI did something similar. In other words, <strong>between 2022 and 2024 the training data changed to include a lot of print books</strong>.</p>\n<p>Remember the punctuation rates study above that showed em-dash rates peaking in 1860? I think it’s a plausible theory that the books AI labs digitized skewed closer to 1860 than the pirated books. Intuitively, pirated content biases towards contemporary and popular literature, because that’s what people want to download. If AI labs wanted to go beyond that, they’d have to go and buy older books, which would probably have more em-dashes. We now arrive at what I think is the most plausible explanation for why AI models include so many em-dashes:</p>\n<p><strong>State-of-the-art models rely on late-1800s and early-1900s print books for high-quality training data, and those books use ~30% more em-dashes than contemporary English prose. That’s why it’s so hard to get models to stop using em-dashes: because they learned English from texts that were full of them.</strong></p>\n<p>I want to thank <a href=\"https://msukhareva.substack.com/p/the-mystery-of-emdashes-part-two\">this blog</a> from Maria Sukhareva for putting me onto this point. I disagree with her that em-dashes are structurally preferred, for reasons I’ve briefly covered above, but I think it’s very plausible that she’s correct about digitization driving em-dash usage. For some more specific examples and a similar point, you can also check out <a href=\"https://medium.com/ghost-channel/the-em-dash-debate-is-broken-heres-what-the-data-actually-shows-023fffd5cd06\">this post</a>, which shows just how many em-dashes some classic works have. My favorite book, <em>Moby-Dick</em>, has a staggering 1728 em-dashes!</p>\n<h3>Summary</h3>\n<p>There are three broad categories of possible explanation for why models use em-dashes so much.</p>\n<p>The first category are structural explanations, which argue that em-dashes are somehow inherently preferred by autoregressive models because they save tokens, or preserve optionality, or something else. I don’t find this convincing because GPT-3.5 didn’t overuse emdashes, and it just doesn’t match my intuition about how inference works.</p>\n<p>The second category are RLHF explanations, which argue that human raters prefer em-dashes because they’re more conversational or they’re more common in the particular variant of English where the RLHF-ers live. I think there’s no support for the variant-of-English argument, but the “it’s more conversational” argument could be right. Hard to say what evidence could confirm or deny it.</p>\n<p>The third category are training data explanations, which argue that em-dashes are just in the training data. I don’t buy this as a general explanation, but it does seem likely to me that they might be overrepresented in some high-quality training data: in particular, early-1900s print books. Overall, I think that’s the strongest explanation.</p>\n<h3>Final thoughts</h3>\n<p>This is still largely based on speculation. Maybe I’m wrong about when OpenAI started digitizing written text. If they did it before GPT-3.5, then it couldn’t be the cause of em-dashes. Certainly models trained today are at least in part infected with em-dashes by training on the output of other AI models. Either they’re deliberately trained on synthetic data, or they just can’t avoid vacuuming in a host of AI-generated content along with other internet texts.</p>\n<p>One thing I’m still a bit confused about: if em-dashes are common because they’re a feature of late-1800s/early-1900s writing, <strong>why doesn’t AI prose read more like Moby-Dick?</strong> Is it plausible that the models are picking up fragments of older English prose stylings, like punctuation, but are still producing contemporary-sounding text?</p>\n<p>I also might be wrong that newly-digitized content would have older publication dates. It’s plausible to me that pirated books would skew more contemporary, but could that be outweighed by the number of older books that are in the public domain?</p>\n<p>There also might be a simpler explanation for em-dash prevalence: for instance, maybe em-dashes just read more conversational, so they were preferred by RLHF-ers, and this created a vicious cycle that biased towards more and more em-dashes. This would kind of line up with a Sam Altman <a href=\"https://www.linkedin.com/posts/curtwoodward_chatgpt-em-dash-deathwatch-sam-altman-activity-7355259218972557312-RH4j/\">interview clip</a> where he says they added more em-dashes because people liked them. I don’t know how you’d go about proving or disproving this.</p>\n<p>In general, I’m still surprised that there’s no widespread consensus about the cause of one of the most identifiable features of AI prose. I do think I’m probably right that digitizing late-1800s/early-1900s works is the cause - but it would be really nice if someone who was at OpenAI between GPT-3.5 and GPT-4o (or who’s in a position to know for some other reason) could confirm that this is what happened.</p>\n<p>edit: this post got some comments on <a href=\"https://news.ycombinator.com/item?id=45788327\">Hacker News</a>. There’s an an interesting <a href=\"https://news.ycombinator.com/item?id=45789077\">comment</a> indicating that the CEO of Medium believes that Medium is responsible, since Medium automatically converts two hyphens (”—”) into an em-dash, and Medium was a source of high-quality training data.</p>\n<p>This just isn’t plausible to me at all. If it were common for humans to use hyphens or doubled hyphens as a standin for em-dashes, and the question was “why do LLMs use the em-dash <em>character</em> instead of a hyphen”, then I might believe a typography-related explanation like this. But the question is “why do LLMs use the em-dash <em>punctuation</em> more than humans do”: i.e. the punctuation mark that acts like parentheses or more punchy final comma.</p>\n<p>For this reason, I’m a bit puzzled by the commenters who refer to <a href=\"https://news.ycombinator.com/item?id=45790985\">Unicode</a>, or <a href=\"https://news.ycombinator.com/item?id=45795391\">Russian-language training</a>, or <a href=\"https://news.ycombinator.com/item?id=45788891\">Wikipedia typographic conventions</a>, or <a href=\"https://news.ycombinator.com/item?id=45789129\">mis-OCR-ed hyphens</a> as possible explanations. None of these could possibly explain why the models are <em>doing em-dash things</em>! Misreading a hyphen (e.g. in “double-crossed”) as an em-dash during training will not make the model more likely to use em-dashes as parentheses, it will make the model more likely to use em-dashes as hyphens. And so on.</p>\n<p>edit: I went on <a href=\"https://www.youtube.com/watch?v=nXfiQrTmDCI\">99% Invisible</a> to chat about this topic. You can skip to 25 minutes in to hear my segment (though the whole thing is worth listening to).</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-0\">\n<p>The linked blog post tries to experimentally show that em-dashes save tokens by asking models to paraphrase em-dash sentences and noting that those paraphrased sentences are longer. To be convinced by this, I would like to see if paraphrased non-em-dash sentences are typically the same length or shorter. I suspect that paraphrasing adds tokens no matter what.</p>\n<a class=\"footnote-backref\" href=\"#fnref-0\">↩</a>\n</li>\n<li id=\"fn-0.5\">\n<p>Incidentally, I <em>do</em> think that GPT-5’s overuse of semicolons probably is a brevity bias, because GPT-5 is noticeably less verbose than its predecessors.</p>\n<a class=\"footnote-backref\" href=\"#fnref-0.5\">↩</a>\n</li>\n<li id=\"fn-1\">\n<p>At least, in the era of GPT-4o.</p>\n<a class=\"footnote-backref\" href=\"#fnref-1\">↩</a>\n</li>\n<li id=\"fn-2\">\n<p>Cultural differences in English is a deep rabbit-hole to go down, which I encountered for the first time via Paul Graham’s <a href=\"https://x.com/paulg/status/1777030573220933716\">tweet</a> about “delve”, subsequent <a href=\"https://x.com/paulg/status/1778887559474495624\">tweet</a> about good writing, and the <a href=\"https://medium.com/@moyosoreale/the-paul-graham-vs-nigerian-twitter-saga-lexical-racism-and-language-bias-masked-as-chatgpt-53ee9f6459aa\">torrent of criticism</a> from Nigerians, Indians, and other post-colonial countries that see a willingness to use flowery language as part of competency with language in general.</p>\n<a class=\"footnote-backref\" href=\"#fnref-2\">↩</a>\n</li>\n<li id=\"fn-3\">\n<p>Maybe some of this can be explained by those models training on the output from American labs’ models, but I doubt it. By now it seems pretty clear that the Chinese labs can train pretty strong models on their own.</p>\n<a class=\"footnote-backref\" href=\"#fnref-3\">↩</a>\n</li>\n</ol>\n</div>",
  "id": "https://seangoedecke.com/em-dashes/"
}