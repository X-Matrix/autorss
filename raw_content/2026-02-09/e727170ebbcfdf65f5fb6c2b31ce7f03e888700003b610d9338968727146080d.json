{
  "title": "Writing an LLM from scratch, part 32b -- Interventions: gradient clipping",
  "link": "https://www.gilesthomas.com/2026/02/llm-from-scratch-32b-interventions-gradient-clipping",
  "published": "Thu, 05 Feb 2026 01:20:00 +0000",
  "summary": "<p>I'm still working on training the best GPT-2 small sized base model that I can\nwith a number of FLOPs roughly equal to two days on my own machine -- my \"extra credit\"\nexercise after having worked through\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".</p>\n\n<p>In the <a href=\"/2026/02/llm-from-scratch-32a-interventions-baseline-model\">last post</a> I trained\na baseline model -- one with the same architecture and almost the same training code as in\nthe minimal training run in the book, just modified to run using DDP on an 8x A100 40 GiB/GPU\nmachine in the cloud.\nThere are a bunch of \"interventions\" I want to try to see if they'll make it better,\nas measured by the loss they get on a test set.  I'll do a post for each intervention,\nand this is the first: gradient clipping.</p>\n<h3 id=\"why\">Why?</h3>\n\n<p>In the training chart for the baseline model, you can see that there are three\nplaces where the loss suddenly spiked up, at around global steps 4,200, 13,000,\nand 23,000:</p>\n\n<p><img alt=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-32a-interventions-baseline-model/baseline-training-run-chart.png\" title=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>There are a number of things that could cause loss spikes like that:</p>\n\n<ul>\n<li>A \"bad batch\" -- that is, one batch, or even one sequence in a batch, was\nmassively different in structure to the others that the model had seen, so it just\nhad much worse loss.  That doesn't seem likely in this case, though: the numbers\non the chart are averages over 617 global steps each, and it would take a truly pathological\nsequence to move the needle that much.</li>\n<li>Something weird in the optimiser.  That's not something I understand well, but\naccording to the various LLMs I'm working with, it's a possibility.</li>\n<li>Exploding gradients.  This is my working hypothesis, and so in this post I'll\ntry out gradient clipping, the normal solution to that problem.</li>\n</ul>\n\n<h3 id=\"what\">What?</h3>\n\n<p>Exploding gradients are common in RNNs, and also happen in LLMs like this one.  I spent a bit\nof time reading around to find out how they happen, and the ah-ha moment came when\nI came across <a href=\"https://medium.com/data-science/what-is-gradient-clipping-b8e815cdfb48\">this post from Wanshun Wong</a>.\nNot only is the post itself a good intro in terms of how it affects RNNs, but in the\n\"further reading\" at the end, there's some gold:</p>\n\n<blockquote>\n  <p>Chapter 10.11 of [1] has a good overview of how gradient clipping works.</p>\n  \n  <p>...</p>\n  \n  <p>References</p>\n  \n  <ol>\n  <li>I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning (2016), MIT Press.</li>\n  </ol>\n</blockquote>\n\n<p>Now, I bought a copy of \"<a href=\"https://www.deeplearningbook.org/\">Deep Learning</a>\" at the\nsame time as I bought Raschka's book, but I'd only glanced through it.  Now was the\ntime to get it down from the shelf -- and, indeed, section 10.11.1 is all about clipping\nto handle exploding gradients.  I'll put the explanation of how they happen into my\nown words, to see if I can clarify things (at least in my mind).</p>\n\n<p>Normally, when we learn about gradient descent, it's illustrated with nice smooth\nloss charts like this imaginary one for a single-parameter model:</p>\n\n<p><img alt=\"A simple loss chart\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/simple-loss-chart.png\" title=\"A simple loss chart\" /></p>\n\n<p>We're told that we might start at point A.  The gradient is quite high and negative,\nso we multiply it by our learning rate and subtract it from our parameter.  That\ngets us to point B.  This time around, the gradient is smaller as the curve is flatter\nthere, so when we do the same -- multiply by LR and subtract -- we take a smaller step, and\nwind up at C.  Rinse and repeat and we'll wind up near the minimum.</p>\n\n<p>The problem is, what if the loss curve actually looks like this:</p>\n\n<p><img alt=\"A more complex loss chart\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/more-complex-loss-chart.png\" title=\"A more complex loss chart\" /></p>\n\n<p>...?</p>\n\n<p>We start at A, with a small gradient, move a little to the right, and now we're at\nB halfway down a cliff!  The gradient is massive, and when we subtract it, even scaled\nby the learning rate, we can zoom off somewhere to the right -- maybe not even on the\nchart.  Indeed, you can imagine a cliff that is so steep that\nit would have vertical portions -- negative infinite gradients in this case -- and no matter what your learning\nrate is, you'll wind up with an infinite parameter update and everything will break.\nIt's hard to see how a model can continue training in a case like that.</p>\n\n<p>Now, what can cause steep cliffs like that?  The book says \"strongly nonlinear functions,\nsuch as those computed by a recurrent neural net over many time steps\".</p>\n\n<p>If you know about RNNs (I <a href=\"/2025/10/revisiting-karpathy-unreasonable-effectiveness-rnns\">wrote about them</a>\nif you'd like a summary), you'll remember that a single RNN might be quite\nshallow -- maybe three or four layers -- but when you're doing backpropagation,\nyou run a number of inputs through, one after the other, work out the overall loss, and then \"unroll\" it to\nsomething similar to a \"vanilla\" neural net to do the backward pass.  To put that in\nconcrete terms, a 3-layer neural network trained with a 100-element sequence would\nunroll to a 300-layer deep network.  Every one of those layers has several operations, including\n(in the implementation I was looking at in my post above), a <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow></math>.  It's not surprising that there are cliffs in the loss landscape -- it's\nmore surprising that there are any smooth bits!</p>\n\n<p>Now in LLMs, we don't have that unrolling through time -- but our network is deep enough\nas it is.  For the GPT-2 small model, disregarding the embeddings and the final output head, we have 12 Transformer layers,\neach of which is multiple matrix multiplications for attention, then a softmax, then another layer, and\nthen a feed-forward... mapping precisely to the equivalent vanilla NN is hard, but I think\nyou can treat each one as at least four layers, so we've got 48. And there are GELUs and logs and\nexps <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup> dotted around, so again -- we should expect cliffs.</p>\n\n<p>So if sometimes we'll get crazy gradients, what can we do about them?  We clip them.</p>\n\n<h3 id=\"how\">How?</h3>\n\n<p>Clipping gradients simply means that if they get larger than a particular number -- <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>,\nwhich we define -- we reduce them to that number.  In other words, we have a cap on how\nbig they can get.</p>\n\n<p>\"Deep Learning\" (\"DL\" from now on) suggests two ways to do it.  Remember that while in the\nexample above, we only had one parameter -- on the X axis -- for the GPT-2 small\nLLM we're training, we have 163 million of them.  So the gradients, instead of\nbeing one number, will be a 163M-long vector, one per parameter.  The two ways to clip are:</p>\n\n<ul>\n<li>We clip element-wise.  If any one of the gradients in the vector is larger than <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>,\nwe reduce it to <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>.</li>\n<li>We clip based on the norm: the length of the gradient vector in -- in our\ncase -- 163M-dimensional space.  That sounds harder than it is -- it's really\njust an extension of the Pythagorean equation that <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>&#x0002b;</mo><msup><mi>b</mi><mn>2</mn></msup><mo>&#x0003d;</mo><msup><mi>c</mi><mn>2</mn></msup></mrow></math> to multiple\ndimensions.  If you want to work out the length of a vector <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo stretchy=\"false\">&#x00028;</mo><mi>a</mi><mo>&#x0002c;</mo><mi>b</mi><mo stretchy=\"false\">&#x00029;</mo></mrow></math> then you\ncan use Pythagoras to work out <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>c</mi><mo>&#x0003d;</mo><msqrt><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>&#x0002b;</mo><msup><mi>b</mi><mn>2</mn></msup></mrow></msqrt></mrow></math>, and that generalises\nto any number of dimensions.  So for our model we'd just square all 163M\nelements of the vector, sum those, and take the square root of the result, and that's the norm. <sup class=\"footnote-ref\" id=\"fnref-2\"><a href=\"#fn-2\">2</a></sup>\nIf the norm is greater than <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>, we just divide every element of the gradient vector by the norm\nand multiply the result by <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>, to produce\na new gradient vector whose norm is <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>.</li>\n</ul>\n\n<p>The second feels more elegant -- we're scaling all of the elements of the gradient\nvector by the same amount, so it still points in the same direction.  Interestingly, though,\nDL says that the two methods \"work similarly\", which I'll read as \"are pretty much\nthe same in practice\".</p>\n\n<p>DL then goes on to say how infinite or not-a-number gradients should be handled.\nWith the first way, clearly doing it naively would set every element in the gradient\nvector to <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>, which would make the total size (norm) of the update very large.  With the\nsecond, it be even worse -- we'd still wind up with completely junk gradients, because\nthe norm would be infinite, and in Python <code>math.inf / math.inf</code> is <code>math.nan</code>, so\nwe'd be applying gradients with NaNs in them at best.  That would be likely to\nknock our model into unrecoverable territory, as any parameter that had that applied\nto it would be NaN forever.</p>\n\n<p>Their suggested solution is that if you get garbage gradients like that, you can take\na random step -- that is, create a new gradient to apply that has the norm <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math>\nbut just points in a random direction. The idea is that this will move you away from\nthe cliff-ridden part of the loss landscape where you've found yourself (more about that later), and things will\ncontinue nicely.</p>\n\n<p>So, anyway, how to do this in practice?</p>\n\n<p>PyTorch has a function, <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_\"><code>clip_grad_norm_</code></a>,\nand that's what's referenced in almost every bit of writing I've found about how\nto clip gradients.  So I decided to use that, assuming it would do what was described\nin DL's second option and that it would do the random updates they suggest for non-finite\ngradients.  (I was half-correct -- see later.)</p>\n\n<p>As to how to use it -- if we had a normal training loop, where we were just using a normal optimiser, we would go from:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">y_logits</span><span class=\"p\">,</span> <span class=\"n\">target_y_ids</span><span class=\"p\">)</span>\n    <span class=\"n\">train_loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...to something like</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">y_logits</span><span class=\"p\">,</span> <span class=\"n\">target_y_ids</span><span class=\"p\">)</span>\n    <span class=\"n\">train_loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">clipping_max_norm</span><span class=\"p\">)</span>\n\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...where <code>clipping_max_norm</code> is the max value <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math> from above.</p>\n\n<p>However, for our training code using Automatic Mixed Precision (AMP),\nit's a little more complicated -- but luckily, the AMP explainer we've been using\n<a href=\"https://docs.pytorch.org/docs/stable/notes/amp_examples.html#gradient-clipping\">has a section explaining what to do</a>.</p>\n\n<p>Right now we have this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n        <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n        <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">train_loss</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>Per that explainer, we need to move to this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n        <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n        <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">train_loss</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">unscale_</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">clipping_max_norm</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>That looks a bit weird; we're \"unscaling\" the gradients,\nthen clipping them, then using the scaler to step the\noptimiser.  You'd think that you'd need to \"re-scale\" the scaler after clipping the gradients --\nto get back to where you started from before the optimiser step.\nFrom the help page I gather it keeps track of whether or not the gradients it has right now are\ncurrently scaled and handles them appropriately based on that state in <code>scaler.step</code>.</p>\n\n<p>Anyway, given that we know what the code looks like now, we need to implement it\nin a way that can be easily switched on for this experiment (and potentially in\nthe future), but which also allows us to not use it if we don't want to.</p>\n\n<p>The best way with our setup is to make it a training option, so we can do it this way:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n        <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n        <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">train_loss</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">clipping_max_norm</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">unscale_</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n        <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">clipping_max_norm</span><span class=\"p\">)</span>\n\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n    <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...with <code>clipping_max_norm</code> extracted from the <code>train.json</code> file where we call it in\n<code>load_datasets_and_train</code>:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train</span><span class=\"p\">(</span>\n        <span class=\"n\">run_dir</span><span class=\"p\">,</span>\n        <span class=\"n\">ddp_model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">scaler</span><span class=\"p\">,</span>\n        <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;clipping_max_norm&quot;</span><span class=\"p\">],</span>\n        <span class=\"n\">train_ds</span><span class=\"p\">,</span>\n        <span class=\"n\">global_step</span><span class=\"p\">,</span> <span class=\"n\">best_loss</span><span class=\"p\">,</span>\n        <span class=\"n\">checkpoint_interval</span><span class=\"o\">=</span><span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;checkpoint_interval&quot;</span><span class=\"p\">],</span>\n        <span class=\"n\">do_checkpoints</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...and we can just pass in <code>None</code> for it in our <code>check_batch_size_works</code> function that\nwe use to find the maximum micro-batch size for our current hardware, as all we're\ntesting for there is memory usage -- we don't care if we're doing good updates.</p>\n\n<p>Here's <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/4abebdd2b81dbc7d0a3113fc1c5daf943361357e\">the code delta for that</a>,\nplus a <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/202a79a1d6ede40aab7a8adb19bc0b17bb5c9f5f\">bugfix</a> to allow\nfor <code>train.json</code> files without a <code>clipping_max_norm</code> in them.</p>\n\n<p>But it would also be useful to be able to track when it \"fired\" -- that is, when we\nhad to clip our gradients.  Then we can see two things:</p>\n\n<ol>\n<li>Whether we actually did wind up clipping them and fixing those loss spikes</li>\n<li>Whether we were clipping at other times -- we don't want to be doing it unnecessarily.</li>\n</ol>\n\n<p>Now, the docs for <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\"><code>clip_grad_norm_</code></a>\nsay that it returns the \"[t]otal norm of the parameter gradients (viewed as a single vector)\".\nIt doesn't say whether that's before or after the clipping, but given that the return value would\nalways be <code>clipping_max_norm</code> if it was after, I'm going to guess that it returns\nthe pre-clipping norm (ChatGPT agrees).</p>\n\n<p>So we can chart that; changes in these diffs: <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/6d5d0cd96b553b420cbf2c8d6c2d2af0f5a36582\">1</a>,\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/ac0ef28ac8879f3a6351da166344d918d38cfc76\">2</a>,\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/4d476239d9ed3aa1e0864536ea4c61632898b3bd\">3</a>,\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/326ccd2ef490aea57371394a949969a687c904ad\">4</a>.</p>\n\n<h3 id=\"how-much\">How much?</h3>\n\n<p>So we now have code to clip gradients to a given norm size and to chart the gradient\nnorms so that we know what they were before clipping.  The question is, what\nshould that clipping norm be?  Some googling around suggested that there was no standard way\nof saying \"for such-and-such a kind of model, gradients should be clipped at around\n<math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>x</mi></mrow></math>\".  For example, on <a href=\"https://www.reddit.com/r/MachineLearning/comments/kqgne3/choosing_gradient_norm_clip_value_d/\">this Reddit thread</a>,\n<code>GLVic</code> says \"Common values are 1, 3, 5, 8, 10\", and likewise sample code in\n<a href=\"https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem\">this tutorial</a>.\nhas 1, as does <a href=\"https://www.geeksforgeeks.org/deep-learning/understanding-gradient-clipping/\">this one</a>.</p>\n\n<p>So my initial thought was, let's just use 1.  But then I wondered, what actually are\nthe gradient norms that we're getting in normal training?  I decided to run a local short\ntrain on 3m tokens (a thousandth of the full training set, taking just less than four minutes) with very frequent checkpointing, and\ngradient clipping set to 1, and\nsee what happened.</p>\n\n<p><img alt=\"Small local train, gradient clipping at 1\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/gradient-clipping-1-locally.png\" title=\"Small local train, gradient clipping at 1\" /></p>\n\n<p>You can see that the \"grad max\" line is almost always above the \"grad clip\" -- we're\nalmost always clipping.   This doesn't sound right.  It looked like the range of the grad max\nwas generally beween 1.1 and a little above 3, so I set the <code>clipping_max_norm</code> to 3.5 and\ndid another train:</p>\n\n<p><img alt=\"Small local train, gradient clipping at 3.5\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/gradient-clipping-3.5-locally.png\" title=\"Small local train, gradient clipping at 3.5\" /></p>\n\n<p>Our loss is about the same, but we're no longer clipping -- and that's what we want;\nthere was no evidence of exploding gradients for that short run -- just big updates\nnear the start, as you'd expect.</p>\n\n<p>I then ran the same with no gradient clipping at all, and got exactly the same shape\nfor the loss chart as I did with gradient clipping at 3.5, and the same final loss -- that's a good signal that clipping is\nnot affecting the train when we stay inside the limit, which is exactly what we want.</p>\n\n<p>So, it was time to train our model!</p>\n\n<h3 id=\"running-the-train\">Running the train</h3>\n\n<p>I kicked off the train, and after a little while, I looked at the training chart,\nwhich is updated dynamically as the model trains:</p>\n\n<p><img alt=\"First run of cloud train, with missing max gradients\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/cloud-run-missing-gradient-maxes.png\" title=\"First run of cloud train, with missing max gradients\" /></p>\n\n<p>You can see the dotted green lines, both the light one and the dark one -- that is,\nthe \"grad max\" and the \"grad avg\" -- disappear starting just before global step\n4,000, only coming back at about 5,500 -- that is, these were not plotted for\nglobal steps 4,319 and 4,936, even though the loss was.  What was going on?</p>\n\n<p>I took a look at the checkpoint meta file for the first of those to see what the actual numbers\nwere, and saw this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;min_train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">3.7176883220672607</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;max_train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">5.877607822418213</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;avg_train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">4.3170230991450085</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;max_grad_norms&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">I</span><span class=\"kc\">nf</span><span class=\"err\">i</span><span class=\"kc\">n</span><span class=\"err\">i</span><span class=\"kc\">t</span><span class=\"err\">y</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;avg_grad_norms&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">I</span><span class=\"kc\">nf</span><span class=\"err\">i</span><span class=\"kc\">n</span><span class=\"err\">i</span><span class=\"kc\">t</span><span class=\"err\">y</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;frac_clipped&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.0016207455429497568</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;global_step&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">4319</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;is_best&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<p>Aha!  The PyPlot code I was using could not handle infinite values, which is entirely\nreasonable.  That was easy enough to <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/78a1e794a84c4e01cd7f812755c17ded07f35b93\">fix</a>,\nthough -- I just replaced positive infinity by 1,000,000 and negative infinity by -1,000,000,\nand then (in the interest of getting a proper from-scratch run) kicked everything\noff from the beginning.</p>\n\n<p>That training run completed with this chart:</p>\n\n<p><img alt=\"Second cloud run, showing clipping periods\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/second-cloud-run-showing-clipping.png\" title=\"Second cloud run, showing clipping periods\" /></p>\n\n<p>That's a little hard to read, but if you look closely at the green lines, you\ncan see that there are seven periods where gradients were either very large or\ninfinite.  Weirdly, though, out of the seven, two of them were two checkpoint periods long\n(that is, two periods of 617 global steps).  That felt weird, though of course\nwe're looking at the maximum gradient norm and the average gradient norm -- so\ntwo single infinite/high-gradient steps in successive 617-step periods would lead to that effect.</p>\n\n<p>What was even stranger, though,\nwas that if you look at the training chart for the run with no gradient clipping,\nwe have only three loss spikes rather than seven:</p>\n\n<p><img alt=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-32a-interventions-baseline-model/baseline-training-run-chart.png\" title=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>...though it's also very noticeable that the gradient-clipped run had only two small loss\nspikes, unlike the three larger ones in the unclipped run.</p>\n\n<p>The training loss the gradient-clipped run reported at the end was better, too:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 12,343.442 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,252,160</span>\n<span class=\"go\">Throughput: 264,128 tokens/second</span>\n<span class=\"go\">Final train loss: 3.728</span>\n</code></pre>\n</div>\n\n<p>...versus 3.743 at the end of the baseline train.  Training took a very small amount\nlonger compared to the baseline's 12,243.523 seconds -- 100 seconds, which may well\nbe just noise.</p>\n\n<p>So it was time to download it, <a href=\"https://huggingface.co/gpjt/8xa100m40-gradient-clipping\">upload it to Hugging Face Hub</a>,\nand run the sequence-completion smoke test:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/model.json<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/checkpoints/best/model.safetensors\nEvery<span class=\"w\"> </span>effort<span class=\"w\"> </span>moves<span class=\"w\"> </span>you<span class=\"w\"> </span>further<span class=\"w\"> </span>afield<span class=\"w\"> </span>the<span class=\"w\"> </span>most<span class=\"w\"> </span>to<span class=\"w\"> </span>get<span class=\"w\"> </span>more<span class=\"w\"> </span><span class=\"nb\">time</span><span class=\"w\"> </span>and<span class=\"w\"> </span>space<span class=\"w\"> </span>out<span class=\"w\"> </span>of<span class=\"w\"> </span>your<span class=\"w\"> </span>pocket.<span class=\"w\"> </span>With<span class=\"w\"> </span>more<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>abundance\n</code></pre>\n</div>\n\n<p>Coherent enough!</p>\n\n<p>Next, we evaluate it against our held-back test set:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets/<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/model.json<span class=\"w\"> </span>runs/8xa100m40-gradient-clipping/checkpoints/best/model.safetensors\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">1471</span>.81it/s<span class=\"o\">]</span>\n<span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3200</span>/3200<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">04</span>:58&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.72it/s<span class=\"o\">]</span>\nLoss<span class=\"w\"> </span>against<span class=\"w\"> </span>our<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>dataset:<span class=\"w\"> </span><span class=\"m\">3</span>.678\n</code></pre>\n</div>\n\n<p>So, the loss had gone down -- but only from 3.692 to 3.678, a reduction of 0.014,\nor about 0.3%.</p>\n\n<p>That's not actually all that bad, though!\nAfter all, in my initial experiments on my local machine, training for a Chinchilla-optimal\nnumber of tokens from FineWeb-Edu (rather than the regular FineWeb I'm using now)\ngot a loss of 4.167 on the same dataset (weirdly worse with the more-curated training set),\nand training for a further Chinchilla-optimal number of tokens only brought that down to\n4.135, for a difference of 0.032, or 0.7%.  That was the total effect of <em>doubling</em>\nthe training time.</p>\n\n<p>It's not really comparable due to the different training sets, but speaking <em>very</em>\nloosely, we could say that adding gradient clipping for this train had almost half as much effect as doubling the training time for\nthe other one, with a negligible increase (about a minute or two) on training time.  That's pretty nifty.</p>\n\n<p>But the question remained: why those long periods of high gradients, even with gradient\nclipping?  And why were there still loss spikes -- in particular the one just before\nglobal step 12,000, which lasted for two checkpoint periods?</p>\n\n<h3 id=\"chasing-infinity\">Chasing infinity</h3>\n\n<p>Remember that when I started the first run of this train, and got the chart with\nthe missing bits, it was because the logged <code>max_grad_norms</code> and <code>avg_grad_norms</code>\nwere infinite.</p>\n\n<p>What happens when <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_\"><code>clip_grad_norm_</code></a> gets an infinite gradient -- either one that has\nan infinity as one of its components, or one that (due to numerical overflow) winds up\nwith a norm of infinity anyway?  I'd been kind of assuming that it did what the authors\ndescribed in \"Deep Learning\" -- a random update of norm <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>v</mi></mrow></math> -- given that the book\nstated pretty confidently that you \"can\" do it but then appeared to consider the topic closed.</p>\n\n<p>But it doesn't!  If you check that link to the docs, you'll see that it has a parameter\n<code>error_if_nonfinite</code>, which is <code>False</code> by default.  If it's set to <code>True</code>, that will\nraise an exception if the norm is positive or negative infinity, or if it's not a number\n-- which catches both the infinite component and the norm overflow cases above.  But if\nit's not set -- and we weren't setting it -- and the norm or the gradients are non-finite, then <code>clip_grad_norm_</code> will essentially\nreturn garbage gradients.  Depending on the exact cause, elements will either be infinities\nof one sign or another, or NaNs.  And if these are added to parameters, then those\nparameters will become garbage too.</p>\n\n<p>Now that leads to the question, given that we know that somewhere in the period\nbetween the checkpoint at global step 4,319 and the previous one at 3,702 there was\nan infinite norm at some point, how on earth did the model manage to continue training\nafter that?  Loss went up at around the same time, but it wasn't completely broken as\nit would have been with NaNs or infinities in its parameters.</p>\n\n<p>Obscurely enough, the answer turned out to be in the <a href=\"https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html#adding-gradscaler\">AMP explainer</a>,\nin a comment in one of the bits of example code.  Regarding the <code>GradScaler</code> class we're using:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>        <span class=\"c1\"># ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.</span>\n        <span class=\"c1\"># If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,</span>\n        <span class=\"c1\"># otherwise, optimizer.step() is skipped.</span>\n</code></pre>\n</div>\n\n<p>So what was happening was that the scaler -- something we introduced into our code\nto get a speedup by using 16-bit floats instead of 32-bit whenever PyTorch thought\nit would make sense -- was protecting us against infinite and NaN gradients as a\nside-effect.  It was skipping updates that would have polluted our weights with\nbad values from non-finite gradients.</p>\n\n<p>Hmph.</p>\n\n<h3 id=\"grumble\">Grumble</h3>\n\n<p>If the above comes across as a little frustrated, then it's because I am a bit!\nFrom a software engineering viewpoint, this situation really does feel a bit like a\nrather messy part of the API.</p>\n\n<p>There are three things that it's reasonable for a library to do with infinite/NaN\ngradients:</p>\n\n<ol>\n<li>Blindly apply them and expect the developer to sanitise their inputs.</li>\n<li>Raise an error.</li>\n<li>Take some kind of default sane action, like skipping the update.</li>\n</ol>\n\n<p>Now, if we look at that <code>error_if_nonfinite</code>, we can see that the first two of those\ncases are handled there; and the developer can choose which option to follow.\nIt's not where I'd personally put it (the <code>step</code> function on the optimiser seems more\nnatural) and I think I'd probably set the default to <code>True</code> too, but I can also imagine\ngood reasons for it being the way it is -- backward compatibility for one.</p>\n\n<p>But the \"skip non-finite gradients\" being a (not even optional!) behaviour that is\non a class designed for handling mixed-precision training just seems outright bonkers.\nI would be surprised if there weren't people out there who've spent days trying\nto work out why their training runs failed catastrophically when they decided to\nswitch from mixed-precision to \"full fat\" 32-bit floats, not realising that a\nhardly-even-documented feature of the scaler <sup class=\"footnote-ref\" id=\"fnref-3\"><a href=\"#fn-3\">3</a></sup> had been saving them from gradient issues\npreviously.</p>\n\n<p>Anyway, rant over.  What does this all mean?</p>\n\n<h3 id=\"so\">So...?</h3>\n\n<p>There are three ways a gradient can explode:</p>\n\n<ol>\n<li>It can get very large, still be finite, and have a finite norm.</li>\n<li>It can get very large, still be finite, but have an infinite norm (eg. due to numerical overflow)</li>\n<li>It can become infinite -- that is, at least one of the parameters' gradients is infinite (which\nof course means an infinite norm regardless of any numerical stuff).</li>\n</ol>\n\n<p>With both the baseline code and our new code, the <code>GradScaler</code> was saving us from\nthe last two of those, by skipping the optimiser steps with non-finite gradients.</p>\n\n<p>However, the baseline run was not protected against the first kind -- large but finite\ngradients with a finite norm -- while this run was protected.</p>\n\n<p>What I'm almost certain is happening here is that in all of my training runs so\nfar, there have been all three kinds of issues with exploding gradients.  The\n<code>GradScaler</code>, which again, we introduced for faster training, happened to be saving\nus from the infinite gradients/norms.  But we were still being bitten by the finite\nbut excessively large ones.</p>\n\n<p>And that, I think, is why this training run had a positive -- not huge, but certainly worthwhile\n-- effect on the test set loss.</p>\n\n<p>If I had more time, I think I'd do another run, logging all three of those categories\nof error to see how frequent they are, and charting the result.  That might go some way to\nexplaining the final question I had here: why is it that the renowned \"Deep Learning\"\nsuggests a random update to get away from the cliff where you've found yourself,\nwhile we seem to be getting away with just skipping the update, which is much simpler?\nWell, the book was written in 2016, and I guess rather a lot has changed in the last 10 years :-)</p>\n\n<p>My guess is that their solution might have been\na solid default in the age of RNNs, but might not make so much sense with the kind of models\nwe're training these days.</p>\n\n<p>I think I can see a way in which that makes sense.  Think of the illustration of a loss \"cliff\"\nin a one-parameter world that we had at the start of this post:</p>\n\n<p><img alt=\"A more complex loss chart\" src=\"/post-assets/llm-from-scratch-32b-interventions-gradient-clipping/more-complex-loss-chart.png\" title=\"A more complex loss chart\" /></p>\n\n<p>If you happen to wind up on that cliff, you're in trouble.</p>\n\n<p>But imagine a two-parameter model -- the line of the loss function becomes a surface.\nJust as in the real world you might be able to walk along the edge at the top of a cliff and\nfind a nice easy slope down next to it, you can imagine that the cliff in the two-parameter\ncase might be less of a problem because you don't need to be lucky enough to jump down it --\nyou can walk around it.</p>\n\n<p>Extrapolating examples like this to higher dimensions is\nrisky, but I think it should hold that the more dimensions you're working with,\nthe less likely it is that a cliff is an issue -- you're more likely to be able to find\na way around it.  I've heard a very similar argument made for why local minima are\nless of an issue with lots of parameters.  It's certainly worth saying that this is\nfar from a mathematical proof, but I think it's a decent grounding for intuition.</p>\n\n<p>Now think about an RNN.  Although you're doing back-propagation through time over\nwhat amounts to a very deep network, there aren't actually all that many parameters,\ncertainly compared to an LLM like this.  Each parameter is involved in the back-propagation\nmultiple times.</p>\n\n<p>So, thinking of it that way, the gradient vector for the RNNs they were dealing with\nwas of much lower dimensionality than the ones we're dealing with, even for this\ntiny model.</p>\n\n<p>They say that the random step \"will typically move away from the numerically unstable\nconfiguration\".  I'm probably playing fast and loose here, but I'll take that as something\nlike: if you wound up on a cliff, you were likely in a very \"cliffy\"\narea of the loss landscape.  \"Teleporting\" randomly to somewhere some distance away\nwas a sensible way to handle that.</p>\n\n<p>In our situation, even if the area is \"cliffy\" in the direction that one particular\nbatch might push us, we have so many extra dimensions that it may well be that it\nwon't be so bad with the next one.  So just skipping the problematic update -- under\nall of those assumptions -- seems a perfectly reasonable way to handle it.</p>\n\n<h3 id=\"validation\">Validation</h3>\n\n<p>All of this, BTW, made me think back to validation loss.  In our previous training runs,\nwhere we were measuring it just before each checkpoint, its spikes were in general correlated\nwith but not identical to spikes in training loss:</p>\n\n<p><img alt=\"Loss in a run with validation\" src=\"/post-assets/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud/training-run-8xh100m80.png\" title=\"Loss in a run with validation\" /></p>\n\n<p>Now, of course, exploding gradients don't have to be related to high training loss --\nthere's enough non-linearity in there that we can treat them as being completely uncorrelated,\nI think.  But you definitely would expect them to have an effect on validation\nloss if applied.  Disregarding the infinite ones (which were being filtered out anyway),\nthe very high ones that we are now clipping would, in the unclipped baseline\ntrain, seem very likely to have caused validation loss spikes.</p>\n\n<p>So: if I hadn't stripped that out, we would likely have been able to see a clear\ndifference in the validation loss line between clipped and unclipped.  That would have\nbeen useful!</p>\n\n<p>I'm not going to re-introduce it, though.  Best to keep the number of code changes\nto a minimum if I'm trying to compare like with like over the course of these intervention\ntests.</p>\n\n<h3 id=\"anyway\">Anyway.</h3>\n\n<p>I think that's enough for gradient clipping.  I may come back and do the experiment\nanother time to see what the relative ratios of the different kinds of problematic\ngradients are.  Are there parts of the train where we get lots of them as a percentage (ie.\nwe're somewhere \"cliffy\" in the loss landscape)?  How many infinite gradient vs infinite norm\nvs big-but-not-infinite instances do we have relative to each other, and to normal\ngradient updates?  What do we see if we have validation loss?  And so on.</p>\n\n<p>But for now: gradient clipping definitely helps, and goes on the positive interventions list!</p>\n\n<p>I'm thinking I'll see what happens with switching off dropout next.  That should at\nleast be a bit easier...</p>\n\n<p>Stay tuned!</p>\n\n<p><a href=\"/2026/02/llm-from-scratch-32c-interventions-removing-dropout\">Here's a link to the next post in this series</a>.</p>\n\n<p>[Update: an earlier version of this post used the wrong number for the baseline\nmodel's loss on the test set, which made it look like there was a 0.065 improvement\nin test loss from adding gradient clipping rather than 0.014.  The above has been updated\nto reflect the real numbers, and the conclusion, at least remains the same: gradient\nclipping is good but not amazing.]</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p><a href=\"https://www.youtube.com/watch?v=DdRnMjfVQi0\">Oh my</a>.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-1\" title=\"Jump back to footnote 1 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-2\">\n<p>Technically the L2 norm -- if you used cubes/cube root it would be L3,\nand likewise for the power of four and L4 and so on.  But the L2 is the\none used for gradient clipping.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-2\" title=\"Jump back to footnote 2 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-3\">\n<p>Shades of <a href=\"https://www.goodreads.com/quotes/40705-but-the-plans-were-on-display-on-display-i-eventually\">Douglas Adams</a>, really:</p>\n\n<p>\"But the plans were on display...\"</p>\n\n<p>\"On display? I eventually had to go down to the cellar to find them.\"</p>\n\n<p>“That’s the display department.\"</p>\n\n<p>“With a flashlight.\"</p>\n\n<p>“Ah, well, the lights had probably gone.\"</p>\n\n<p>“So had the stairs.\"</p>\n\n<p>“But look, you found the notice, didn’t you?\"</p>\n\n<p>“Yes,\" said Arthur, “yes I did. It was on display in the bottom of a locked filing cabinet stuck in a disused lavatory with a sign on the door saying ‘Beware of the Leopard.\"&#160;<a class=\"footnoteBackLink\" href=\"#fnref-3\" title=\"Jump back to footnote 3 in the text.\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
  "id": "/2026/02/llm-from-scratch-32b-interventions-gradient-clipping"
}