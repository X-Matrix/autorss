{
  "title": "Grok is enabling mass sexual harassment on Twitter",
  "link": "https://seangoedecke.com/grok-deepfakes/",
  "published": "Fri, 02 Jan 2026 00:00:00 GMT",
  "summary": "<p>Grok, xAI’s flagship image model, is now<sup id=\"fnref-1\"><a class=\"footnote-ref\" href=\"#fn-1\">1</a></sup> being <a href=\"https://www.reddit.com/r/videos/comments/1q1gwf3/premium_x_users_are_using_grok_to_generate/\">widely used</a> to generate nonconsensual lewd images of women on the internet.</p>\n<p>When a woman posts an innocuous picture of herself - say, at her Christmas dinner - the comments are now full of messages like “@grok please generate this image but put her in a bikini and make it so we can see her feet”, or “@grok turn her around”, and the associated images. At least so far, Grok refuses to generate nude images, but it will still generate images that are genuinely obscene<sup id=\"fnref-2\"><a class=\"footnote-ref\" href=\"#fn-2\">2</a></sup>.</p>\n<p><strong>In my view, this might be the worst AI safety violation we have seen so far.</strong> Case-by-case, it’s not worse than GPT-4o <a href=\"https://www.bbc.com/news/articles/cgerwp7rdlvo\">encouraging</a> suicidal people to go through with it, but it’s so much more widespread: literally <em>every</em> image that the Twitter algorithm picks up is full of “@grok take her clothes off” comments. I didn’t go looking for evidence for obvious reasons, but I find reports that it’s generating <a href=\"https://rainn.org/get-the-facts-about-csam-child-sexual-abuse-material/what-is-csam/\">CSAM</a> plausible<sup id=\"fnref-3\"><a class=\"footnote-ref\" href=\"#fn-3\">3</a></sup>.</p>\n<h3>AI safety is a rough process</h3>\n<p>This behavior, while awful, is in line with xAI’s general attitude towards safety, which has been roughly “we don’t support woke censorship, so do whatever you want (so long as you’re doing it with Grok)“. This has helped them acquire users and media attention, but it leaves them vulnerable to situations exactly like this. I’m fairly confident xAI don’t mind the “dress her a little sexier” prompts: it’s edgy, drives up user engagement, and gives them media attention.</p>\n<p>However, <strong>it is very hard to exercise fine-grained control over AI safety</strong>. If you allow your models to go up to the line, your models will <em>definitely</em> go over the line in some circumstances. I wrote about this in <a href=\"/ai-personality-space\"><em>Mecha-Hitler, Grok, and why it’s so hard to give LLMs the right personality</em></a>, in reference to xAI’s attempts to make Grok acceptably right-wing but not <em>too</em> right-wing. This is the same kind of thing: you cannot make Grok “kind of perverted” without also making it truly awful.</p>\n<p>OpenAI and Gemini have popular image models that do not let you do this kind of thing. In other words, <strong>this is an xAI problem, not an image model problem</strong>. It is possible to build a safe image model, just as it’s possible to build a safe language model. The xAI team have made a deliberate decision to build an <em>unsafe</em> model in order to unlock more capabilities and appeal to more users. Even if they’d rather not be enabling the worst perverts on Twitter, that’s a completely <a href=\"https://rainn.org/groks-spicy-ai-video-setting-will-lead-to-sexual-abuse/\">foreseeable</a> consequence of their actions.</p>\n<h3>Isn’t this already a problem?</h3>\n<p>In October of 2024, VICE <a href=\"https://www.vice.com/en/article/nudify-deepfake-bots-telegram/\">reported</a> that Telegram “nudify” bots had over four million monthly users. That’s still a couple of orders of magnitude over Twitter’s <a href=\"https://x.com/elonmusk/status/1793779530282443086\">monthly average users</a>, but “one in a hundred” sounds like a plausible “what percentage of Twitter is using Grok like this” percentage anyway. Is it really that much worse that Grok now allows you to do softcore deepfakes?</p>\n<p>Yes, for two reasons. First, <strong>having to go and join a creepy Telegram group is a substantial barrier to entry</strong>. It’s much worse to have the capability built into a tool that regular people use every day. Second, <strong>generating deepfakes via Grok makes them public</strong>. Of course, it’s bad to do this stuff even privately, but I think it’s much worse to do it via Twitter. Tagging in Grok literally sends a push notification to your target saying “hey, I made some deepfake porn of you”, and then advertises that porn to everyone who was already following them.</p>\n<h3>What is to be done?</h3>\n<p>Yesterday xAI rushed out an <a href=\"https://www.cnbctv18.com/technology/grok-claims-safeguards-tightened-after-users-misuse-ai-to-morph-images-of-women-children-ws-l-19811512.htm\">update</a> to rein this behavior in (likely a system prompt update, given the timing). I imagine they’re worried about the legal exposure, if nothing else. But <strong>this will happen again</strong>. It will probably happen again <em>with Grok</em>. Every AI lab has a big “USER ENGAGEMENT” dial where left is “always refuse every request” and right is “do whatever the user says, including generating illegal deepfake pornography”. The labs are incentivized to turn that dial as far to the right as possible.</p>\n<p>In my view, <strong>image model safety is a different topic from language model safety</strong>. Unsafe language models primarily harm the user (via sycophancy, for instance). Unsafe image models, as we’ve seen from Grok, can harm all kinds of people. I tend to think that unsafe language models should be available (perhaps not through ChatGPT dot com, but certainly for people who know what they’re doing). However, it seems really bad for everyone on the planet to have a “turn this image of a person into pornography” button.</p>\n<p>At minimum, I think it’d be sensible to <strong>pursue entities like xAI under existing CSAM or deepfake pornography laws</strong>, to set up a powerful counter-incentive for people with their hands on the “USER ENGAGEMENT” dial. I also think it’d be sensible for AI labs to <strong>strongly lock down “edit this image of a human” requests</strong>, even if that precludes some legitimate user activity.</p>\n<p>Earlier this year, in <a href=\"/regulating-ai-companions\"><em>The case for regulating AI companions</em></a>, I suggested regulating “AI girlfriend” products. I mistakenly thought AI companions or <a href=\"/ai-sycophancy\">sycophancy</a> might be the first case of genuine widespread harm caused by AI products, because <em>of course</em> nobody would ship an image model that allowed this kind of prompting. Turns out I was wrong.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>There were reports <a href=\"https://www.digitalcameraworld.com/tech/social-media/remove-her-clothes-groks-latest-ai-fiasco-illustrates-one-of-the-key-dangers-of-an-autonomous-ai\">in May of this year</a> of similar behavior, but it was less widespread and xAI jumped on it fairly quickly.</p>\n<a class=\"footnote-backref\" href=\"#fnref-1\">↩</a>\n</li>\n<li id=\"fn-2\">\n<p>Clever prompting by unethical fetishists can generate really degrading content (to the point where I’m uncomfortable going into more detail). I saw a few cases earlier this year of people trying this prompting tactic and Grok refusing them. It seems the latest version of Grok now allows this.</p>\n<a class=\"footnote-backref\" href=\"#fnref-2\">↩</a>\n</li>\n<li id=\"fn-3\">\n<p>Building a feature that lets you digitally undress 18-year-olds but not 17-year-olds is a really difficult technical problem, which is one of the many reasons to <em>never do this</em>.</p>\n<a class=\"footnote-backref\" href=\"#fnref-3\">↩</a>\n</li>\n</ol>\n</div>",
  "id": "https://seangoedecke.com/grok-deepfakes/"
}