{
  "title": "Porting MiniJinja to Go With an Agent",
  "link": "https://lucumr.pocoo.org/2026/1/14/minijinja-go-port/",
  "published": "2026-01-14T00:00:00+00:00",
  "summary": "<p>Turns out you can just port things now.  I already attempted this experiment in\nthe summer, but it turned out to be a bit too much for what I had time for.\nHowever, things have advanced since.  Yesterday I ported\n<a href=\"https://github.com/mitsuhiko/minijinja\">MiniJinja</a> (a Rust Jinja2 template\nengine) to native Go, and I used an agent to do pretty much all of the work.  In\nfact, I barely did anything beyond giving some high-level guidance on how I\nthought it could be accomplished.</p>\n<p>In total I probably spent around 45 minutes actively with it.  It worked for\naround 3 hours while I was watching, then another 7 hours alone.  This post is a\nrecollection of what happened and what I learned from it.</p>\n<p>All prompting was done by voice using <a href=\"https://buildwithpi.ai/\">pi</a>, starting\nwith Opus 4.5 and switching to GPT-5.2 Codex for the long tail of test fixing.</p>\n<ul>\n<li><a href=\"https://github.com/mitsuhiko/minijinja/pull/854\">PR #854</a></li>\n<li><a href=\"https://shittycodingagent.ai/session/?29f75b708237ceead8b1c8cb55ea2305\">Pi session transcript</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=rqzY8Adxxns\">Narrated video of the porting session</a></li>\n</ul>\n<h2>What is MiniJinja</h2>\n<p>MiniJinja is a re-implementation of Jinja2 for Rust. I originally wrote it\nbecause I wanted to do a infrastructure automation project in Rust and Jinja was\npopular for that.  The original project didn&#8217;t go anywhere, but MiniJinja itself\ncontinued being useful for both me and other users.</p>\n<p>The way MiniJinja is tested is with snapshot tests: inputs and expected outputs,\nusing <a href=\"https://insta.rs/\">insta</a> to verify they match.  These snapshot tests were\nwhat I wanted to use to validate the Go port.</p>\n<h2>Test-Driven Porting</h2>\n<p>My initial prompt asked the agent to figure out how to validate the port.\nThrough that conversation, the agent and I aligned on a path: reuse the existing\nRust snapshot tests and port incrementally (lexer -&gt; parser -&gt; runtime).</p>\n<p>This meant the agent built Go-side tooling to:</p>\n<ul>\n<li>Parse Rust&#8217;s test input files (which embed settings as JSON headers).</li>\n<li>Parse the reference insta <code>.snap</code> snapshots and compare output.</li>\n<li>Maintain a skip-list to temporarily opt out of failing tests.</li>\n</ul>\n<p>This resulted in a pretty good harness with a tight feedback loop.  The agent had\na clear goal (make everything pass) and a progression (lexer -&gt; parser -&gt;\nruntime).  The tight feedback loop mattered particularly at the end where it was\nabout getting details right.  Every missing behavior had one or more failing\nsnapshots.</p>\n<h2>Branching in Pi</h2>\n<p>I used Pi&#8217;s branching feature to structure the session into phases.  I rewound\nback to earlier parts of the session and used the branch switch feature to\ninform the agent automatically what it had already done.  This is similar to\ncompaction, but Pi shows me what it puts into the context.  When Pi switches\nbranches it does two things:</p>\n<ol>\n<li>It stays in the same session so I can navigate around, but it makes a new\nbranch off an earlier message.</li>\n<li>When switching, it adds a summary of what it did as a priming message into\nwhere it branched off.  I found this quite helpful to avoid the agent doing\nvision quests from scratch to figure out how far it had already gotten.</li>\n</ol>\n<p>Without switching branches, I would probably just make new sessions and have\nmore plan files lying around or use something like Amp&#8217;s handoff feature which\nalso allows the agent to consult earlier conversations if it needs more\ninformation.</p>\n<h2>First Signs of Divergence</h2>\n<p>What was interesting is that the agent went from literal porting to behavioral\nporting quite quickly.  I didn&#8217;t steer it away from this as long as the behavior\naligned.  I let it do this for a few reasons.  First, the code base isn&#8217;t that\nlarge, so I felt I could make adjustments at the end if needed.  Letting the\nagent continue with what was already working felt like the right strategy.\nSecond, it was aligning to idiomatic Go much better this way.</p>\n<p>For instance, on the runtime it implemented a tree-walking interpreter (not a\nbytecode interpreter like Rust) and it decided to use Go&#8217;s reflection for the\nvalue type.  I didn&#8217;t tell it to do either of these things, but they made more\nsense than replicating my Rust interpreter design, which was partly motivated by\nnot having a garbage collector or runtime type information.</p>\n<h2>Where I Had to Push Back</h2>\n<p>On the other hand, the agent made some changes while making tests pass that I\ndisagreed with.  It completely gave up on all the &#8220;must fail&#8221; tests because the\nerror messages were impossible to replicate perfectly given the runtime\ndifferences.  So I had to steer it towards fuzzy matching instead.</p>\n<p>It also wanted to regress behavior I wanted to retain (e.g., exact HTML escaping\nsemantics, or that <code>range</code> must return an iterator).  I think if I hadn&#8217;t steered\nit there, it might not have made it to completion without going down problematic\npaths, or I would have lost confidence in the result.</p>\n<h2>Grinding to Full Coverage</h2>\n<p>Once the major semantic mismatches were fixed, the remaining work was filling\nin all missing pieces: missing filters and test functions, loop extras, macros,\ncall blocks, etc.  Since I wanted to go to bed, I switched to Codex 5.2 and\nqueued up a few &#8220;continue making all tests pass if they are not passing yet&#8221;\nprompts, then let it work through compaction.  I felt confident enough that the\nagent could make the rest of the tests pass without guidance once it had the\nbasics covered.</p>\n<p>This phase ran without supervision overnight.</p>\n<h2>Final Cleanup</h2>\n<p>After functional convergence, I asked the agent to document internal functions\nand reorganize (like moving filters to a separate file).  I also asked it to\ndocument all functions and filters like in the Rust code base.  This was also\nwhen I set up CI, release processes, and talked through what was created to come\nup with some finalizing touches before merging.</p>\n<h2>Parting Thoughts</h2>\n<p>There are a few things I find interesting here.</p>\n<p>First: these types of ports are possible now.  I know porting was already\npossible for many months, but it required much more attention.  This changes some\ndynamics.  I feel less like technology choices are constrained by ecosystem lock-in.\nSure, porting NumPy to Go would be a more involved undertaking, and getting it\ncompetitive even more so (years of optimizations in there).  But still, it feels\nlike many more libraries can be used now.</p>\n<p>Second: for me, the value is shifting from the code to the tests and\ndocumentation.  A good test suite might actually be worth more than the code.\nThat said, this isn&#8217;t an argument for keeping tests secret &#8212; generating tests\nwith good coverage is also getting easier.  However, for keeping code bases in\ndifferent languages in sync, you need to agree on shared tests, otherwise\ndivergence is inevitable.</p>\n<p>Lastly, there&#8217;s the social dynamic.  Once, having people port your code to other\nlanguages was something to take pride in.  It was a sign of accomplishment &#8212; a\nproject was &#8220;cool enough&#8221; that someone put time into making it available\nelsewhere.  With agents, it doesn&#8217;t invoke the same feelings.  Will McGugan\n<a href=\"https://bsky.app/profile/willmcgugan.bsky.social/post/3mccn3l4qdk26\">also called out this\nchange</a>.</p>\n<h2>Session Stats</h2>\n<p>Lastly, some boring stats for the main session:</p>\n<ul>\n<li>Agent run duration: <del>10 hours (</del>3 hours supervised)</li>\n<li>Active human time: ~45 minutes</li>\n<li>Total messages: 2,698</li>\n<li>My prompts: 34</li>\n<li>Tool calls: 1,386</li>\n<li>Raw API token cost: $60</li>\n<li>Total tokens: 2.2 million</li>\n<li>Models: <code>claude-opus-4-5</code> and <code>gpt-5.2-codex</code> for the unattended overnight run</li>\n</ul>\n<p>This did not count the adding of doc strings and smaller fixups.</p>",
  "id": "https://lucumr.pocoo.org/2026/1/14/minijinja-go-port/"
}