{
  "title": "Writing an LLM from scratch, part 28 -- training a base model from scratch on an RTX 3090",
  "link": "https://www.gilesthomas.com/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch",
  "published": "Tue, 02 Dec 2025 18:15:00 +0000",
  "summary": "<p>Having worked through the main body of <a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\",\nI wanted to try an experiment: is it possible to train a base model of my\nown, on my own hardware?</p>\n\n<p>The book shows you how to train your LLM, does a basic training run\non a small dataset, and then we switch to downloading the \"pre-cooked\" weights\nfrom OpenAI.  That makes sense given that not every reader will have access to enough\nhardware to really train from scratch.  And right back at\n<a href=\"/2024/12/llm-from-scratch-1\">the start of this series</a>, I did some naive scaling of\nnumbers I'd got when fine-tuning LLMs and came to the conclusion that it would be\nimpossible in a reasonable time.</p>\n\n<p>But the speed I got with my RTX 3090 on the book's small training run made me\nthink that perhaps --\njust perhaps! -- it might actually be possible to train a model of this size -- about\n163M parameters -- on my own hardware.  Not, perhaps, on a small laptop, but at least on\na reasonably high-end \"gaming\" PC.</p>\n\n<p>Additionally, Andrej Karpathy recently announced <a href=\"https://github.com/karpathy/nanochat\">nanochat</a>,\n\"the best ChatGPT that $100 can buy\".  He mentions on the main page that he's trained\na model called <code>d32</code>, with 32 Transformer layers, which has 1.9B parameters, for about $800.\nHis smaller 20-layer <code>d20</code> model, with 561M parameters, he says should be trainable\nin about four hours on an 8x H100 GPU node, which costs about $24/hour -- hence the\n$100 total price.</p>\n\n<p>What's even more interesting about nanochat is that it's built with PyTorch; initially\nI'd got the impression that it was based on his pure C/CUDA <a href=\"https://github.com/karpathy/llm.c\"><code>llm.c</code></a>,\nwhich I would imagine would give a huge speedup.  But no -- he's using the same stack\nas I have been in this series!</p>\n\n<p>Karpathy's models are both larger than 163M parameters, so it definitely sounded like this might be doable.  Obviously, I'm nowhere near as experienced an AI developer,\nand he's using a larger machine (8 GPUs and each of them has &gt; 3x more VRAM than mine),\nbut he's also including the time to train a tokeniser and instruction fine-tune\ninto that four hours -- and his smaller model is more than three times larger than mine.  So that should all\nhelp.</p>\n\n<p>This post is a little less structured than the others in my LLM from scratch series,\nas it's essentially a tidied version of the notes I kept as I worked through the\nproject.</p>\n\n<p>But so as not to bury the lede: using the Hugging Face FineWeb-series datasets,\nI was able to train a GPT-2 small sized\nbase model to a level where it was almost as good as the original in just over 48\nhours on my own hardware!  Base models: not just for the big AI labs.</p>\n\n<p>Here's the full story.</p>\n<h3 id=\"the-model\">The model</h3>\n\n<p>For this project, I want to use the exact same model code as Raschka presented in the\nLLM from scratch book -- <a href=\"https://github.com/gpjt/llm-from-scratch/blob/main/gpt.py\">my copy here</a>.\nThere have been <a href=\"https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the\">a number of architectural improvements</a>\nto LLMs since GPT-2, but for now it's best to keep things simple.</p>\n\n<p>But there are still some settings to decide on.  The config dictionary for the\nmodels we've been using has these parameters:</p>\n\n<ul>\n<li><code>vocab_size</code>.  This is determined by the tokenizer, and I want to use the GPT-2 one, so\nit will need to be <code>50257</code>.</li>\n<li><code>context_length</code>.  GPT-2 has a 1,024-token context length, so I'll stick with that.</li>\n<li><code>emb_dim</code>, <code>n_heads</code>, <code>n_layers</code> --- these define which of the different GPT-2 model\nclasses we're training, and I want to stick to the smallest <code>gpt2-small</code> one, so\nthey will be <code>768</code>, <code>12</code> and <code>12</code> respectively</li>\n<li><code>drop_rate</code>.  One of the most surprising things to me in the \"architectural improvements\" post\nlinked above was that dropout is no longer used so much.  However, this appears to be\ntied in to the one-epoch training that has taken off since GPT-2, so I think it\nwould be best to stick to <code>0.1</code> here.</li>\n<li><code>qkv_bias</code>.  From what Raschka says in the book, this doesn't add on much value, even though\nthe original GPT-2 used it, so let's set it to <code>False</code>.</li>\n</ul>\n\n<p>There's also the aspect of weight-tying -- the original GPT-2 reused its embedding\nmatrix as the weights for the linear layer that <a href=\"/2025/05/llm-from-scratch-15-from-context-vectors-to-logits\">projects the context vectors from\nthe last Transformers layer into vocab space to get the logits</a>.</p>\n\n<p>There's nothing in the code we've been working with to enforce that, though -- when\nwe do our small train in the book, we're using independent weights for each of those\nsteps.  The only time it is \"enforced\" is when we download the pretrained weights\nfrom OpenAI, where we put the same values into both the embedding matrix and the final\noutput head.</p>\n\n<p>Given that Raschka says that it's in general better to avoid weight-tying, and actually doing\nit would be harder than not doing it, then it seems a no-brainer to not do it.</p>\n\n<p>So, what does that mean about our model?</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span> <span class=\"n\">big_train_params</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;vocab_size&quot;</span><span class=\"p\">:</span> <span class=\"mi\">50257</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;context_length&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1024</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;emb_dim&quot;</span><span class=\"p\">:</span> <span class=\"mi\">768</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;n_heads&quot;</span><span class=\"p\">:</span> <span class=\"mi\">12</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;n_layers&quot;</span><span class=\"p\">:</span> <span class=\"mi\">12</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;drop_rate&quot;</span><span class=\"p\">:</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;qkv_bias&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"p\">}</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">gpt</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">GPTModel</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">GPTModel</span><span class=\"p\">(</span><span class=\"n\">big_train_params</span><span class=\"p\">)</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">p</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">())</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"mi\">163009536</span>\n</code></pre>\n</div>\n\n<p>That matches what we got when working through the book; 163M parameters.  Can we train it?</p>\n\n<h3 id=\"the-data\">The data</h3>\n\n<p>It seems like every AI project starts with the question \"what data can we use?\"</p>\n\n<p>The original report on GPT-2,\n\"<a href=\"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">Language Models are Unsupervised Multitask Learners</a>\",\nis frustratingly lacking in details.  However, it does say that they trained it on\n\"8 million documents for a total of 40 GB of text\".  Now, <a href=\"https://platform.openai.com/tokenizer\">according to OpenAI</a>,\nit's reasonable to assume roughly four characters per token for typical English\ntext.  So 40 GB of text is ~10 billion tokens.  That data was essentially gathered\nby scraping pages linked from Reddit that had more than three upvotes there, so was\nreasonably high quality.  Can we get something similar?</p>\n\n<p>Conveniently, Hugging Face host a big dataset called <a href=\"https://huggingface.co/datasets/HuggingFaceFW/fineweb\">FineWeb</a>,\nand that has a 10 billion token \"sample\" dataset, randomly selected from the full\n18.5 <em>trillion</em> tokens.  So the sample feels like it's order-of-magnitude right.  And\nwhile reading more about Karpathy's nanochat, I spotted that it uses <a href=\"https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\">FineWeb-Edu</a>,\nwhich is a version of FineWeb that contains \"only the most educational web pages\".</p>\n\n<p>I wrote <a href=\"https://github.com/gpjt/llm-from-scratch/blob/main/download-fineweb-10b.py\">a script to download both of those</a>,\nand kicked it off.  It took about 20 minutes\nfor each one (slow wifi in my study, I was getting &lt; 5MB/s); FineWeb's 10B sample took\nup about 29 GiB, and FineWeb-Edu's about 27 GiB.</p>\n\n<p>Time to take a look at them.  The Hugging Face <a href=\"https://pypi.org/project/datasets/\"><code>datasets</code></a> <code>load_dataset</code> function loads up all of the files\nyou provide, and you can tell it how to split them up into train/validation/test sets.\nThis command just loads up the whole FineWeb one and says \"treat it all as the train split\",\nwhich is good enough for now:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span> <span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">datasets</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">load_dataset</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"n\">fw</span> <span class=\"o\">=</span> <span class=\"n\">load_dataset</span><span class=\"p\">(</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;parquet&quot;</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"n\">data_files</span><span class=\"o\">=</span><span class=\"s2\">&quot;./fineweb/sample/10BT/*.parquet&quot;</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"n\">split</span><span class=\"o\">=</span><span class=\"s2\">&quot;train&quot;</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"p\">)</span>\n<span class=\"n\">Generating</span> <span class=\"n\">train</span> <span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"mi\">14868862</span> <span class=\"n\">examples</span> <span class=\"p\">[</span><span class=\"mi\">01</span><span class=\"p\">:</span><span class=\"mi\">53</span><span class=\"p\">,</span> <span class=\"mf\">130852.34</span> <span class=\"n\">examples</span><span class=\"o\">/</span><span class=\"n\">s</span><span class=\"p\">]</span>\n<span class=\"n\">Loading</span> <span class=\"n\">dataset</span> <span class=\"n\">shards</span><span class=\"p\">:</span> <span class=\"mi\">100</span><span class=\"o\">%|</span><span class=\"err\">███████████████████████████████████████████████████████████████████████████████████████████████████████████</span><span class=\"o\">|</span> <span class=\"mi\">102</span><span class=\"o\">/</span><span class=\"mi\">102</span> <span class=\"p\">[</span><span class=\"mi\">00</span><span class=\"p\">:</span><span class=\"mi\">03</span><span class=\"o\">&lt;</span><span class=\"mi\">00</span><span class=\"p\">:</span><span class=\"mi\">00</span><span class=\"p\">,</span> <span class=\"mf\">31.90</span><span class=\"n\">it</span><span class=\"o\">/</span><span class=\"n\">s</span><span class=\"p\">]</span>\n</code></pre>\n</div>\n\n<p>Yikes.  It took 1 minute, 53 seconds to generate the train split.  However, that appears\nto be a one-off cost -- when I accessed it again later using the same code in a different\nPython session, it just did the second \"Loading dataset shards\" portion, taking three seconds,\nnot the generation of the split.  Presumably it caches it.</p>\n\n<p>Anyway, let's see what's in it:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">fw</span><span class=\"p\">)</span>\n<span class=\"n\">Dataset</span><span class=\"p\">({</span>\n    <span class=\"n\">features</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">'text'</span><span class=\"p\">,</span> <span class=\"s1\">'id'</span><span class=\"p\">,</span> <span class=\"s1\">'dump'</span><span class=\"p\">,</span> <span class=\"s1\">'url'</span><span class=\"p\">,</span> <span class=\"s1\">'date'</span><span class=\"p\">,</span> <span class=\"s1\">'file_path'</span><span class=\"p\">,</span> <span class=\"s1\">'language'</span><span class=\"p\">,</span> <span class=\"s1\">'language_score'</span><span class=\"p\">,</span> <span class=\"s1\">'token_count'</span><span class=\"p\">],</span>\n    <span class=\"n\">num_rows</span><span class=\"p\">:</span> <span class=\"mi\">14868862</span>\n<span class=\"p\">})</span>\n</code></pre>\n</div>\n\n<p>Great, so we have 14,868,862 rows, each of which has various bits of information.  Checking the first one's text:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">]:</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">fw</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s2\">&quot;text&quot;</span><span class=\"p\">][:</span><span class=\"mi\">500</span><span class=\"p\">])</span>\n<span class=\"o\">|</span><span class=\"n\">Viewing</span> <span class=\"n\">Single</span> <span class=\"n\">Post</span> <span class=\"n\">From</span><span class=\"p\">:</span> <span class=\"n\">Spoilers</span> <span class=\"k\">for</span> <span class=\"n\">the</span> <span class=\"n\">Week</span> <span class=\"n\">of</span> <span class=\"n\">February</span> <span class=\"mi\">11</span><span class=\"n\">th</span><span class=\"o\">|</span>\n<span class=\"o\">|</span><span class=\"n\">Lil</span><span class=\"o\">||</span><span class=\"n\">Feb</span> <span class=\"mi\">1</span> <span class=\"mi\">2013</span><span class=\"p\">,</span> <span class=\"mi\">09</span><span class=\"p\">:</span><span class=\"mi\">58</span> <span class=\"n\">AM</span><span class=\"o\">|</span>\n<span class=\"n\">Don</span><span class=\"s1\">'t care about Chloe/Taniel/Jen-Jen. Don'</span><span class=\"n\">t</span> <span class=\"n\">care</span> <span class=\"n\">about</span> <span class=\"n\">Sami</span><span class=\"p\">,</span> <span class=\"n\">really</span><span class=\"p\">,</span> <span class=\"n\">but</span> <span class=\"n\">hoping</span>\n<span class=\"n\">that</span> <span class=\"n\">we</span> <span class=\"n\">get</span> <span class=\"n\">some</span> <span class=\"n\">good</span> <span class=\"s2\">&quot;SAMANTHA GENE!!&quot;</span> <span class=\"n\">Marlena</span> <span class=\"n\">Death</span><span class=\"o\">-</span><span class=\"n\">Stares</span> <span class=\"n\">out</span> <span class=\"n\">of</span> <span class=\"n\">it</span><span class=\"o\">.</span> <span class=\"n\">And</span>\n<span class=\"s2\">&quot;newfound&quot;</span> <span class=\"n\">feelings</span><span class=\"o\">.</span> <span class=\"n\">Please</span><span class=\"o\">.</span> <span class=\"n\">If</span> <span class=\"n\">only</span><span class=\"o\">.</span>\n<span class=\"n\">STEFANO</span><span class=\"err\">!!</span> <span class=\"n\">STEFANO</span><span class=\"p\">,</span> <span class=\"n\">STEFANO</span><span class=\"p\">,</span> <span class=\"n\">STEFANO</span><span class=\"err\">!!!!</span> <span class=\"p\">:</span><span class=\"n\">cheer</span><span class=\"p\">:</span>\n<span class=\"o\">|</span><span class=\"n\">Spoilers</span> <span class=\"k\">for</span> <span class=\"n\">the</span> <span class=\"n\">Week</span> <span class=\"n\">of</span> <span class=\"n\">February</span> <span class=\"mi\">11</span><span class=\"n\">th</span> <span class=\"err\">·</span> <span class=\"n\">DAYS</span><span class=\"p\">:</span> <span class=\"n\">News</span><span class=\"p\">,</span> <span class=\"n\">Spoilers</span> <span class=\"o\">&amp;</span> <span class=\"n\">Discussion</span><span class=\"o\">|</span>\n</code></pre>\n</div>\n\n<p>Well, for FineWeb, that doesn't look particularly \"fine\", but I guess it's better than the\nstuff that Karpathy talked about in\n<a href=\"https://www.dwarkesh.com/p/andrej-karpathy\">his recent interview with Dwarkesh Patel</a>:</p>\n\n<blockquote>\n  <p>When you’re looking at a pre-training dataset in the frontier lab and you\n  look at a random internet document, it’s total garbage. I don't even know how\n  this works at all. It’s [stuff] like stock tickers, symbols, it's a huge amount\n  of slop and garbage from like all the corners of the internet</p>\n</blockquote>\n\n<p>Let's take a look at FineWeb-Edu.</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">8</span><span class=\"p\">]:</span> <span class=\"n\">fw_edu</span> <span class=\"o\">=</span> <span class=\"n\">load_dataset</span><span class=\"p\">(</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;parquet&quot;</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"n\">data_files</span><span class=\"o\">=</span><span class=\"s2\">&quot;./fineweb-edu/sample/10BT/*.parquet&quot;</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"n\">split</span><span class=\"o\">=</span><span class=\"s2\">&quot;train&quot;</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"p\">)</span>\n<span class=\"n\">Generating</span> <span class=\"n\">train</span> <span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"mi\">9672101</span> <span class=\"n\">examples</span> <span class=\"p\">[</span><span class=\"mi\">01</span><span class=\"p\">:</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mf\">104057.34</span> <span class=\"n\">examples</span><span class=\"o\">/</span><span class=\"n\">s</span><span class=\"p\">]</span>\n<span class=\"n\">Loading</span> <span class=\"n\">dataset</span> <span class=\"n\">shards</span><span class=\"p\">:</span> <span class=\"mi\">100</span><span class=\"o\">%|</span><span class=\"err\">█████████████████████████████████████████████████████████████████████████████████████████████████████████████</span><span class=\"o\">|</span> <span class=\"mi\">98</span><span class=\"o\">/</span><span class=\"mi\">98</span> <span class=\"p\">[</span><span class=\"mi\">00</span><span class=\"p\">:</span><span class=\"mi\">02</span><span class=\"o\">&lt;</span><span class=\"mi\">00</span><span class=\"p\">:</span><span class=\"mi\">00</span><span class=\"p\">,</span> <span class=\"mf\">48.62</span><span class=\"n\">it</span><span class=\"o\">/</span><span class=\"n\">s</span><span class=\"p\">]</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">]:</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">fw_edu</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s2\">&quot;text&quot;</span><span class=\"p\">][:</span><span class=\"mi\">500</span><span class=\"p\">])</span>\n<span class=\"n\">The</span> <span class=\"n\">Independent</span> <span class=\"n\">Jane</span>\n<span class=\"n\">For</span> <span class=\"nb\">all</span> <span class=\"n\">the</span> <span class=\"n\">love</span><span class=\"p\">,</span> <span class=\"n\">romance</span> <span class=\"ow\">and</span> <span class=\"n\">scandal</span> <span class=\"ow\">in</span> <span class=\"n\">Jane</span> <span class=\"n\">Austen</span><span class=\"err\">’</span><span class=\"n\">s</span> <span class=\"n\">books</span><span class=\"p\">,</span> <span class=\"n\">what</span> <span class=\"n\">they</span> <span class=\"n\">are</span>\n<span class=\"n\">really</span> <span class=\"n\">about</span> <span class=\"ow\">is</span> <span class=\"n\">freedom</span> <span class=\"ow\">and</span> <span class=\"n\">independence</span><span class=\"o\">.</span> <span class=\"n\">Independence</span> <span class=\"n\">of</span> <span class=\"n\">thought</span> <span class=\"ow\">and</span> <span class=\"n\">the</span>\n<span class=\"n\">freedom</span> <span class=\"n\">to</span> <span class=\"n\">choose</span><span class=\"o\">.</span>\n<span class=\"n\">Elizabeth</span><span class=\"err\">’</span><span class=\"n\">s</span> <span class=\"n\">refusal</span> <span class=\"n\">of</span> <span class=\"n\">Mr</span><span class=\"o\">.</span> <span class=\"n\">Collins</span> <span class=\"n\">offer</span> <span class=\"n\">of</span> <span class=\"n\">marriage</span> <span class=\"n\">showed</span> <span class=\"n\">an</span> <span class=\"n\">independence</span>\n<span class=\"n\">seldom</span> <span class=\"n\">seen</span> <span class=\"ow\">in</span> <span class=\"n\">heroines</span> <span class=\"n\">of</span> <span class=\"n\">the</span> <span class=\"n\">day</span><span class=\"o\">.</span> <span class=\"n\">Her</span> <span class=\"n\">refusal</span> <span class=\"n\">of</span> <span class=\"n\">Mr</span><span class=\"o\">.</span> <span class=\"n\">Darcy</span> <span class=\"k\">while</span> <span class=\"n\">triggered</span> <span class=\"n\">by</span>\n<span class=\"n\">anger</span> <span class=\"n\">showed</span> <span class=\"n\">a</span> <span class=\"n\">level</span> <span class=\"n\">of</span> <span class=\"n\">independence</span> <span class=\"n\">that</span> <span class=\"n\">left</span> <span class=\"n\">him</span> <span class=\"n\">shocked</span> <span class=\"ow\">and</span> <span class=\"n\">stunned</span><span class=\"o\">.</span>\n<span class=\"n\">The</span> <span class=\"n\">freedom</span> <span class=\"n\">she</span> <span class=\"n\">exhibited</span> <span class=\"ow\">in</span> <span class=\"k\">finally</span> <span class=\"n\">accepting</span> <span class=\"n\">him</span> <span class=\"ow\">in</span> <span class=\"n\">direct</span> <span class=\"n\">defiance</span> <span class=\"n\">of</span> <span class=\"n\">Lady</span> <span class=\"n\">Cath</span>\n</code></pre>\n</div>\n\n<p>That looks a lot better!</p>\n\n<p>Now let's take a look at the document lengths in terms of tokens.  There's a\n<code>token_count</code> column, but I don't know which tokeniser that's for, so to be safe we'll\ncalculate it ourselves.</p>\n\n<p>How long would\nit take to tokenise every row in FineWeb 10B to check?  Let's tokenise the first\n10,000 of the 14,868,862 that we have, and see how long that would take -- then we\ncan work out the estimated time for the whole thing.</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">25</span><span class=\"p\">]:</span> <span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">tiktoken</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">26</span><span class=\"p\">]:</span> <span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">time</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">27</span><span class=\"p\">]:</span> <span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">tiktoken</span><span class=\"o\">.</span><span class=\"n\">get_encoding</span><span class=\"p\">(</span><span class=\"s2\">&quot;gpt2&quot;</span><span class=\"p\">)</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">28</span><span class=\"p\">]:</span> <span class=\"n\">start</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"k\">for</span> <span class=\"n\">entry</span> <span class=\"ow\">in</span> <span class=\"n\">fw</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10_000</span><span class=\"p\">)):</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">entry</span><span class=\"p\">[</span><span class=\"s2\">&quot;text&quot;</span><span class=\"p\">])</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">end</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">29</span><span class=\"p\">]:</span> <span class=\"n\">end</span> <span class=\"o\">-</span> <span class=\"n\">start</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">29</span><span class=\"p\">]:</span> <span class=\"mf\">1.4528205394744873</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">30</span><span class=\"p\">]:</span> <span class=\"n\">fw</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">30</span><span class=\"p\">]:</span>\n<span class=\"n\">Dataset</span><span class=\"p\">({</span>\n    <span class=\"n\">features</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">'text'</span><span class=\"p\">,</span> <span class=\"s1\">'id'</span><span class=\"p\">,</span> <span class=\"s1\">'dump'</span><span class=\"p\">,</span> <span class=\"s1\">'url'</span><span class=\"p\">,</span> <span class=\"s1\">'date'</span><span class=\"p\">,</span> <span class=\"s1\">'file_path'</span><span class=\"p\">,</span> <span class=\"s1\">'language'</span><span class=\"p\">,</span> <span class=\"s1\">'language_score'</span><span class=\"p\">,</span> <span class=\"s1\">'token_count'</span><span class=\"p\">],</span>\n    <span class=\"n\">num_rows</span><span class=\"p\">:</span> <span class=\"mi\">14868862</span>\n<span class=\"p\">})</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">31</span><span class=\"p\">]:</span> <span class=\"p\">(</span><span class=\"mi\">14868862</span> <span class=\"o\">/</span> <span class=\"mi\">10_000</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">1.4528205394744873</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">31</span><span class=\"p\">]:</span> <span class=\"mf\">2160.1788112211702</span>\n</code></pre>\n</div>\n\n<p>2,160 seconds or about 36 minutes.  Yikes!</p>\n\n<p>After a bit of digging, though, I found that <code>tiktoken</code> tokenisers can handle batches\n(poorly documented, but it's there <a href=\"https://github.com/openai/tiktoken/blob/97e49cb/tiktoken/core.py#L175#L175\">in the source</a>):</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">45</span><span class=\"p\">]:</span> <span class=\"n\">text_batch</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">&quot;a&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;b&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;c&quot;</span><span class=\"p\">]</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">46</span><span class=\"p\">]:</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">encode_batch</span><span class=\"p\">(</span><span class=\"n\">text_batch</span><span class=\"p\">)</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">46</span><span class=\"p\">]:</span> <span class=\"p\">[[</span><span class=\"mi\">64</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">65</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">66</span><span class=\"p\">]]</span>\n</code></pre>\n</div>\n\n<p>Also, we can map a function over an entire HF dataset, and that can be made to run\nwith multiple processes.  So, we can combine the two:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">47</span><span class=\"p\">]:</span> <span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">os</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">53</span><span class=\"p\">]:</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">add_len</span><span class=\"p\">(</span><span class=\"n\">examples</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"n\">texts</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">t</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;&quot;</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">examples</span><span class=\"p\">[</span><span class=\"s2\">&quot;text&quot;</span><span class=\"p\">]]</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">encode_batch</span><span class=\"p\">(</span><span class=\"n\">texts</span><span class=\"p\">,</span> <span class=\"n\">disallowed_special</span><span class=\"o\">=</span><span class=\"p\">())</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s2\">&quot;tok_len&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">tokens</span><span class=\"p\">]}</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">54</span><span class=\"p\">]:</span> <span class=\"n\">start</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">fw_with_len</span> <span class=\"o\">=</span> <span class=\"n\">fw</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"n\">add_len</span><span class=\"p\">,</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"n\">batched</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">,</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"n\">num_proc</span><span class=\"o\">=</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">cpu_count</span><span class=\"p\">(),</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"p\">)</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">end</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n<span class=\"n\">Map</span> <span class=\"p\">(</span><span class=\"n\">num_proc</span><span class=\"o\">=</span><span class=\"mi\">24</span><span class=\"p\">):</span> <span class=\"mi\">100</span><span class=\"o\">%|</span><span class=\"err\">████████████████████████████████████████████████████████████████████████████████████████████</span><span class=\"o\">|</span> <span class=\"mi\">14868862</span><span class=\"o\">/</span><span class=\"mi\">14868862</span> <span class=\"p\">[</span><span class=\"mi\">03</span><span class=\"p\">:</span><span class=\"mi\">15</span><span class=\"o\">&lt;</span><span class=\"mi\">00</span><span class=\"p\">:</span><span class=\"mi\">00</span><span class=\"p\">,</span> <span class=\"mf\">75869.33</span> <span class=\"n\">examples</span><span class=\"o\">/</span><span class=\"n\">s</span><span class=\"p\">]</span>\n</code></pre>\n</div>\n\n<p>Just over three minutes, not too bad!  (The reason the command count\nabove jumps from 47 to 53 was that in the first run I didn't have the\n<code>disallowed_special=()</code> in there -- one of the rows in the dataset had <code>&lt;|endoftext|&gt;</code> in\nit, and the tokenizer rejected it.  I'm going to play fast and loose and ignore that for now.)</p>\n\n<p>Now let's see how it added it:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">56</span><span class=\"p\">]:</span> <span class=\"n\">fw_with_len</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">56</span><span class=\"p\">]:</span> <span class=\"n\">dict_keys</span><span class=\"p\">([</span><span class=\"s1\">'text'</span><span class=\"p\">,</span> <span class=\"s1\">'id'</span><span class=\"p\">,</span> <span class=\"s1\">'dump'</span><span class=\"p\">,</span> <span class=\"s1\">'url'</span><span class=\"p\">,</span> <span class=\"s1\">'date'</span><span class=\"p\">,</span> <span class=\"s1\">'file_path'</span><span class=\"p\">,</span> <span class=\"s1\">'language'</span><span class=\"p\">,</span> <span class=\"s1\">'language_score'</span><span class=\"p\">,</span> <span class=\"s1\">'token_count'</span><span class=\"p\">,</span> <span class=\"s1\">'tok_len'</span><span class=\"p\">])</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">57</span><span class=\"p\">]:</span> <span class=\"n\">fw_with_len</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s2\">&quot;tok_len&quot;</span><span class=\"p\">]</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">57</span><span class=\"p\">]:</span> <span class=\"mi\">142</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">58</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">fw_with_len</span><span class=\"p\">[</span><span class=\"s2\">&quot;tok_len&quot;</span><span class=\"p\">])</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">58</span><span class=\"p\">]:</span> <span class=\"mi\">14868862</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">59</span><span class=\"p\">]:</span> <span class=\"n\">fw_with_len</span><span class=\"p\">[</span><span class=\"s2\">&quot;tok_len&quot;</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">59</span><span class=\"p\">]:</span> <span class=\"mi\">142</span>\n</code></pre>\n</div>\n\n<p>Cool!  We've added a <code>tok_len</code> column with the number of GPT-2 tokens for each row, and we\ncan extract what amounts to a list of those values.  Let's plot them as a histogram.</p>\n\n<p>Trying to do it directly -- that is, just doing</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">hist</span><span class=\"p\">(</span><span class=\"n\">fw_with_len</span><span class=\"p\">[</span><span class=\"s2\">&quot;tok_len&quot;</span><span class=\"p\">],</span> <span class=\"n\">bins</span><span class=\"o\">=</span><span class=\"n\">bins</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...seems to make MatPlotLib very unhappy, and my interpreter crashed with an OOM -- I think it might be trying to load all\nof the dataset -- text, IDs, etc -- into RAM in one go.</p>\n\n<p>So I started a fresh one and did the stuff to load it and annotate it with token lengths\nagain -- weirdly, this time the mapping only took 10 seconds or so!  That was strange,\nI'll need to look into that.  Perhaps the earlier command added the <code>tok_len</code> column to the files on\ndisk?</p>\n\n<p>To work around the memory issue, I converted the <code>tok_len</code> column from the dataset to an actual list:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">11</span><span class=\"p\">]:</span> <span class=\"n\">lengths</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">n</span> <span class=\"k\">for</span> <span class=\"n\">n</span> <span class=\"ow\">in</span> <span class=\"n\">fw_with_len</span><span class=\"p\">[</span><span class=\"s2\">&quot;tok_len&quot;</span><span class=\"p\">]]</span>\n</code></pre>\n</div>\n\n<p>That took ten or twenty seconds.  Let's then try the plot again (full code this time):</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">19</span><span class=\"p\">]:</span> <span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">bins</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2048</span> <span class=\"o\">+</span> <span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">)</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xkcd</span><span class=\"p\">()</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">rcParams</span><span class=\"p\">[</span><span class=\"s1\">'font.family'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;xkcd&quot;</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">fig</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">))</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">gca</span><span class=\"p\">()</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">hist</span><span class=\"p\">(</span><span class=\"n\">lengths</span><span class=\"p\">,</span> <span class=\"n\">bins</span><span class=\"o\">=</span><span class=\"n\">bins</span><span class=\"p\">)</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s2\">&quot;TOKENIZED LENGTH (GPT-2 TOKENS)&quot;</span><span class=\"p\">)</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s2\">&quot;COUNT&quot;</span><span class=\"p\">)</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s2\">&quot;FINEWEB DISTRIBUTION OF TOKENIZED LENGTHS&quot;</span><span class=\"p\">)</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">mean_len</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">lengths</span><span class=\"p\">))</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">median_len</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">median</span><span class=\"p\">(</span><span class=\"n\">lengths</span><span class=\"p\">))</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">h_mean</span> <span class=\"o\">=</span> <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">axvline</span><span class=\"p\">(</span><span class=\"n\">mean_len</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s2\">&quot;--&quot;</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">&quot;MEAN = </span><span class=\"si\">{</span><span class=\"n\">mean_len</span><span class=\"si\">:</span><span class=\"s2\">.1f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">h_med</span>  <span class=\"o\">=</span> <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">axvline</span><span class=\"p\">(</span><span class=\"n\">median_len</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s2\">&quot;:&quot;</span><span class=\"p\">,</span>  <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">&quot;MEDIAN = </span><span class=\"si\">{</span><span class=\"n\">median_len</span><span class=\"si\">:</span><span class=\"s2\">.1f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">handles</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">h_mean</span><span class=\"p\">,</span> <span class=\"n\">h_med</span><span class=\"p\">])</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"s2\">&quot;y&quot;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">)</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">()</span>\n    <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">savefig</span><span class=\"p\">(</span><span class=\"s2\">&quot;fineweb-token-length-distribution.png&quot;</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>That took about 11s to run, and the result is this:</p>\n\n<p><img alt=\"Histogram of GPT-2 token count across FineWeb samples\" src=\"/post-assets/llm-from-scratch-28-training-a-base-model-from-scratch/fineweb-token-length-distribution.png\" title=\"Histogram of GPT-2 token count across FineWeb samples\" /></p>\n\n<p>That's really promising!  The bulk of them are less than our 1,024 token sequence length. <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup>\nIf we present each row in the dataset as a stand-alone training sample, cropping them\nwhen necessary, perhaps we won't lose too much data?  Let's see.</p>\n\n<p>First step, how many tokens are there in total?</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">20</span><span class=\"p\">]:</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">lengths</span><span class=\"p\">)</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">20</span><span class=\"p\">]:</span> <span class=\"mi\">10336315397</span>\n</code></pre>\n</div>\n\n<p>Nice, about 10B, as expected.  How many tokens would we have if we cropped them to the default GPT-2 context length\nof 1,024?</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">21</span><span class=\"p\">]:</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">l</span> <span class=\"k\">if</span> <span class=\"n\">l</span> <span class=\"o\">&lt;</span> <span class=\"mi\">1024</span> <span class=\"k\">else</span> <span class=\"mi\">1024</span> <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"n\">lengths</span><span class=\"p\">)</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">21</span><span class=\"p\">]:</span> <span class=\"mi\">7354541756</span>\n</code></pre>\n</div>\n\n<p>Ouch, 7.3B. That's quite a reduction:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">22</span><span class=\"p\">]:</span> <span class=\"mi\">7354541756</span> <span class=\"o\">/</span> <span class=\"mi\">10336315397</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">22</span><span class=\"p\">]:</span> <span class=\"mf\">0.7115245107685639</span>\n</code></pre>\n</div>\n\n<p>So we're losing 29% of our tokens by that cropping.  That's from curtailing just\n16% of the sequences:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">26</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">([</span><span class=\"n\">l</span> <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"n\">lengths</span> <span class=\"k\">if</span> <span class=\"n\">l</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1024</span><span class=\"p\">])</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">26</span><span class=\"p\">]:</span> <span class=\"mi\">2438899</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">27</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">lengths</span><span class=\"p\">)</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">27</span><span class=\"p\">]:</span> <span class=\"mi\">14868862</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">28</span><span class=\"p\">]:</span> <span class=\"mi\">2438899</span> <span class=\"o\">/</span> <span class=\"mi\">14868862</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">28</span><span class=\"p\">]:</span> <span class=\"mf\">0.1640272806351959</span>\n</code></pre>\n</div>\n\n<p>That's not great.</p>\n\n<p>I feel that we have two options here:</p>\n\n<ol>\n<li>Crop all of the input sequences -- that is, each row in the dataset -- so that\neach one is no more than our 1,024 sequence length.  Then we can pad them out\nwith end-of-sequence tokens (as is the standard) so that they're all 1,024.  This\nwill lose us quite a lot of tokens, but has the big benefit of being easy.</li>\n<li>Treat the corpus as, essentially, one long document, with end-of-sequence delimiters\nbetween each row, then split that up into 1,024-token sequences.\nDoing it this way would mean we'd\nuse all of our training data.  But it would be more complicated, especially\nif we hit memory constraints.</li>\n</ol>\n\n<p>At this point in the experiment, I'm going to keep both options open.  I'm inclined\ntowards the latter (I believe it's closer to what the real GPT-2 train did), but\nI'm not sure.</p>\n\n<p>Anyway, we're scoping things out here, so let's move on.</p>\n\n<h3 id=\"epochs\">Epochs</h3>\n\n<p>After looking at the data, I've thought a bit more about this.  I'd previously been thinking\nin terms of training across all of the tokens in the dataset; we'd work our way through the 10B\ntokens, and then we'd be done.</p>\n\n<p>But when training a model, you do multiple epochs, normally -- you run through the\ndataset once, updating your gradients as you go, then run through it again likewise,\nand eventually you stop when your validation loss starts rising.</p>\n\n<p>I think that because I'd read that LLMs are normally trained on just one epoch\nthese days, I'd kind of internalised that we only need to do one.  But it wasn't the\ncase in 2019 when GPT-2\ncame out.  They had less data -- just 10B tokens or so, compared to insanely huge\ndatasets like the full FineWeb (not the 10B one we've been looking at -- the 18.5T full one), so they\nwould have trained it for some number of epochs.</p>\n\n<p>How many?  That's another case where the GPT-2 paper is annoyingly light.\n<a href=\"https://wandb.ai/bkkaggle/lm-finetuning/reports/Pretraining-a-124-M-Parameter-GPT-2-Language-Model--VmlldzoyMjg4NzA\">This report</a>\nsays in the \"Replicating GPT-2\" section that OpenAI trained it for 800k iterations with a batch size of 512.  Plugging\nin a sequence length of 1024, that gives us this many tokens:</p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>800</mn><mo>&#x0002c;</mo><mn>000</mn><mi>&#x000d7;</mi><mn>512</mn><mi>&#x000d7;</mi><mn>1</mn><mo>&#x0002c;</mo><mn>024</mn><mo>&#x0003d;</mo><mn>419</mn><mo>&#x0002c;</mo><mn>430</mn><mo>&#x0002c;</mo><mn>400</mn><mo>&#x0002c;</mo><mn>000</mn></mrow></math>\n\n<p>Over 419B tokens!</p>\n\n<p>Now, if we believe that their dataset was 10B tokens, then we can work out how many epochs\nthat came to:</p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>419</mn><mo>&#x0002c;</mo><mn>430</mn><mo>&#x0002c;</mo><mn>400</mn><mo>&#x0002c;</mo><mn>000</mn><mo>&#x0002f;</mo><mn>10</mn><mo>&#x0002c;</mo><mn>000</mn><mo>&#x0002c;</mo><mn>000</mn><mo>&#x0002c;</mo><mn>000</mn><mo>&#x0003d;</mo><mn>41.94</mn></mrow></math>\n\n<p>The same report says that they -- as in, the report authors -- make that \"around a total of 60 epochs through the training set\" --\nI believe that the training set they're talking about could well be slightly shorter than\nthe original GPT-2 one -- the GPT-2 authors didn't release their own, which is called \"WebText\", so the report's\nauthor is using a different one that tries to replicate it, <a href=\"https://skylion007.github.io/OpenWebTextCorpus/\">OpenWebText</a>.</p>\n\n<p>That sounds expensive; even without knowing how many tokens per second we can train\nfor, 40-odd epochs of 10B tokens each sounds like it would take a long time.  Are there\nany other comparison points that might tell us how long to train for?</p>\n\n<p>Well, there's a \"Chinchilla heuristic\" that I've heard of, which says that you should train on about 20 tokens\nper model parameter.  I spent some time reading into where that comes from; originally\nit's in \"<a href=\"https://arxiv.org/abs/2203.15556\">Training Compute-Optimal Large Language Models</a>\"\nfrom Google DeepMind, and it's an interesting paper, and is surprisingly easy to read,\nwith a few bits of maths that get a bit hairy (but aren't required to get a good-enough\nfeel for what they're saying).  I recommend you take a look.</p>\n\n<p>It was written in 2022, and the authors felt that people were scaling up models\na lot, but weren't increasing the number of tokens that they used for training enough.\nSo, they trained a huge number of models, trying to answer the question: \"given a\nparticular budget in training FLOPs, what is the optimal balance of training tokens\nversus parameters to make sure you're using those FLOPs most efficiently?\".  They\nwere arguing against the method taken in a particular paper, where another team had trained a model (called Gopher)\non significantly fewer tokens than they thought optimal.</p>\n\n<p>The number of FLOPs used to train a model is linear with both the number of parameters\nand the number of tokens you train it on, so if you get 2x the number of FLOPs that\nyou had before, you can either train the same model on twice as many tokens, or\nyou can double its size.  Which is better?  Their conclusion was that you should\nactually scale both parameters and tokens up by the same amount -- that is, in the 2x\ncase you'd want to have <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msqrt><mrow><mn>2</mn></mrow></msqrt></mrow></math> times both the parameters and tokens, which\nwould double your FLOPs and get you better performance.</p>\n\n<p>As you can probably see, by doing this they indirectly\nworked out an optimal number of tokens to train a particular size of model for.\nThey don't state the \"20x\" heuristic themselves, but it's pretty clear in table 3\nin the paper, where they give a number of model sizes and the optimal number of tokens\nfor each.</p>\n\n<p>Now, this number is not the number of tokens you need to train for to get the <em>best</em>\nmodel you can for a particular number of parameters; a model of a given size\ncan always be trained more and will (hopefully) get better.  But it tells you when you've\ntrained on enough tokens that you could get better results by training a larger model\nthan you have right now.</p>\n\n<p>They're implicitly assuming\nthat models can get as large as you want, which of course is not the case -- in reality,\nyou're going to be targeting a particular model size, the size that can fit on your\ntraining hardware (or more likely with production models, the size that can fit on\nyour planned inference hardware).</p>\n\n<p>But interestingly, looking at the <a href=\"https://github.com/karpathy/nanochat/blob/master/README.md\">README.md for Karpathy's nanochat</a>\nproject, he trained his 1.9B \"d32\" model on 38B tokens -- exactly 20x.  And\nif you look at the <a href=\"https://github.com/karpathy/nanochat/blob/master/speedrun.sh\"><code>speedrun.sh</code></a>\nscript in the same repo, he explicitly says that he's training for 20x parameters\nfor the smaller <code>d20</code> model:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"c1\"># The d20 model is 561M parameters.</span>\n<span class=\"c1\"># Chinchilla says #tokens = 20X #params, so we need 561e6 * 20 = 11.2B tokens.</span>\n</code></pre>\n</div>\n\n<p>If Andrej Karpathy thinks that training for Chinchilla-optimality is the right way\nto go, then who am I to disagree?  ;-)</p>\n\n<p>More seriously, perhaps the better quality of the dataset makes this a reasonable\nthing to do.  From the GPT-2 paper, their description of how they got the data:</p>\n\n<blockquote>\n  <p>...we created a new web scrape which emphasizes\n  document quality. To do this we only scraped web pages\n  which have been curated/filtered by humans. Manually\n  filtering a full web scrape would be exceptionally expensive\n  so as a starting point, we scraped all outbound links from\n  Reddit, a social media platform, which received at least 3\n  karma. This can be thought of as a heuristic indicator for\n  whether other users found the link interesting, educational,\n  or just funny.</p>\n</blockquote>\n\n<p>That's a clever trick, but I believe that FineWeb is much more carefully filtered and improved\nthan the WebText dataset they got from that.  Back in 2019, they had to do everything from scratch -- find appropriate\nways to get data, filter it, and so on.  Now we can just download stuff from Hugging Face.\nSo maybe Chinchilla-optimal is enough.</p>\n\n<p>Anyway, we have 163,009,536 parameters, so on that basis, let's train for:</p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>163</mn><mo>&#x0002c;</mo><mn>009</mn><mo>&#x0002c;</mo><mn>536</mn><mi>&#x000d7;</mi><mn>20</mn><mo>&#x0003d;</mo><mn>3</mn><mo>&#x0002c;</mo><mn>260</mn><mo>&#x0002c;</mo><mn>190</mn><mo>&#x0002c;</mo><mn>720</mn></mrow></math>\n\n<p>...tokens.  (I'll just use 3.2B from now on, but that's the actual number I mean.)</p>\n\n<p>That's pretty cool!  We have more than that number of tokens already in our\nFineWeb 10B sample, so we can do a single-epoch training run.</p>\n\n<p>So the question is -- is that even doable on my hardware?</p>\n\n<h3 id=\"tokens-per-second\">Tokens per second</h3>\n\n<p>It all hinges on how many tokens per second we can train at.  A good way to check this is to write a throwaway \"trainer\".  We can use that to\nwork out what our maximum batch size on the RTX 3090's 24 GiB of VRAM, then run a bunch\nof batches through -- a forward and backward pass for each -- and see how many\nwe get.</p>\n\n<p>This won't estimate how much time we'll spend validating the model, of course.  But\nmy gut is telling me that we should spend no more than 5% of our training time running\nvalidations, so we can later on do a similar test, eval mode, forward pass only with no gradient\ntracking, and use that to work out how many tokens should be in the validation set.</p>\n\n<p>So, let's estimate training speed.  <a href=\"https://github.com/gpjt/llm-from-scratch/blob/36196755f850adeba348d15e2f4f81e87ad4d14f/measure-tokens-per-second.py\">This code</a>\ngets an estimate of tokens/second at different batch sizes.\nHopefully it's clear enough to not need an in-depth explanation.  An outline:</p>\n\n<ul>\n<li>We load enough GPT-2 tokens from FineWeb for <code>NUM_BATCHES</code> batches of <code>MAX_BATCH_SIZE</code> sequences each,\nevery one of those sequences being <code>SEQ_LENGTH</code> long (plus one extra token for the targets we're\ncomparing them to).  Note that we're not bothering to separate them with anything\nfor this test.</li>\n<li>We then loop over batch sizes from <code>1</code> to <code>MAX_BATCH_SIZE</code>.</li>\n<li>Then we create our model and put it on the CUDA device.  We do this for each\nbatch size rather than creating one and then using it for all of them so that they're all\nstarting from the same point -- the <code>torch.manual_seed</code> should make sure that they're\nidentical.</li>\n<li>For each batch size, we create input and output batches as tensors -- note that\nwe're not putting these on CUDA yet, I wanted to do that in the training loop to\nmirror what a real training loop will have to do.  When we're training with\n3.2B tokens then having them all on CUDA will be a waste of VRAM, so we'll be\npushing a batch there for each iteration.</li>\n<li>We do a stripped-down training loop -- for each batch, put the inputs and outputs\nonto CUDA, then a forward pass, work out the loss, backward pass, and optimiser\nstep.  We do the same <code>NUM_BATCHES</code> iterations per batch size.</li>\n<li>Finally, we print out the number of tokens we trained on for this batch size, how long it took, and the\nnumber of tokens per second.</li>\n</ul>\n\n<p>Here's what it prints out:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Loading dataset shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [00:00&lt;00:00, 362.71it/s]</span>\n<span class=\"go\">Testing with batch size 1</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10&lt;00:00,  9.77it/s]</span>\n<span class=\"go\">Done, trained on 102,400 tokens in 10.2348s.</span>\n<span class=\"go\">Tokens per second: 10,005</span>\n\n<span class=\"go\">Testing with batch size 2</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:17&lt;00:00,  5.60it/s]</span>\n<span class=\"go\">Done, trained on 204,800 tokens in 17.8631s.</span>\n<span class=\"go\">Tokens per second: 11,464</span>\n\n<span class=\"go\">Testing with batch size 3</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:25&lt;00:00,  3.93it/s]</span>\n<span class=\"go\">Done, trained on 307,200 tokens in 25.4152s.</span>\n<span class=\"go\">Tokens per second: 12,087</span>\n\n<span class=\"go\">Testing with batch size 4</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:33&lt;00:00,  3.02it/s]</span>\n<span class=\"go\">Done, trained on 409,600 tokens in 33.1185s.</span>\n<span class=\"go\">Tokens per second: 12,367</span>\n\n<span class=\"go\">Testing with batch size 5</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:40&lt;00:00,  2.46it/s]</span>\n<span class=\"go\">Done, trained on 512,000 tokens in 40.6351s.</span>\n<span class=\"go\">Tokens per second: 12,599</span>\n\n<span class=\"go\">Testing with batch size 6</span>\n<span class=\"go\">  0%|                                                                                                                                             | 0/100 [00:00&lt;?, ?it/s]</span>\n<span class=\"go\">Traceback (most recent call last):</span>\n<span class=\"go\">  File &quot;/home/giles/Dev/llm-from-scratch/measure-tokens-per-second.py&quot;, line 89, in &lt;module&gt;</span>\n<span class=\"go\">    main()</span>\n<span class=\"go\">    ~~~~^^</span>\n<span class=\"go\">...</span>\n<span class=\"go\">torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 23.56 GiB of which 269.19 MiB is free. Including non-PyTorch memory, this process has 20.99 GiB memory in use. Of the allocated memory 18.67 GiB is allocated by PyTorch, and 2.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)</span>\n</code></pre>\n</div>\n\n<p>So we can see that it gets faster as we increase the batch size, which makes sense\nbecause we're handling sequences in parallel, but it does flatten off a bit, which\nmakes sense because there's a limit to how much parallelism we can do, even on a GPU.</p>\n\n<p>Let's see how that fits in with the different training sizes we looked at above:</p>\n\n<ul>\n<li>Chinchilla heuristic, 20x parameters -- 3.2B tokens: 247,850 seconds, which is just less than three days</li>\n<li>Estimated GPT-2 train, 419B tokens: 32,452,947 seconds, which is just over a year.</li>\n</ul>\n\n<p>OK.  We're definitely not going to be able to train this thing the GPT-2 way!  I\nexpected that to be the case, but now we have a solid proof of that.</p>\n\n<p>But the three-day Chinchilla-optimal train actually sounds doable!  I'm heading to London\nto visit family soon, so won't be using my home PC.  With a bit of help from\n<a href=\"https://tailscale.com/\">Tailscale</a> I'll be able to log into it from my laptop, though,\nso I can potentially nurse a run through.</p>\n\n<p>Can we make it any faster?</p>\n\n<p>Now, when doing the fine-tuning work, I found that you could generally speed things\nup by doing everything in 16-bit rather than 32-bit.  Intuitively that makes sense --\nlower-precision numbers, fewer bits, means less work for the GPU doing the various\nmultiplications and additions that are involved in our train.</p>\n\n<p>Working with ChatGPT, I found a couple of ways to take advantage of that.  Firstly,\nusing TF32.</p>\n\n<p>The normal float32 format uses 8 bits for the exponent, and 23 for the mantissa.  If\nyou haven't looked into how floats are represented in memory (or if you've forgotten),\nthat means that, using <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>m</mi></mrow></math> to mean the mantissa and <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>x</mi></mrow></math> the exponent, the numbers are represented\nin memory as</p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>m</mi><mi>&#x000d7;</mi><msup><mn>2</mn><mi>x</mi></msup></mrow></math>\n\n<p>TF32 is messier; it has the same exponent size -- and thus the same range -- as float32, but it essentially ignores\nthe lower 13 bits of the mantissa.  So it takes up the same amount of memory, but is lower-precision,\nwhich means that calculations can be faster.  Most importantly, cards like the RTX 3090\nhave dedicated \"tensor cores\" -- as opposed to the normal CUDA cores that do normal\nmatrix multiplications -- and they operate in TF32.  Unsurprisingly, \"TF32\" is\n\"tensor float 32-bit\".</p>\n\n<p>The PyTorch <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\"><code>set_float32_matmul_precision</code></a>\nallows you to tell it what precision to use for matrix multiplications; the default is\n<code>\"highest\"</code>, which means \"use float32 all of the time\", so you're stuck using just the\nCUDA cores.  If, instead, you set it to\n<code>\"high\"</code>, then it will use TF32 if the hardware supports it and it has the appropriate\nkernels available.  So that will let us use the tensor cores.</p>\n\n<p>I added this to the code above just above the loop over the different batch sizes:</p>\n\n<pre><code>torch.set_float32_matmul_precision(\"high\")\n</code></pre>\n\n<p>Let it run, and:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Testing with batch size 1</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08&lt;00:00, 11.66it/s]</span>\n<span class=\"go\">Done, trained on 102,400 tokens in 8.5799s.</span>\n<span class=\"go\">Tokens per second: 11,934</span>\n\n<span class=\"go\">Testing with batch size 2</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:15&lt;00:00,  6.65it/s]</span>\n<span class=\"go\">Done, trained on 204,800 tokens in 15.0287s.</span>\n<span class=\"go\">Tokens per second: 13,627</span>\n\n<span class=\"go\">Testing with batch size 3</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20&lt;00:00,  4.85it/s]</span>\n<span class=\"go\">Done, trained on 307,200 tokens in 20.6374s.</span>\n<span class=\"go\">Tokens per second: 14,885</span>\n\n<span class=\"go\">Testing with batch size 4</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:27&lt;00:00,  3.61it/s]</span>\n<span class=\"go\">Done, trained on 409,600 tokens in 27.7148s.</span>\n<span class=\"go\">Tokens per second: 14,779</span>\n\n<span class=\"go\">Testing with batch size 5</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:33&lt;00:00,  3.01it/s]</span>\n<span class=\"go\">Done, trained on 512,000 tokens in 33.2420s.</span>\n<span class=\"go\">Tokens per second: 15,402</span>\n</code></pre>\n</div>\n\n<p>That's a 22% speedup!  Of course, the precision of the training isn't as good.  But\ngiven that many modern models are trained at 16-bit (I've seen suggestions that\nsome are even trained as low as 4-bit) then that shouldn't matter.</p>\n\n<p>Let's see whether we can train in 16-bit instead.  PyTorch has a smart mode where\nyou can tell it \"use 16-bit where it makes sense, otherwise use 32-bit\" -- AMP, which\nstands for \"Automatic Mixed Precision\".  <a href=\"https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html\">There's a great recipe for how to use it in the docs</a>,\nso let's use that.  We need to create a <code>Scaler</code> object to handle scaling parameters\nfrom 16-bit to 32-bit as needed -- we can re-use that across all batch sizes\nso we can create it just before the loop:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">scaler</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">GradScaler</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...then we need to replace this core part of our training loop:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>            <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n            <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">cross_entropy</span><span class=\"p\">(</span>\n                <span class=\"n\">logits</span><span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">outputs</span><span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">()</span>\n            <span class=\"p\">)</span>\n            <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n            <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...with some code to use AMP and that scaler -- basically we use a context manager\nto switch it on when we're doing the forward pass and work out the loss, and then use the scaler\nto manage the backward pass and the optimiser's step:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>            <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n                <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n                <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">cross_entropy</span><span class=\"p\">(</span>\n                    <span class=\"n\">logits</span><span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">outputs</span><span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">()</span>\n                <span class=\"p\">)</span>\n            <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n            <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n            <span class=\"n\">scaler</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>Running that gives us these results:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp gp-VirtualEnv\">(llm-from-scratch)</span> <span class=\"gp\">giles@perry:~/Dev/llm-from-scratch (main)$ </span>python<span class=\"w\"> </span>measure-tokens-per-second.py\n<span class=\"go\">Loading dataset shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [00:00&lt;00:00, 340.25it/s]</span>\n<span class=\"go\">Testing with batch size 1</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:07&lt;00:00, 13.38it/s]</span>\n<span class=\"go\">Done, trained on 102,400 tokens in 7.4764s.</span>\n<span class=\"go\">Tokens per second: 13,696</span>\n\n<span class=\"go\">Testing with batch size 2</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12&lt;00:00,  8.11it/s]</span>\n<span class=\"go\">Done, trained on 204,800 tokens in 12.3286s.</span>\n<span class=\"go\">Tokens per second: 16,611</span>\n\n<span class=\"go\">Testing with batch size 3</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16&lt;00:00,  6.02it/s]</span>\n<span class=\"go\">Done, trained on 307,200 tokens in 16.6238s.</span>\n<span class=\"go\">Tokens per second: 18,479</span>\n\n<span class=\"go\">Testing with batch size 4</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21&lt;00:00,  4.67it/s]</span>\n<span class=\"go\">Done, trained on 409,600 tokens in 21.3936s.</span>\n<span class=\"go\">Tokens per second: 19,145</span>\n\n<span class=\"go\">Testing with batch size 5</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:25&lt;00:00,  3.87it/s]</span>\n<span class=\"go\">Done, trained on 512,000 tokens in 25.8624s.</span>\n<span class=\"go\">Tokens per second: 19,797</span>\n\n<span class=\"go\">Testing with batch size 6</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:30&lt;00:00,  3.25it/s]</span>\n<span class=\"go\">Done, trained on 614,400 tokens in 30.7239s.</span>\n<span class=\"go\">Tokens per second: 19,997</span>\n\n<span class=\"go\">Testing with batch size 7</span>\n<span class=\"go\">  0%|                                                                                                                                             | 0/100 [00:00&lt;?, ?it/s]</span>\n<span class=\"go\">Traceback (most recent call last):</span>\n<span class=\"go\">  File &quot;/home/giles/Dev/llm-from-scratch/measure-tokens-per-second.py&quot;, line 94, in &lt;module&gt;</span>\n<span class=\"go\">    main()</span>\n</code></pre>\n</div>\n\n<p>Wow!  With that we can train on 3.2B tokens in about 160,000 seconds, which is 44 hours.\nThat's definitely doable.</p>\n\n<p>Now, what happens if we remove the</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">set_float32_matmul_precision</span><span class=\"p\">(</span><span class=\"s2\">&quot;high&quot;</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...so that we're using AMP, but not the tensor cores?</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp gp-VirtualEnv\">(llm-from-scratch)</span> <span class=\"gp\">giles@perry:~/Dev/llm-from-scratch (main)$ </span>python<span class=\"w\"> </span>measure-tokens-per-second.py\n<span class=\"go\">Loading dataset shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [00:00&lt;00:00, 365.94it/s]</span>\n<span class=\"go\">Testing with batch size 1</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:07&lt;00:00, 13.03it/s]</span>\n<span class=\"go\">Done, trained on 102,400 tokens in 7.6736s.</span>\n<span class=\"go\">Tokens per second: 13,344</span>\n\n<span class=\"go\">Testing with batch size 2</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12&lt;00:00,  8.04it/s]</span>\n<span class=\"go\">Done, trained on 204,800 tokens in 12.4383s.</span>\n<span class=\"go\">Tokens per second: 16,465</span>\n\n<span class=\"go\">Testing with batch size 3</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16&lt;00:00,  5.96it/s]</span>\n<span class=\"go\">Done, trained on 307,200 tokens in 16.7851s.</span>\n<span class=\"go\">Tokens per second: 18,301</span>\n\n<span class=\"go\">Testing with batch size 4</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21&lt;00:00,  4.64it/s]</span>\n<span class=\"go\">Done, trained on 409,600 tokens in 21.5571s.</span>\n<span class=\"go\">Tokens per second: 19,000</span>\n\n<span class=\"go\">Testing with batch size 5</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:25&lt;00:00,  3.85it/s]</span>\n<span class=\"go\">Done, trained on 512,000 tokens in 25.9610s.</span>\n<span class=\"go\">Tokens per second: 19,721</span>\n\n<span class=\"go\">Testing with batch size 6</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:30&lt;00:00,  3.24it/s]</span>\n<span class=\"go\">Done, trained on 614,400 tokens in 30.8405s.</span>\n<span class=\"go\">Tokens per second: 19,921</span>\n\n<span class=\"go\">Testing with batch size 7</span>\n<span class=\"go\">  0%|                                                                                                                                             | 0/100 [00:00&lt;?, ?it/s]</span>\n<span class=\"go\">Traceback (most recent call last):</span>\n<span class=\"go\">  File &quot;/home/giles/Dev/llm-from-scratch/measure-tokens-per-second.py&quot;, line 93, in &lt;module&gt;</span>\n<span class=\"go\">    main()</span>\n<span class=\"go\">    ~~~~^^</span>\n<span class=\"go\">  File &quot;/home/giles/Dev/llm-from-scratch/measure-tokens-per-second.py&quot;, line 81, in main</span>\n</code></pre>\n</div>\n\n<p>It's basically the same.  300tps slower at the start, down to 70 at the end.\nStill, it looks better to keep the \"high\" precision in place, rather than the \"highest\".</p>\n\n<p>Right.  We have the beginnings of a training loop that <em>should</em> be able to let us\nrun a Chinchilla-optimal train on a GPT-2 small sized model in 44 hours, and I have the\ntime to do it.  And it looks like a batch size of six is what we can fit into the\nRTX 3090's 24 GiB of VRAM.</p>\n\n<p>What else are we going to need to build something to do this?</p>\n\n<h3 id=\"checkpointing\">Checkpointing</h3>\n\n<p>If I want to do a long training run, then stuff might go wrong -- it might crash for\nsome reason.\nSo we're going to need to save checkpoints as we go and be able to restart training\nfrom those checkpoints.</p>\n\n<p>In those, we're going to need to save the model and the\noptimiser's state, plus some kind of info about how far through the dataset we are.\nWe should keep training and validation losses too, so that we can easily chart and\nrecover our progress, and according to <a href=\"https://discuss.pytorch.org/t/do-i-need-to-save-the-state-dict-oof-gradscaler/95718/4\">this forum post</a>\nwe're going to need to save the scaler (which makes me think that it actually has state in\nit, so we probably should have used a fresh scaler for each batch size in the\nabove -- let's hope that doesn't prove to be a problem [note from later: it wasn't]).</p>\n\n<p>I wrote a <a href=\"https://github.com/gpjt/llm-from-scratch/blob/main/test-checkpointing.py\">script</a> to create a model, train it for a bit, and then dump out all of that\napart from the metadata (which I reckon is going to be less than 1kB).  I wanted to\nuse the <a href=\"https://huggingface.co/docs/safetensors/en/index\">safetensors</a> format for\nall of it, but unfortunately I couldn't get it to work for the optimiser or the scaler,\nso had to use <code>torch.save</code> for those (which I don't like because it uses <a href=\"https://docs.python.org/3/library/pickle.html\">pickle</a>,\nwhich introduces serious problems if you ever want to move files from machine to machine,\nas the Python and library versions need to match perfectly).  Ah well.  Here's what\nthe test checkpoint looks like:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp gp-VirtualEnv\">(llm-from-scratch)</span> <span class=\"gp\">giles@perry:~/Dev/llm-from-scratch (main)$ </span>du<span class=\"w\"> </span>-sh<span class=\"w\"> </span>test-checkpoint\n<span class=\"go\">1.9G    test-checkpoint</span>\n<span class=\"gp gp-VirtualEnv\">(llm-from-scratch)</span> <span class=\"gp\">giles@perry:~/Dev/llm-from-scratch (main)$ </span>ls<span class=\"w\"> </span>-lh<span class=\"w\"> </span>test-checkpoint\n<span class=\"go\">total 1.9G</span>\n<span class=\"go\">-rw-r--r-- 1 giles giles 670M Nov 11 15:21 model.safetensors</span>\n<span class=\"go\">-rw-r--r-- 1 giles giles 1.3G Nov 11 15:21 optimizer.pt</span>\n<span class=\"go\">-rw-r--r-- 1 giles giles 1.4K Nov 11 15:21 scaler.pt</span>\n</code></pre>\n</div>\n\n<p>That's huge!  And it's almost all the optimiser.  From what I read, that stores two numbers per parameter, so\nit makes sense that it's double the size of the model weights.  And at 32-bit,\n4 bytes per param, then 670MiB for the model is sane.</p>\n\n<p>Timing-wise, it takes about a second to save, the same to load, so that's fine.</p>\n\n<p>So that sounds reasonable in terms of timing, and disk space is pretty high, but not\nso huge that it can't be managed with careful planning -- don't checkpoint so much that\nwe run out of disk during the train (I have a 2TiB disk, but it's far from empty).</p>\n\n<p>It's probably worth double-checking that it works, though!  Because my checkpoint\ntest already did some training, I changed it so that it does this:</p>\n\n<ul>\n<li>Create a model, optimiser and scaler.</li>\n<li>Train the model for a bit.</li>\n<li>Work out the loss.</li>\n<li>Save a checkpoint.</li>\n<li>Create a new model, optimiser, and scaler, and then restore the checkpoint into them.</li>\n<li>Work out the loss</li>\n<li>Train for a bit more to check that the optimiser and scaler still work.</li>\n</ul>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp gp-VirtualEnv\">(llm-from-scratch)</span> <span class=\"gp\">giles@perry:~/Dev/llm-from-scratch (main)$ </span>python<span class=\"w\"> </span>test-checkpointing.py\n<span class=\"go\">Loading dataset shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [00:00&lt;00:00, 387.76it/s]</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:30&lt;00:00,  3.30it/s]</span>\n<span class=\"go\">Loss prior to checkpoint: 7.0519</span>\n<span class=\"go\">Checkpoint saved in 0.96s</span>\n<span class=\"go\">Checkpoint loaded in 0.89s</span>\n<span class=\"go\">Loss after checkpoint load: 7.0519</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:30&lt;00:00,  3.27it/s]</span>\n<span class=\"go\">Loss after further training: 6.8996</span>\n</code></pre>\n</div>\n\n<p>Looks sane!  The numbers for loss are the same before and after, so I think it's vanishingly\nimplausible that the checkpoint we restored is different from the one we saved.  And\nthe continued training seems to be working -- at least, loss is going down -- so that\nsounds reasonable too.</p>\n\n<p>OK, so, again, the time taken to checkpoint is negligible, but the disk space isn't.  I\nreckon we can comfortably do 100 checkpoints over the train.  That's roughly one every\nhalf-hour over 44 hours.</p>\n\n<p>We're going to want to do a validation run each time we checkpoint, so let's think about that next.</p>\n\n<h3 id=\"validation\">Validation</h3>\n\n<p>How big should our validation set be?\nLet's say we only want to spend 5m per checkpoint period doing validation.  How many\nbatches can we get through in that time?</p>\n\n<p>I wrote a simple script to run a model (after a few hundred training steps) in eval\nmode on different numbers of iterations to see how long each one\ntook.  It used the same <code>autocast</code> trick as the\ntraining loop above in order to use mixed precision, and I ran it with <code>torch.inference_mode</code> instead\nof the <code>torch.no_grad</code> that I've used in the past (ChatGPT tells me it's a little faster).\nI also put in some calls to <code>torch.cuda.synchronize</code> around the loop that I was timing,\nwhich should apparently help make sure that the numbers are precise.  The code is\n<a href=\"https://github.com/gpjt/llm-from-scratch/blob/main/measure-validation-timing.py\">here</a> if\nyou'd like to take a look.</p>\n\n<p>After some fiddling with the min/max numbers at the top:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp gp-VirtualEnv\">(llm-from-scratch)</span> <span class=\"gp\">giles@perry:~/Dev/llm-from-scratch (main)$ </span>python<span class=\"w\"> </span>measure-validation-timing.py\n<span class=\"go\">Loading dataset shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [00:00&lt;00:00, 352.52it/s]</span>\n<span class=\"go\">Doing initial train</span>\n<span class=\"go\">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:30&lt;00:00,  3.25it/s]</span>\n<span class=\"go\">Timing validation batch size 2900</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2900/2900 [04:29&lt;00:00, 10.76it/s]</span>\n<span class=\"go\">Got loss 7.3029 in 269.5059s</span>\n<span class=\"go\">Timing validation batch size 3000</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [04:39&lt;00:00, 10.73it/s]</span>\n<span class=\"go\">Got loss 7.3044 in 279.4869s</span>\n<span class=\"go\">Timing validation batch size 3100</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3100/3100 [04:46&lt;00:00, 10.81it/s]</span>\n<span class=\"go\">Got loss 7.3042 in 286.6812s</span>\n<span class=\"go\">Timing validation batch size 3200</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3200/3200 [04:55&lt;00:00, 10.82it/s]</span>\n<span class=\"go\">Got loss 7.3043 in 295.7016s</span>\n<span class=\"go\">Timing validation batch size 3300</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3300/3300 [05:04&lt;00:00, 10.82it/s]</span>\n<span class=\"go\">Got loss 7.3065 in 304.9547s</span>\n<span class=\"go\">Timing validation batch size 3400</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3400/3400 [05:14&lt;00:00, 10.82it/s]</span>\n<span class=\"go\">Got loss 7.3060 in 314.3070s</span>\n<span class=\"go\">Timing validation batch size 3500</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3500/3500 [05:25&lt;00:00, 10.76it/s]</span>\n<span class=\"go\">Got loss 7.3062 in 325.1689s</span>\n<span class=\"go\">Timing validation batch size 3600</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3600/3600 [05:35&lt;00:00, 10.73it/s]</span>\n<span class=\"go\">Got loss 7.3064 in 335.6270s</span>\n<span class=\"go\">Timing validation batch size 3700</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3700/3700 [05:44&lt;00:00, 10.73it/s]</span>\n<span class=\"go\">Got loss 7.3083 in 344.8765s</span>\n<span class=\"go\">Timing validation batch size 3800</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3800/3800 [05:54&lt;00:00, 10.73it/s]</span>\n<span class=\"go\">Got loss 7.3111 in 354.3010s</span>\n<span class=\"go\">Timing validation batch size 3900</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3900/3900 [06:03&lt;00:00, 10.72it/s]</span>\n<span class=\"go\">Got loss 7.3104 in 363.6413s</span>\n<span class=\"go\">Timing validation batch size 4000</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [06:11&lt;00:00, 10.76it/s]</span>\n<span class=\"go\">Got loss 7.3110 in 371.8712s</span>\n</code></pre>\n</div>\n\n<p>OK, so let's call it 3200.  That's 3200 * 6 * 1024 tokens = 19,660,800 tokens.</p>\n\n<p>That's about 0.006144 of our training set.  Pretty low, but we're talking about such\na large training set that I think we're OK.  And practically we can't do more --\nwe're already talking about 5 mins every half-hour, so we're bumping up our train time\nby 88 * 5 = 440 minutes, which is seven hours.</p>\n\n<p>Now let's start thinking about the datasets.</p>\n\n<h3 id=\"datasets\">Datasets</h3>\n\n<p>We can split the HF thing into train and validation sets.  I'm thinking\nit might be useful to load all of our training and validation data into RAM for the train loop.  3.2B tokens\nwith four bytes per token should be about 13 GiB, after all, and I have 64 GiB RAM on the\nmachine.</p>\n\n<p>...but wait, int64 is the default for PyTorch for long ints -- that's what our token lists are in the original,\nand it's twice the size, so we're talking 26 GiB.\nI believe that PyTorch expects that format for the cross entropy loss.</p>\n\n<p>That's not the end of\nthe world, though -- we can store the data as int32 in RAM (with 50,257 as our vocab size we\ncould even use int16 if we wanted to) and then we'll need to make them\nthe right type just before using them.  We can do that when splatting them onto the\nGPU, eg.</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x_int32</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">long</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>First thought, can we store them as a Python list?  Turns out they're not all that memory-efficient, though:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3_200_000_000</span><span class=\"p\">))</span>\n<span class=\"n\">Killed</span>                     <span class=\"n\">ipython</span>\n</code></pre>\n</div>\n\n<p>How about PyTorch tensors?</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">((</span><span class=\"mi\">3_200_000_000</span><span class=\"p\">))</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"mf\">0.6668</span><span class=\"p\">,</span> <span class=\"mf\">0.1471</span><span class=\"p\">,</span> <span class=\"mf\">0.9428</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"mf\">0.3548</span><span class=\"p\">,</span> <span class=\"mf\">0.5738</span><span class=\"p\">,</span> <span class=\"mf\">0.5723</span><span class=\"p\">])</span>\n</code></pre>\n</div>\n\n<p>Promising!  (Though ChatGPT pointed out when reviewing a draft of this post that\nI was using the default <code>float32</code> rather than an <code>int32</code> type here.  Still, it's\nthe same size.)</p>\n\n<p>Let's measure memory usage in a new interpreter.</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span> <span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">psutil</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">os</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"n\">rss_before</span> <span class=\"o\">=</span> <span class=\"n\">psutil</span><span class=\"o\">.</span><span class=\"n\">Process</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getpid</span><span class=\"p\">())</span><span class=\"o\">.</span><span class=\"n\">memory_info</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">rss</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">]:</span> <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">((</span><span class=\"mi\">3_200_000_000</span><span class=\"p\">))</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">]:</span> <span class=\"n\">rss_after</span> <span class=\"o\">=</span> <span class=\"n\">psutil</span><span class=\"o\">.</span><span class=\"n\">Process</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getpid</span><span class=\"p\">())</span><span class=\"o\">.</span><span class=\"n\">memory_info</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">rss</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">]:</span> <span class=\"n\">rss_after</span> <span class=\"o\">-</span> <span class=\"n\">rss_before</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">]:</span> <span class=\"mi\">12801474560</span>\n</code></pre>\n</div>\n\n<p>Yup, 12,801,474,560, so about 12 GiB.  Can we save it?</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">8</span><span class=\"p\">]:</span> <span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">safetensors.torch</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">save_file</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">]:</span> <span class=\"n\">save_file</span><span class=\"p\">({</span><span class=\"s2\">&quot;tokens&quot;</span><span class=\"p\">:</span> <span class=\"n\">t</span><span class=\"p\">},</span> <span class=\"s2\">&quot;xxx&quot;</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"o\">(</span>llm-from-scratch<span class=\"o\">)</span><span class=\"w\"> </span>giles@perry:~/Dev/llm-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>ls<span class=\"w\"> </span>-l<span class=\"w\"> </span>xxx\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\"> </span><span class=\"m\">12800000088</span><span class=\"w\"> </span>Nov<span class=\"w\"> </span><span class=\"m\">11</span><span class=\"w\"> </span><span class=\"m\">20</span>:43<span class=\"w\"> </span>xxx\n<span class=\"o\">(</span>llm-from-scratch<span class=\"o\">)</span><span class=\"w\"> </span>giles@perry:~/Dev/llm-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>ls<span class=\"w\"> </span>-lh<span class=\"w\"> </span>xxx\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\"> </span>12G<span class=\"w\"> </span>Nov<span class=\"w\"> </span><span class=\"m\">11</span><span class=\"w\"> </span><span class=\"m\">20</span>:43<span class=\"w\"> </span>xxx\n</code></pre>\n</div>\n\n<p>OK, let's try reloading it in a fresh session:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span> <span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">safetensors.torch</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">load_file</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">load_file</span><span class=\"p\">(</span><span class=\"s2\">&quot;xxx&quot;</span><span class=\"p\">)[</span><span class=\"s2\">&quot;tokens&quot;</span><span class=\"p\">]</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"n\">t</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"mf\">0.5421</span><span class=\"p\">,</span> <span class=\"mf\">0.1613</span><span class=\"p\">,</span> <span class=\"mf\">0.8055</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"mf\">0.7002</span><span class=\"p\">,</span> <span class=\"mf\">0.7609</span><span class=\"p\">,</span> <span class=\"mf\">0.5629</span><span class=\"p\">])</span>\n</code></pre>\n</div>\n\n<p>Nice.  So, I think we can write a quick script that splits our incoming dataset\ninto say 99/1% train and validation, grabs the first 3.2B tokens from the training set,\nglomming them together into one big tensor with EOSes between them, and saves them, and then does likewise\nfor the first 19,660,800 tokens from the validation set.  We'll use FineWeb, with\nthe possibility of switching to FineWeb-Edu later on.  Doing it that way means that\nwe're actually using the second of the two options I considered earlier:</p>\n\n<blockquote>\n  <p>Treat the corpus as, essentially, one long document, with end-of-sequence delimiters\n  between each row, then split that up into 1,024-token sequences.</p>\n</blockquote>\n\n<p>I thought it would be harder than concatenating/padding rows, but it actually turns out to be simple enough.</p>\n\n<p>Let's give it a go.  <a href=\"https://github.com/gpjt/llm-from-scratch/blob/843da6e19927ef1235b7989032e00c31d6c4396b/big-train-prepare-datasets.py\">Here's the code</a>.\nI wanted to have an round number of 6-sequence batches of 1,024 tokens each, so the\nthe number of training tokens worked out at</p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>534</mn><mo>&#x0002c;</mo><mn>200</mn><mi>&#x000d7;</mi><mn>6</mn><mi>&#x000d7;</mi><mn>1</mn><mo>&#x0002c;</mo><mn>024</mn><mo>&#x0003d;</mo><mn>3</mn><mo>&#x0002c;</mo><mn>282</mn><mo>&#x0002c;</mo><mn>124</mn><mo>&#x0002c;</mo><mn>800</mn></mrow></math>\n\n<p>...rather than the strict Chinchilla-optimal 3,260,190,720, but that's no biggie.</p>\n\n<p>Running it takes 5m55s, and then:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"o\">(</span>llm-from-scratch<span class=\"o\">)</span><span class=\"w\"> </span>giles@perry:~/Dev/llm-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>ls<span class=\"w\"> </span>-lh<span class=\"w\"> </span>big-train-datasets/\ntotal<span class=\"w\"> </span>13G\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\"> </span>13G<span class=\"w\"> </span>Nov<span class=\"w\"> </span><span class=\"m\">11</span><span class=\"w\"> </span><span class=\"m\">23</span>:08<span class=\"w\"> </span>train.safetensors\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\"> </span>76M<span class=\"w\"> </span>Nov<span class=\"w\"> </span><span class=\"m\">11</span><span class=\"w\"> </span><span class=\"m\">23</span>:02<span class=\"w\"> </span>validation.safetensors\n</code></pre>\n</div>\n\n<p>Looks about the right size -- 19M * 4 for val, 3.2B * 4 for train.</p>\n\n<p>Cool!  Let's finally write our training script.</p>\n\n<h3 id=\"finally-training-an-llm\">Finally training an LLM!</h3>\n\n<p>You can see <a href=\"https://github.com/gpjt/llm-from-scratch/blob/main/big_train.py\">the full training script here</a> -- note\nthat this is the final version from the repo, so isn't exactly what I'm running\nat this point in the post.  The checkpointing code is (sensibly enough) in a separate file,\n<a href=\"https://github.com/gpjt/llm-from-scratch/blob/main/checkpointing.py\"><code>checkpointing.py</code></a>.</p>\n\n<p>It took two days to run, and...</p>\n\n<p><img alt=\"Training and validation loss over two days, FineWeb\" src=\"/post-assets/llm-from-scratch-28-training-a-base-model-from-scratch/big-training-run-chart-fineweb.png\" title=\"Training and validation loss over two days, FineWeb\" /></p>\n\n<p>Both train and validation losses fall nicely!  Training loss is a bit choppy, but that's because I erroneously\nonly plotted the most recent iteration's training loss rather than an average over all iterations\nbetween the last and current validation run; the validation loss is correct because I\ndid average all of the validation numbers. (The version of the code linked above fixes that\nerror.)</p>\n\n<p>The best epoch for val loss is not the last one but it was close.  Looking at the last 5 iterations,\ntheir val losses were:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">3.991096583977342</span>\n<span class=\"go\">3.940103444904089  &lt;-- best</span>\n<span class=\"go\">3.9403586230427026</span>\n<span class=\"go\">3.9464842446893456</span>\n<span class=\"go\">3.9469190353155135 &lt;-- latest</span>\n</code></pre>\n</div>\n\n<p>It's time to do some evals</p>\n\n<h3 id=\"evals\">Evals</h3>\n\n<p>Firstly, let's try the smoke test that we do in the book.  What does our model\nthink should come after the text \"Every effort moves you\"?</p>\n\n<p>With uninitialised weights we get gibberish, as expected</p>\n\n<pre><code>Every effort moves youワISIS Keectar handling holistic Supply query prolongidation Joey flaw camerasIdent formula\n</code></pre>\n\n<p>But with our best checkpoint we get this:</p>\n\n<pre><code>Every effort moves you towards a sustainable and holistic diet of water, protein, vitamins, and protein\n</code></pre>\n\n<p>Nice!  The multiple mentions of protein is actually the kind of repetition that small\nmodels tend to do, so that's not bad news.</p>\n\n<p>Let's try with the last iteration's checkpoint:</p>\n\n<pre><code>Every effort moves you towards a new level of success, and you’re likely to continue\n</code></pre>\n\n<p>Also very nice, perhaps better!</p>\n\n<p>I think that both of those are qualitatively as good as the result we got when\nwe <a href=\"/2025/10/llm-from-scratch-22-finally-training-our-llm\">loaded the pre-trained weights from OpenAI</a>,\nwhich was:</p>\n\n<pre><code>Every effort moves you as far as the hand can go until the end of your turn unless something interrupts your control flow. As you may observe I\n</code></pre>\n\n<p>That's very reassuring.  But is there something a bit more quantitative that we can do?</p>\n\n<p>Firstly, can we compare it to anything in the GPT-2 paper?  In figure 4 they give\ntheir perplexity against their train and test sets for the different model sizes;\nfor the small one it's a bit over 16,  Let's assume that they're basing that on natural logarithms,\nso they mean that they have a loss of <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>ln</mi><mn>16</mn></mrow></math>.  That's <code>2.77</code>, which is much lower than our\nbest loss of 3.9401.</p>\n\n<p>However, that is across different datasets, so while it makes me suspect that their\nmodel is better than ours, we can't really say for sure either way.</p>\n\n<p>The cool thing is, though, that we <em>have</em> their model -- so we can actually run it against\nour dataset.  I wrote a script called <a href=\"https://github.com/gpjt/llm-from-scratch/blob/main/test_openai_weights_against_our_val_dataset.py\"><code>test_openai_weights_against_our_val_dataset.py</code></a>,\nand running it gives us this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Loss against our validation dataset: 3.4987249702960255</span>\n</code></pre>\n</div>\n\n<p>Still better than ours :-(</p>\n\n<p>I considered doing the same thing against Qwen to see whether that was also better,\nbut with a different tokeniser we couldn't really treat it as comparable.  Loss and\nperplexity are both over next-token predictions, and if the meaning of \"token\" changes,\nthen the numbers will change. <sup class=\"footnote-ref\" id=\"fnref-2\"><a href=\"#fn-2\">2</a></sup></p>\n\n<p>OK, so we have a model, but it's not as good as the original GPT-2 small.  Our\nloss on our validation set is roughly 3.94, while the original weights get about 3.50.  Expressing\nthat in terms of perplexity gives our own model about 51.4, while the original\nhas 33.1.  That's actually still higher than the 16 that they had in the paper, which\nis interesting -- presumably it's related to the fact that they're validating over\ntheir own WebText test set rather than ours; they're both samples of web content,\nbut there must be differences.</p>\n\n<p>At this point, my guess is that this shows that all of that extra training that the OpenAI team did beyond\nthe Chinchilla-optimal number of tokens did have a real benefit -- and that's not\nsuprising.  Remember that the Chinchilla paper is about the best way to spend a FLOPs\nbudget.  They're not saying that you can't drive down loss by continuing to train\nyour model further -- of course you can.  They're saying that when you pass the\noptimal number of tokens, you should increase the model parameters and the tokens\nby the same ratio, and by doing that you'll get the best balance.</p>\n\n<p>But still, a Chinchilla-optimal model of 163M parameters might still be useful.\nWhat happens if we instruction fine-tune it <a href=\"/2025/10/llm-from-scratch-25-instruction-fine-tuning\">like we did the original model in\nChapter 7 of the book</a>?  In that\npost and its <a href=\"/2025/11/llm-from-scratch-26-evaluating-the-fine-tuned-model\">followup</a>,\nwe used some training samples using the \"Alpaca\" one-shot\nquestion-answering format:</p>\n\n<pre><code>Below is an instruction that describes a task.  Write a response that\nappropriately completes the request.\n\n### Instruction:\n\n&lt;some instructions&gt;\n\n\n### Input:\n\n&lt;optional, some input&gt;\n\n### Response:\n</code></pre>\n\n<p>...to get a model that we then provided a test set of questions in the same format,\nthen used the Llama 3 7B model to judge the results on a scale of 0 to 100.  We then\naveraged the results and got a plausible-looking indicator of how useful the model was,\nas compared to the more narrowly technical loss number.</p>\n\n<p>One problem with that is that we ran those tests on the OpenAI weights for the medium-sized 355M-parameter\nGPT-2 model.  If we don't want to be comparing apples to oranges, we'll need to re-run it on\ntheir weights for the small model.  Let's see how we do.</p>\n\n<p>First, let's run it for five epochs just to see when/if it starts overfitting:</p>\n\n<p><img alt=\"Loss over five epochs training GPT-2 original weights on Alpaca\" src=\"/post-assets/llm-from-scratch-28-training-a-base-model-from-scratch/instruction-fine-tune-5-epochs-losses-plot-original-weights.png\" title=\"Loss over five epochs training GPT-2 original weights on Alpaca\" /></p>\n\n<p>OK, so two epochs looks like the right amount, just as it was with the medium model.\nSo we can train for that (because I'm using the original code I wrote when working\nthrough the chapter, I didn't checkpoint during training -- but it takes less than a\nminute to run the whole thing, so no biggie).  Here's the loss chart:</p>\n\n<p><img alt=\"Loss over two epochs training GPT-2 original weights on Alpaca\" src=\"/post-assets/llm-from-scratch-28-training-a-base-model-from-scratch/instruction-fine-tune-2-epochs-losses-plot-original-weights.png\" title=\"Loss over two epochs training GPT-2 original weights on Alpaca\" /></p>\n\n<p>Validation loss at the end is 0.733, noticeably above the 0.649 that I got with the\nmedium-sized model.  And the sample outputs shown at the end aren't as good, either.\nWith the medium-sized model, I got these:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Instruction:</span>\n<span class=\"go\">Rewrite the sentence using a simile.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Input</span>\n<span class=\"go\">The car is very fast.</span>\n\n<span class=\"go\">Correct response:</span>\n<span class=\"go\">&gt;&gt; The car is as fast as lightning.</span>\n\n<span class=\"go\">Model response:</span>\n<span class=\"go\">&gt;&gt; The car is as fast as a bullet.</span>\n<span class=\"go\">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Instruction:</span>\n<span class=\"go\">What type of cloud is typically associated with thunderstorms?</span>\n\n<span class=\"go\">Correct response:</span>\n<span class=\"go\">&gt;&gt; The type of cloud typically associated with thunderstorms is cumulonimbus.</span>\n\n<span class=\"go\">Model response:</span>\n<span class=\"go\">&gt;&gt; The type of cloud typically associated with thunderstorms is a cumulus cloud.</span>\n<span class=\"go\">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Instruction:</span>\n<span class=\"go\">Name the author of 'Pride and Prejudice'.</span>\n\n<span class=\"go\">Correct response:</span>\n<span class=\"go\">&gt;&gt; Jane Austen.</span>\n\n<span class=\"go\">Model response:</span>\n<span class=\"go\">&gt;&gt; The author of 'Pride and Prejudice' is Jane Austen.</span>\n</code></pre>\n</div>\n\n<p>...but with the small model (remember, this is with OpenAI's original weights) I get this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Instruction:</span>\n<span class=\"go\">Rewrite the sentence using a simile.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Input</span>\n<span class=\"go\">The car is very fast.</span>\n\n<span class=\"go\">Correct response:</span>\n<span class=\"go\">&gt;&gt; The car is as fast as lightning.</span>\n\n<span class=\"go\">Model response:</span>\n<span class=\"go\">&gt;&gt; The car is as fast as a horse.</span>\n<span class=\"go\">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Instruction:</span>\n<span class=\"go\">What type of cloud is typically associated with thunderstorms?</span>\n\n<span class=\"go\">Correct response:</span>\n<span class=\"go\">&gt;&gt; The type of cloud typically associated with thunderstorms is cumulonimbus.</span>\n\n<span class=\"go\">Model response:</span>\n<span class=\"go\">&gt;&gt; A type of cloud typically associated with thunderstorms is the active layer.</span>\n<span class=\"go\">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Instruction:</span>\n<span class=\"go\">Name the author of 'Pride and Prejudice'.</span>\n\n<span class=\"go\">Correct response:</span>\n<span class=\"go\">&gt;&gt; Jane Austen.</span>\n\n<span class=\"go\">Model response:</span>\n<span class=\"go\">&gt;&gt; The author of 'Pride and Prejudice' is Robert Frost.</span>\n</code></pre>\n</div>\n\n<p>Definitely worse, especially the last one!  Let's see what Llama 3 thinks of it,\nagain using the code from the book:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Number of scores: 110 of 110</span>\n<span class=\"go\">Average score: 35.50</span>\n</code></pre>\n</div>\n\n<p>The medium model got an average of 50, so the OpenAI small model is definitely much worse, as the examples\nsuggested.  Makes sense.</p>\n\n<p>Let's see how our own base model performs when fine-tuned on the same data.\nAfter a bit of fiddling I found that validation loss settled down at the end of epoch\n10:</p>\n\n<p><img alt=\"Loss over ten epochs training our FineWeb base model on Alpaca\" src=\"/post-assets/llm-from-scratch-28-training-a-base-model-from-scratch/instruction-fine-tune-10-epochs-losses-plot-our-model-fineweb.png\" title=\"Loss over ten epochs training our FineWeb base model on Alpaca\" /></p>\n\n<p>(It's hard to see from the chart, but validation loss was actually very slowly\ndropping even after epoch 5.)</p>\n\n<p>It's interesting that our own model took longer to train here, but it does make sense\nin terms of it being that little bit dumber.</p>\n\n<p>The samples it printed out at the end are also interesting:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Instruction:</span>\n<span class=\"go\">Rewrite the sentence using a simile.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Input</span>\n<span class=\"go\">The car is very fast.</span>\n\n<span class=\"go\">Correct response:</span>\n<span class=\"go\">&gt;&gt; The car is as fast as lightning.</span>\n\n<span class=\"go\">Model response:</span>\n<span class=\"go\">&gt;&gt; The car is as fast as a cheetah.</span>\n<span class=\"go\">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Instruction:</span>\n<span class=\"go\">What type of cloud is typically associated with thunderstorms?</span>\n\n<span class=\"go\">Correct response:</span>\n<span class=\"go\">&gt;&gt; The type of cloud typically associated with thunderstorms is cumulonimbus.</span>\n\n<span class=\"go\">Model response:</span>\n<span class=\"go\">&gt;&gt; A thunder storm is a type of thunder.</span>\n<span class=\"go\">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span>\n\n<span class=\"gp\">#</span><span class=\"c1\">## Instruction:</span>\n<span class=\"go\">Name the author of 'Pride and Prejudice'.</span>\n\n<span class=\"go\">Correct response:</span>\n<span class=\"go\">&gt;&gt; Jane Austen.</span>\n\n<span class=\"go\">Model response:</span>\n<span class=\"go\">&gt;&gt; The author of 'Pride and Prejudice' is Robert Frost.</span>\n</code></pre>\n</div>\n\n<p>The simile is pretty good, I think better than the OpenAI original weights' one,\nbut the storm clouds one is dreadful.  It's fascinating that they both chose the same\nwrong answer for \"Pride and Prejudice\" -- my guess is that it's because the training\nset contained this question:</p>\n\n<pre><code>Identify the tone used in the poem 'The Road Not Taken' by Robert Frost.\n</code></pre>\n\n<p>...so both models picked up on Robert Frost being a useful author to reference in\nanswers.</p>\n\n<p>Anyway, what does Llama 3 think of the output?</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Number of scores: 110 of 110</span>\n<span class=\"go\">Average score: 29.35</span>\n</code></pre>\n</div>\n\n<p>Yup, it's dumber than the original weights -- but, at least to my mind, closer to\nthe original weights' score than you might have thought based on that loss/perplexity\nnumber alone.</p>\n\n<p>But, on the other hand, I'm not convinced that Llama 3 7B is smart enough to be\ndoing a good job.  In the stuff the eval script printed out, we have this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Dataset response:</span>\n<span class=\"go\">&gt;&gt; The type of cloud typically associated with thunderstorms is cumulonimbus.</span>\n\n<span class=\"go\">Model response:</span>\n<span class=\"go\">&gt;&gt; The type of cloud is typically associated with thunderstorms.</span>\n\n<span class=\"go\">Score:</span>\n<span class=\"go\">&gt;&gt; I'd score this model response a 40 out of 100.</span>\n\n<span class=\"go\">Here's why:</span>\n\n<span class=\"go\">* The response partially answers the question by mentioning that cumulonimbus clouds are associated with thunderstorms.</span>\n<span class=\"go\">* However, it lacks specific details and doesn't provide a clear answer to the question.</span>\n<span class=\"go\">* A good response should provide a complete and accurate answer, which this one does not.</span>\n\n<span class=\"go\">A better response would be something like: &quot;The type of cloud typically associated with thunderstorms is cumulonimbus.&quot; This response provides a clear and accurate answer to the question.</span>\n</code></pre>\n</div>\n\n<p>This is clearly completely wrong, the mention of cumulonimbus is coming from the\ndataset response, not the model response.  Llama 3 7B is tripping up over what\ncame from where, which is pretty normal for a small model.</p>\n\n<p>Of course, it's possible that the scores for the OpenAI GPT-2 small weights also have\nbeen given a higher rating than they deserve -- or, indeed, that there were right\nanswers that were incorrectly judged wrong.  Conceivably it averages out.  But there's\nno reason to assume it would, so it's essentially noise and is making the results less\nuseful.</p>\n\n<p>Let's try using a much smarter LLM as a judge and run both of the models responses\nthrough it -- the just-released OpenAI GPT-5.1 model.  The code is <a href=\"https://github.com/gpjt/llm-from-scratch/blob/main/evaluate-with-openai.py\">here</a>.</p>\n\n<p>Running that against our own model's answers:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Number of scores: 110 of 110</span>\n<span class=\"go\">Average score: 16.14</span>\n</code></pre>\n</div>\n\n<p>...and against the model fine-tuned from the small OpenAI weights:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Number of scores: 110 of 110</span>\n<span class=\"go\">Average score: 20.39</span>\n</code></pre>\n</div>\n\n<p>...and, of course, it didn't make the mistake of confusing the dataset response with\nthe model's in any of the cases printed out.  ChatGPT 5.1 in the chat interface is\nvery smart, I expect these results are much closer to a reasonable ground truth.</p>\n\n<p>Out of interest, what does it make of the model based on the GPT-2 <strong>medium</strong> weights that we train as part of the book?</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Number of scores: 110 of 110</span>\n<span class=\"go\">Average score: 38.41</span>\n</code></pre>\n</div>\n\n<p>That's as compared to an average of about 50 from Llama 3 7B.  It seems like GPT 5.1\nis a tougher judge than the small local model -- and my guess is that that is because\nit's more accurate. <sup class=\"footnote-ref\" id=\"fnref-3\"><a href=\"#fn-3\">3</a></sup></p>\n\n<p>Anyway, the ranking remains the same; after fine-tuning on the same Alpaca dataset,\nGPT-2 medium &gt; GPT-2 small &gt; our model.  But it's still a relatively close-run thing\nbetween our model and GPT-2 small.  Can we close the gap without vast amounts of\nextra training?</p>\n\n<h3 id=\"fineweb-edu\">FineWeb-Edu</h3>\n\n<p>The results so far were from using 3.2B tokens of the FineWeb 10B corpus.  Now, as\nI noted at the start of this post, Andrej Karpathy's nanochat project uses FineWeb-Edu,\na separate corpus designed to be really informative.  Indeed, back at the start when\nwe were looking at the two datasets, the first row in the Edu dataset was about\nJane Austen, so maybe we would wind up with a model that at least got that question right!</p>\n\n<p>That's going to take another two days to train, but that's no big deal.  We first\nneed to change our script that generates the train/validation splits to regenerate\nthem using the Edu dataset; we'll move the old ones to one side, though -- it will\nbe interesting to see what loss we get on the non-edu validation data with the new model.</p>\n\n<p>(Note to self: work out some way to split out different datasets and training runs for\nfuture experiments like this.  The setup I had in my <a href=\"/2025/10/retro-language-models-rebuilding-karpathys-rnn-in-pytorch\">recent post on RNNs</a>\nworked quite well.  Throughout the remainder of this post I'm juggling directories of\ncheckpoints and datasets, and I'm sure I got it right, but it was an error-prone process.)</p>\n\n<p>That being done, it's time to move the checkpoints we already have to one side, and\nto kick off the train!</p>\n\n<p>Here's what we have after two days on that -- oops, I forgot to add the code to average\ntraining loss across all of the batches, so again it's a bit spiky.</p>\n\n<p><img alt=\"Training and validation loss over two days, FineWeb-Edu\" src=\"/post-assets/llm-from-scratch-28-training-a-base-model-from-scratch/big-training-run-chart-fineweb-edu.png\" title=\"Training and validation loss over two days, FineWeb-Edu\" /></p>\n\n<p>But we got to\na final eval loss of about 3.693 this time.  Of course, that's on its own validation\nset, so it's not comparable with the numbers from before; loss is specific to a particular\ndataset.  Let's see what it makes\nof the original run's validation set.  Juggle some directories around (my messy file\nstructure means that there is just one \"datasets\" directory and one \"checkpoints\" one,\nso I'm moving them around to make sure I'm using the right combination):</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp gp-VirtualEnv\">(llm-from-scratch)</span> <span class=\"gp\">giles@perry:~/Dev/llm-from-scratch (main)$ </span>python<span class=\"w\"> </span>test_our_weights_against_our_dataset.py\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3200/3200 [04:52&lt;00:00, 10.92it/s]</span>\n<span class=\"go\">Loss against our validation dataset: 4.164705707877874</span>\n</code></pre>\n</div>\n\n<p>We get 4.16!  That's truly terrible, worse than both the original base model that\nwe trained on FineWeb's non-edu dataset, and than the OpenAI GPT-2 small weights.</p>\n\n<p>Let's see what we get from the closer-to-real-world instruction fine-tuning test.\nFive epochs turns out to be best:</p>\n\n<p><img alt=\"Loss over five epochs training our FineWeb-Edu base model on Alpaca\" src=\"/post-assets/llm-from-scratch-28-training-a-base-model-from-scratch/instruction-fine-tune-5-epochs-losses-plot-our-model-fineweb-edu.png\" title=\"Loss over five epochs training our FineWeb-Edu base model on Alpaca\" /></p>\n\n<p>I won't bother running it past Llama 3 7B, as that's proven unhelpful, so we'll go\nstraight to GPT-5.1.</p>\n\n<pre><code>Number of scores: 110 of 110\nAverage score: 15.18\n</code></pre>\n\n<p>Gosh!  So it's judged slightly worse than our weights based on FineWeb.\nThat does surprise me a bit.  I was definitely expecting the Edu version of the\ndataset to give us a better model.</p>\n\n<p>So: OpenAI medium &gt; OpenAI small &gt; our FineWeb base model &gt; our FineWeb-Edu base model.\nThat last pairing does surprise me a bit.  Handwaving wildly, perhaps the more \"regular\" nature of\nthe Edu dataset meant that the model saw less variation in its training set, and\nthat actually made it learn less?</p>\n\n<p>I think there's one more experiment I want to do before bringing this (<em>very</em>\nlengthy) post to a close.  We've shown that Chinchilla-optimal training of models\nproduces worse results than OpenAI's original, we think longer, train.</p>\n\n<p>What would happen if we continued training for another two days?</p>\n\n<h3 id=\"continuing-training\">Continuing training</h3>\n\n<p>As I have it easily to hand, I want to use the FineWeb-Edu model for this.  I want\nto start with the best checkpoint (which happens to be the last one), and train\nit on another 3.2B tokens from FineWeb-Edu.  Let's see what we get.</p>\n\n<p>Getting a dataset is going to be a bit messy, as our existing script to generate the\nsafetensors datasets\njust grabs tokens from the original dataset until it gets 534,200 batches of 6 sequences, each\nof 1024 tokens (3,282,124,800 total).</p>\n\n<p>Might as well hack it (and note that this is something worth improving for any\nlater experiments).  I'll just loop round the code to do that twice, throwing\naway the first set of 3.2B tokens.</p>\n\n<p>I was pretty sure that the ordering of the datasets I'm\ngetting is fixed, but perhaps not -- it spent time regenerating the train/val split\nat the start of the script, so there's no guarantee we have different data this time.\nThat feels like a note-to-self about data pipeline hygiene -- if the train/val split\nis randomised by the infra I'm using, I should persist the raw data in case I need to\nuse more data than I though I would need to.</p>\n\n<p>Still, for this experiment, we can play relatively fast and loose.  After all, GPT-2\nsmall -- the original OpenAI weights -- was trained on multiple epochs, so it saw tokens\nmultiple times.  What we're trying to see here is what happens if you train for longer;\na more scientific experiment can happen later (if at all...).</p>\n\n<p>Anyway, we have 3.2B tokens that should at least be reasonably different from the original 3.2B.</p>\n\n<p>Right, let's clean up some disk space so that we have enough for the new train (deleted\nsome old optimiser checkpoints, keeping the metadata and the weights).</p>\n\n<p>Now, we create a new checkpoints directory, and we can copy the last/best checkpoint\nfrom the original FineWeb-Edu train there.  Hack the <code>train_ds_offset</code> in there to\nzero, create <code>best</code> and <code>latest</code> symlinks, and then we can \"restart\" from that checkpoint.\nDue to the way the restart-from-checkpoint code works in the training script, that means that it will start with an offset of 1 into the dataset, so we're\ndropping one of about 530,000 iterations, but that's not exactly the end of the\nworld.</p>\n\n<p><img alt=\"Training and validation loss over a second period of two days, FineWeb-Edu\" src=\"/post-assets/llm-from-scratch-28-training-a-base-model-from-scratch/big-training-run-chart-fineweb-edu-2x.png\" title=\"Training and validation loss over a second period of two days, FineWeb-Edu\" /></p>\n\n<p>There are some interesting spikes on validation loss in there -- in particular that one\nat around iteration 300,000 where it goes up from 3.6 or so to 7.5 for two validation\nperiods (which, remember, happen every ~30 minutes, or every 7020 iterations).</p>\n\n<p>My guess\nis that we got some kind of gradient spike prior to those, which led to a bad update\nto the parameters.  However, it looks like the loss recovered really quickly after it,\nso while gradient clipping (that is, limiting the size of the gradients so that one-off\nspikes don't cause massive updates) might have prevented them, I don't think it would\nhave improved matters much -- we might have \"lost\" an hour so of training, but out\nof a 44-hour train (48 hours including breaks for validation), it's not the end\nof the world.</p>\n\n<p>But, looking at the raw numbers, after our second two days of training on a fresh\nsample from FineWeb-Edu 10B, we've managed to get the loss on our validation set down from\n3.693 to... drumroll... 3.661.  And that's on the \"best\" measurement, which was an hour\nbefore the end.  The last validation number was 3.663.</p>\n\n<p>By spending twice the time, we've managed to get our loss down by 0.032, which is\na touch less than 1%.  Even measured in terms of perplexity (which, being an exponential,\nis more sensitive to this kind of change), we've gone from 40.2 to 38.9, which is\nhardly show-stopping.</p>\n\n<p>Let's see how this one measures up against the non-edu FineWeb validation dataset that we\noriginally used to calibrate our first training run.  Run it, and:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp gp-VirtualEnv\">(llm-from-scratch)</span> <span class=\"gp\">giles@perry:~/Dev/llm-from-scratch (main)$ </span>python<span class=\"w\"> </span>test_our_weights_against_our_dataset.py\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3200/3200 [04:53&lt;00:00, 10.89it/s]</span>\n<span class=\"go\">Loss against our validation dataset: 4.134009174928069</span>\n</code></pre>\n</div>\n\n<p>...we get 4.13 -- that's opposed to 4.16 on the last model, trained on half as much data.</p>\n\n<p>Well, maybe it's a much better base model for instruction fine-tuning?  Let's give that\na go, again with the Alpaca training set from the book.  8 epochs turns out to be\nthe right number:</p>\n\n<p><img alt=\"Loss over eight epochs training our &quot;double-trained&quot; FineWeb-Edu base model on Alpaca\" src=\"/post-assets/llm-from-scratch-28-training-a-base-model-from-scratch/instruction-fine-tune-8-epochs-losses-plot-our-model-fineweb-edu-2x.png\" title=\"Loss over eight epochs training our &quot;double-trained&quot; FineWeb-Edu base model on Alpaca\" /></p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Number of scores: 110 of 110</span>\n<span class=\"go\">Average score: 16.62</span>\n</code></pre>\n</div>\n\n<p>Certainly better than the 15.18 that we got on our Chinchilla-optimal FineWeb-Edu model,\nand a bit better than the 16.14 we got on the Chinchilla-optimal FineWeb one.\nSo by training for double the time on twice the data, we've definitely got a better\nmodel.  It's just not <em>that much</em> better.</p>\n\n<p>I think that's more -- significantly more -- than enough experimentation for one blog post, so let's do some\nanalysis.</p>\n\n<h3 id=\"flops\">FLOPs</h3>\n\n<p>I want to sanity-check the number of FLOPs spent on this train, just to make sure\nthat I hadn't messed up.  Feel free to <a href=\"#but-why-is-our-model-worse-than-openais\">skip this</a> if you want to jump straight to the\nconclusion :-)</p>\n\n<p>In appendix F, the Chinchilla paper mentions a common approximation for how many FLOPs, <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>C</mi></mrow></math>, you\nspend training a model with <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>N</mi></mrow></math> parameters over <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>D</mi></mrow></math> tokens:</p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>C</mi><mo>&#x0003d;</mo><mn>6</mn><mi>D</mi><mi>N</mi></mrow></math>\n\n<p>So based on that, each of those training runs cost us (using the exact numbers for <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>N</mi></mrow></math> and <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>D</mi></mrow></math>) this many FLOPs:</p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>C</mi><mo>&#x0003d;</mo><mn>6</mn><mi>&#x000d7;</mi><mn>3</mn><mo>&#x0002c;</mo><mn>282</mn><mo>&#x0002c;</mo><mn>124</mn><mo>&#x0002c;</mo><mn>800</mn><mi>&#x000d7;</mi><mn>163</mn><mo>&#x0002c;</mo><mn>009</mn><mo>&#x0002c;</mo><mn>536</mn><mo>&#x0003d;</mo><mn>3</mn><mo>&#x0002c;</mo><mn>210</mn><mo>&#x0002c;</mo><mn>105</mn><mo>&#x0002c;</mo><mn>844</mn><mo>&#x0002c;</mo><mn>452</mn><mo>&#x0002c;</mo><mn>556</mn><mo>&#x0002c;</mo><mn>800</mn><mo>&#x0003d;</mo><mn>3.21</mn><mi>&#x000d7;</mi><msup><mn>10</mn><mrow><mn>18</mn></mrow></msup><mtext>FLOPS</mtext></mrow></math>\n\n<p>They also give a more carefully-worked out calculation; it doesn't look all that\ndifficult -- it's just a case of plugging in the numbers from our architecture and\npulling out a result <sup class=\"footnote-ref\" id=\"fnref-4\"><a href=\"#fn-4\">4</a></sup> -- but the numbers they get from that are generally within\n10% of the simpler calculations, so we may as well stick with the above. <sup class=\"footnote-ref\" id=\"fnref-5\"><a href=\"#fn-5\">5</a></sup></p>\n\n<p>Now, in terms of how many FLOPs we actually spent... well, manufacturers' datasheets\nfor hardware are based on carefully-selected benchmarks and won't really be comparable\nto the code we were running (especially given that it's my crappy code based on top\nof a huge stack of PyTorch, CUDA kernels, CUDA itself, and so on), but we can do a\n<a href=\"https://en.wikipedia.org/wiki/Fermi_problem\">Fermi estimate</a>.</p>\n\n<p>From Wikipedia, the <a href=\"https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units#RTX_30_series\">RTX 3090</a> has\n35.58 TFLOPS performance on FP32.  Way back earlier in this post, when I was\nmeasuring how many tokens per second I could get locally, the first experiment\ncapped out at 12,599 tokens/second with FP32.  <code>nvtop</code> showed the GPU usage at 100%,\nso let's say (again, this is very approximate) that we were getting about 35.58 TFLOPs\nand that enabled 12,599 tokens/second.</p>\n\n<p>We wound up training at about 19,921 tokens/second\nafter adding in mixed precision and using the tensor cores.  So, hand-wavingly\nwe can say that we were getting</p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mfrac><mrow><mn>19</mn><mo>&#x0002c;</mo><mn>921</mn></mrow><mrow><mn>12</mn><mo>&#x0002c;</mo><mn>599</mn></mrow></mfrac><mi>&#x000d7;</mi><mn>35.58</mn><mo>&#x0003d;</mo><mn>56.26</mn><mtext>TFLOPs</mtext></mrow></math>\n\n<p>Now, we trained for 44 hours (48 including validation), so the total number of training FLOPs\nshould have been the number of seconds in that times the total FLOPS <sup class=\"footnote-ref\" id=\"fnref-6\"><a href=\"#fn-6\">6</a></sup>\nof <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>56.27</mn><mi>&#x000d7;</mi><msup><mn>10</mn><mrow><mn>12</mn></mrow></msup></mrow></math></p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>44</mn><mi>&#x000d7;</mi><mn>60</mn><mi>&#x000d7;</mi><mn>60</mn><mi>&#x000d7;</mi><mn>56.27</mn><mi>&#x000d7;</mi><msup><mn>10</mn><mrow><mn>12</mn></mrow></msup><mo>&#x0003d;</mo><mn>8.91</mn><mi>&#x000d7;</mi><msup><mn>10</mn><mrow><mn>18</mn></mrow></msup></mrow></math>\n\n<p>That's pleasingly close to the <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>3.19</mn><mi>&#x000d7;</mi><msup><mn>10</mn><mrow><mn>18</mn></mrow></msup></mrow></math> above!  I can easily imagine that the stack we're using could\nsomewhat-more-than-halve performance from the theoretically optimal, or that we're running at 50% of\nthe GPU's theoretical capacity, or some combination of the two.  We're in the same\norder of magnitude, and for a Fermi approximation, that's what matters.</p>\n\n<p>Now, looking at figure 3 in the Chinchilla paper, their IsoFLOP curves (each one showing the loss they got\non their training set for models of a particular size, using the same number of\nFLOPs for each curve), we can see that the top one, which is\ntraining runs of <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>6</mn><mi>&#x000d7;</mi><msup><mn>10</mn><mrow><mn>18</mn></mrow></msup></mrow></math> FLOPs, the lowest point is pretty much bang-on\nthe 168M point on the X axis.</p>\n\n<p>So that is at least reassuring that we did do a proper Chinchilla-optimal train here.\n(Their loss on that chart is showing 3, but they're using a different dataset, so I don't think\nit's comparable.)</p>\n\n<h3 id=\"but-why-is-our-model-worse-than-openais\">But why is our model worse than OpenAI's?</h3>\n\n<p>Apart from the obvious answer of \"skill issue\", let's see if there are any obvious\nreasons why the base model I've trained (and retrained) in this post is worse than\nthe original OpenAI GPT-2 small.  Let's review the results first:</p>\n\n<table>\n<thead>\n<tr>\n  <th></th>\n  <th>FineWeb train</th>\n  <th>FineWeb-Edu train</th>\n  <th>FineWeb-Edu extended train</th>\n  <th>OpenAI weights</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>Val loss on own dataset</td>\n  <td>3.94</td>\n  <td>3.693</td>\n  <td>3.661</td>\n  <td>2.80 <sup class=\"footnote-ref\" id=\"fnref-7\"><a href=\"#fn-7\">7</a></sup></td>\n</tr>\n<tr>\n  <td>Val loss on FineWeb dataset</td>\n  <td>3.94</td>\n  <td>4.16</td>\n  <td>4.13</td>\n  <td>3.50</td>\n</tr>\n<tr>\n  <td>Alpaca answers judged by GPT-5.1</td>\n  <td>16.14</td>\n  <td>15.18</td>\n  <td>16.62</td>\n  <td>20.39</td>\n</tr>\n</tbody>\n</table>\n\n<p>The first row is not super-interesting, it's the second and third that matter.</p>\n\n<ul>\n<li>On <em>our own</em> validation set from FineWeb, our we have OpenAI &gt; our FineWeb train &gt; our FineWeb-Edu extended train &gt; our FineWeb-Edu train</li>\n<li>On the answers judged by GPT-5.1 after instruction fine-tuning, we have OpenAI &gt; our FineWeb-Edu extended train &gt; our FineWeb train &gt; our FineWeb-Edu train</li>\n</ul>\n\n<p>OpenAI is clearly winning by quite some margin!  Earlier on I assumed that the difference\nwas that they trained on more data, but let's be a bit more systematic here.</p>\n\n<p>What specific differences do we\nhave to the original train?  Again, the amount of data in the paper is frustratingly\nlimited, but:</p>\n\n<h4 id=\"amount-of-training-data\">Amount of training data</h4>\n\n<p>Right at the start, I estimated that the WebText dataset they trained on was about 10B\ntokens.  We've trained on 3.2B tokens for two of our models, and 6.4B tokens for the extended\ntrain one.</p>\n\n<p>That could well have an effect.  There's more information in their larger dataset,\nboth in terms of raw facts like \"Jane Austen wrote Pride and Prejudice\", and in terms of\ninformation about the structure of language.</p>\n\n<p>On the other hand, their dataset is, as they say, comprised of the contents of web pages that\nwere linked from Reddit posts with more than three upvotes.  FineWeb (and even more FineWeb-Edu) is\na much more curated dataset, so you would expect it has more facts, and better structure\n-- less of the slop and junk that Andrej Karpathy talked about in his interview with Dwarkesh\nPatel.</p>\n\n<p>So I'm not sure that this is it, but it's worth keeping in mind.</p>\n\n<h4 id=\"number-of-epochs\">Number of epochs</h4>\n\n<p>Again, we don't know how many epochs they trained on, but the report I linked to\nright at the start of this post estimated that they trained for 60, while I calculated based\non their numbers that it would be 41 epochs with WebText.</p>\n\n<p>It certainly makes sense that grinding along, epoch after epoch, will get your loss\ndown, at least on the training set!  And there's also a phenomenon with certain kinds\nof neural networks where if keep training past the point where you're overfitting\n(that is, validation loss starts rising while training loss continues to fall),\nsuddenly the model can have an \"aha\" moment and <a href=\"https://arxiv.org/abs/2201.02177\">start generalising again</a>. <sup class=\"footnote-ref\" id=\"fnref-8\"><a href=\"#fn-8\">8</a></sup></p>\n\n<p>It's not quite comparable, because it was not a second epoch, but rather continued training\nwith more data, but we were able to eke out an extra reduction of 0.032 in loss by\ntraining our FineWeb-Edu model for twice as long.  If we'd trained it for 40 times\nas long, then we presumably would have managed to grind it down even further.  I\nhave no idea how much further we could get it, but I'd guess that it's going to be\nworse than linear (that is, each extra two days gets you less loss reduction than\nthe previous) -- so we can bound the loss reduction at a <em>maximum</em>\nof <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>39</mn><mi>&#x000d7;</mi><mn>0.032</mn><mo>&#x0003d;</mo><mn>1.248</mn></mrow></math>.</p>\n\n<p>So... maybe?  It would be a dull experiment to run, though, taking 78 days.  If\nI want to do that, it would be better to find a way to do it quickly, so that I can get\na better feedback loop going.  The reason this post has taken so long has in part been\nbecause each training run has taken so long (as well as trips to London and other life\nstuff).</p>\n\n<h4 id=\"architectural-differences\">Architectural differences</h4>\n\n<p>The original GPT-2 model from OpenAI had bias on the <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math> and <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math> projections -- that is,\nthey were normal NN biased linear layers rather than simple matrices, so they did\na projection into their respective spaces followed by a translation.  In the book,\nRaschka says that this is not normally done these days, which is why I didn't do it\nfor this base model train.</p>\n\n<p>But perhaps it actually is valuable with this architecture or size?  Modern models\npresumably differ in multiple ways, and perhaps the bias would have been useful for\nthis old design.</p>\n\n<p>Likewise, weight-tying -- the original GPT-2 re-used its embedding matrix to do the\nfinal projection from embedding space to vocab space, rather than having a separate one.\nThat seems intuitively clever but not necessarily \"right\", given that it gives the\nmodel less flexibility in what it can output from the last layer.  But perhaps with\nthis size and architecture, it's the right thing to do?</p>\n\n<h4 id=\"dropout\">Dropout</h4>\n\n<p>Contrariwise, having made those two changes to GPT-2 because I believed that modern\nmodels don't work that way, there was one \"modern\" change that I didn't make.  In his post on the\narchitectural changes since GPT-2, Raschka mentioned that dropout is normally not used nowadays.\nThis looked to me like it was due to the move to single-epoch training.  But\nsingle-epoch training was exactly what we were doing in this post!  Perhaps I was\nholding myself back by keeping dropout in place.</p>\n\n<h4 id=\"the-learning-rate\">The learning rate</h4>\n\n<p>I don't have a good intuition as to what the right level is for this at the moment.\nMy code blindly uses the optimiser setup from the book:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">AdamW</span><span class=\"p\">(</span>\n        <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n        <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.0004</span><span class=\"p\">,</span> <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>I have at best a vague understanding of how those work, at least when using an\noptimiser (LR for simple gradient descent isn't too hard to understand, although it's\nhard to work out an intuition for what the right value might be in any given case).\nAdditionally, in the Chinchilla paper, they talk about using a cosine\nfunction to vary the learning rate, which is something I'm completely unfamiliar\nwith.</p>\n\n<h4 id=\"the-precision\">The precision</h4>\n\n<p>I gained about a day in training time by using AMP and the TF32 tensor cores; however,\nI lost precision.  I don't know for sure, but I suspect that the original weights\nwere trained with pure full-fat FP32.  Perhaps reducing precision lost something?  I know that\nmodern models are often trained with lower precisions, but perhaps that's balanced\nout by something else?</p>\n\n<h4 id=\"the-batch-size\">The batch size</h4>\n\n<p>This is the one that I think it least likely, but it's worth mentioning.  The post that\nI linked to estimating the size of the training run for GPT-2 small mentioned that they\nused a batch size of 512, which (of course) is completely impossible on consumer hardware\nlike mine.  Indeed, I think you'd be lucky to get 512 onto a single 8-GPU node -- we're\ntalking serious cluster training scale here.  Larger batches lead to more stable\nupdates to the gradients.  So maybe that helped for OpenAI when they did their train?  I suspect it did, but I'm pretty much\ncertain that it's not a large part of the difference.</p>\n\n<p>(Counterpoint: Gemini thinks that this might actually be a big part of the problem!\nIt recommends using gradient accumulation -- that is, not stepping the optimiser every\niteration, but instead giving gradients time to build up -- as a way of getting\na larger batch effective batch size.)</p>\n\n<h4 id=\"exploding-gradients\">Exploding gradients</h4>\n\n<p>While it doesn't look like we had any issues with these on the original FineWeb\nand FineWeb-Edu trains, they definitely did kick in on the extended Edu train.\nThe code to clip them is easy enough, and I think it's likely that the original\nGPT-2 trains would have had it.  I doubt this was a major part of the difference,\nbut it probably would have helped, at least a bit.</p>\n\n<hr />\n\n<p>Anyway, I think that's it in terms of differences that I can see between my train and OpenAI's\n(as always, comments welcome -- let me know if you spot any others!),\nso it's time to (finally) wrap this post up.</p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>At the start of this (ridiculously long) post, I asked the question: can we train\na GPT-2 style base model at home on a single RTX 3090.  The answer is a resounding\n\"yes we can\", which is great!  Training base models: not just for the GPU-rich.  If\nyou have a couple of days and a decent graphics card, you can train a Chinchilla-optimal GPT-2 pretty easily.</p>\n\n<p>But the model itself isn't quite as good as the original GPT-2 small one, and I have some ideas\nabout why that might be.  Testing any of those would take quite a long time,\ngiven that each training run takes two days.</p>\n\n<p>Now, my next planned step was to see whether I could work out how to move this up to\nthe cloud and train the same model on an 8x A100 or similar machine on Lambda Labs.\nThis still sounds like an excellent plan!  With his <code>nanochat</code> project, Karpathy trains\na larger model on more tokens in four hours; if we could get the experiment time\ndown to one hour (plausible if training time is linear in both tokens and parameters)\nthen it would be much easier to check out those hypotheses above. <sup class=\"footnote-ref\" id=\"fnref-9\"><a href=\"#fn-9\">9</a></sup></p>\n\n<p>So, I think that's still the right way to go: after training a base model at home\nfor free (if you ignore the electricity costs -- and it's cold enough in Lisbon\nright now that the heat from the PC was probably saving me money on my home heating bill -- and the\ncost of having bought the RTX 3090 in the first place),\nthe next step is to see how cheaply we can train it in the cloud.</p>\n\n<p>Stay tuned :-)</p>\n\n<p><a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud\">Here's a link to the next post in this series</a>.</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>It's useful here, but it does make me wonder how good FineWeb would be for training a base model\nwith a longer context length, however.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-1\" title=\"Jump back to footnote 1 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-2\">\n<p>There are ways to get comparable numbers even with a different tokeniser, using a\nbits-per-byte or nats-per-byte measure.  Let's say we're using the normal\ncross entropy loss with the natural logarithm; that means that loss is expressed\nin nats.  So you add up all of the per-token\nlosses and divide it by the number of bytes across all of the inputs you've\nseen, and that would give you nats-per-byte.  Likewise, if you used <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub></mrow></math> for\ncross entropy, you'd get bits-per-byte.  The latter is used in the Chinchilla\npaper (eg. table A5) as a way to compare their\nmodel with the Gopher model.  I did consider digging into this a bit, but I\nthink it's a bit of a side quest for now.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-2\" title=\"Jump back to footnote 2 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-3\">\n<p>Those evals cost me $0.09 in API credits, which is actually a little more than\nI was expecting -- there were some responses which took quite a while to come back,\nthough, and I believe that the GPT 5.1 model spends time thinking when it seems\nappropriate, so perhaps I spent a bit on thinking tokens.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-3\" title=\"Jump back to footnote 3 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-4\">\n<p>Apart from a reference to a \"dense layer\", which I'm unsure about -- I believe\nit's the linear feed-forward layer after the attention calculations, though, as\nthat doesn't appear elsewhere, and the calculation looks right.  I also noticed\nthat they don't have any terms in there for things like normalisation, which\nseems odd for such a carefully-worked-out formula; I assume they are small enough\nto vanish into the noise.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-4\" title=\"Jump back to footnote 4 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-5\">\n<p>If you want a more careful calculation of the numbers -- and indeed a really nice\nexplanation of some of the details of the Chinchilla paper, I recommend\n<a href=\"https://tomekkorbak.com/2022/10/10/compute-optimal-gpt2/\">this blog post from Tomek Korbak</a>.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-5\" title=\"Jump back to footnote 5 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-6\">\n<p>I hate that we appear to have settled on FLOPs with a lower-case \"s\" for \"floating-point operations\"\nwhen \"FLOPS\" (and equivalently MFLOPS, GFLOPS, TFLOPS) with an upper-case \"S\" already\nmeant \"floating-point operations per second\" because the difference in capitalisation\nshould really not change the units.  But here we are.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-6\" title=\"Jump back to footnote 6 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-7\">\n<p>I estimated the OpenAI weights loss on their own dataset by taking the perplexity\nnumber for the small model from figure 4, which is about 16.5, and then taking its\nnatural log.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-7\" title=\"Jump back to footnote 7 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-8\">\n<p>The authors of the paper call it \"grokking\", which is a great name, but is so\noverloaded in the context of LLMs (even if you disregard xAI's <a href=\"https://x.ai/grok\">Grok</a>)\nthat I'm slightly loath to use it here.  This phenomenon also looks somewhat\nmore limited in scope than I thought -- I'd been under the impression that it happens\na lot with LLMs, but it looks like it's more a thing that happens with small models\ntrained on very structured datasets.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-8\" title=\"Jump back to footnote 8 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-9\">\n<p>It would also be interesting to see how easy it is to offload the optimiser to the CPU:\nin my old fine-tuning experiments I found that freed up a ton of VRAM, so we could benefit\nfrom that and maybe get the batch size up to something closer to the 512 that OpenAI\napparently trained with.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-9\" title=\"Jump back to footnote 9 in the text.\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
  "id": "/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch"
}