{
  "title": "Three Inverse Laws of Robotics",
  "link": "https://susam.net/inverse-laws-of-robotics.html",
  "published": "Mon, 12 Jan 2026 00:00:00 +0000",
  "summary": "<h2 id=\"introduction\">Introduction<a href=\"#introduction\"></a></h2>\n<p>\n  Since the launch of ChatGPT in November 2022, generative artificial\n  intelligence (AI) chatbot services have become increasingly\n  sophisticated and popular.  These systems are now embedded in search\n  engines, software development tools as well as office software.  For\n  many people, they have quickly become part of everyday computing.\n</p>\n<p>\n  I personally find these services incredibly useful, particularly for\n  exploring unfamiliar topics and as a general productivity aid.\n  However, I also think that the way these services are advertised and\n  consumed can pose a danger, especially if we get into the habit of\n  trusting their output without further scrutiny.\n</p>\n<h2 id=\"contents\">Contents<a href=\"#contents\"></a></h2>\n<ul>\n  <li><a href=\"#introduction\">Introduction</a></li>\n  <li><a href=\"#pitfalls\">Pitfalls</a></li>\n  <li><a href=\"#laws\">Inverse Laws of Robotics</a>\n    <ul>\n      <li><a href=\"#non-anthromorphism\">Non-Anthropomorphism</a></li>\n      <li><a href=\"#non-deference\">Non-Deference</a></li>\n      <li><a href=\"#non-abdication-of-responsibility\">Non-Abdication of Responsibility</a></li>\n    </ul>\n  </li>\n  <li><a href=\"#conclusion\">Conclusion</a></li>\n</ul>\n<h2 id=\"pitfalls\">Pitfalls<a href=\"#pitfalls\"></a></h2>\n<p>\n  Certain design choices in modern AI systems can encourage uncritical\n  acceptance of their output.  For example, many popular search\n  engines are already highlighting answers generated by AI at the very\n  top of the page.  When this happens, it is easy to stop scrolling,\n  accept the generated answer and move on.  Over time, this could\n  inadvertently train users to treat AI as the default authority\n  rather than as a starting point for further investigation.  I wish\n  that each such generative AI service came with a brief but\n  conspicuous warning explaining that these systems can sometimes\n  produce output that is factually incorrect, misleading or\n  incomplete.  Such warnings should highlight that habitually trusting\n  AI output can be dangerous.  In my experience, even when such\n  warnings exist, they tend to be minimal and visually deemphasised.\n</p>\n<p>\n  In the world of science fiction, there are the\n  <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics\">Three\n  Laws of Robotics</a> devised by Isaac Asimov, which recur throughout\n  his work.  These laws were designed to constrain the behaviour of\n  robots in order to keep humans safe.  As far as I know, Asimov never\n  formulated any equivalent laws governing how humans should interact\n  with robots.  I think we now need something to that effect to keep\n  ourselves safe.  I will call them the <em>Inverse Laws of\n  Robotics</em>.  These apply to any situation that requires us humans\n  to interact with a robot, where the term 'robot' refers to any\n  machine, computer program, software service or AI system that is\n  capable of performing complex tasks automatically.  I use the term\n  'inverse' here not in the sense of logical negation but to indicate\n  that these laws apply to humans rather than to robots.\n</p>\n<h2 id=\"laws\">Inverse Laws of Robotics<a href=\"#laws\"></a></h2>\n<p>\n  Here are the three inverse laws of robotics:\n</p>\n<ul>\n  <li>\n    Humans must not anthropomorphise AI systems.\n  </li>\n  <li>\n    Humans must not blindly trust the output of AI systems.\n  </li>\n  <li>\n    Humans must remain fully responsible and accountable for\n    consequences arising from the use of AI systems.\n  </li>\n</ul>\n<h3 id=\"non-anthromorphism\">Non-Anthropomorphism<a href=\"#non-anthromorphism\"></a></h3>\n<p>\n  Humans must not anthropomorphise AI systems.  That is, humans must\n  not attribute emotions, intentions or moral agency to them.\n  Anthropomorphism distorts judgement.  In extreme cases,\n  anthropomorphising can lead to emotional dependence.\n</p>\n<p>\n  Modern chatbot systems often sound conversational and empathetic.\n  They use polite phrasing and social cues that closely resemble human\n  interaction.  While this makes them easier and more pleasant to use,\n  it also makes it easier to forget what they actually are: large\n  statistical models producing plausible text based on patterns in\n  data.\n</p>\n<p>\n  I think vendors of AI based chatbot services could do a better job\n  here.  In many cases, the systems are deliberately tuned to feel more\n  human rather than more mechanical.  I would argue that the opposite\n  approach would be healthier in the long term.  A slightly more\n  robotic tone would reduce the likelihood that users mistake fluent\n  language for understanding, judgement or intent.\n</p>\n<p>\n  Whether or not vendors make such changes, the responsibility for\n  avoiding this pitfall still lies with users.  We must actively avoid\n  the habit of treating AI systems as social actors or moral agents.\n  Doing so preserves clear thinking about their capabilities and\n  limitations.\n</p>\n<h3 id=\"non-deference\">Non-Deference<a href=\"#non-deference\"></a></h3>\n<p>\n  Humans must not blindly trust the output of AI systems.\n  AI-generated content must not be treated as authoritative without\n  independent verification appropriate to its context.\n</p>\n<p>\n  This principle is not unique to AI.  In most areas of life, we\n  should not accept information uncritically.  In practice, of course,\n  this is not always feasible.  Not everyone is an expert in medicine\n  or law, so we often rely on trusted institutions and public health\n  authorities for guidance.  However, the guidance published by such\n  institutions is in most cases peer reviewed by experts in their\n  fields.  On the other hand, when we receive an answer to a question\n  from an AI chatbot in a private chat session, there has been no peer\n  review of the particular stochastically generated response presented\n  to us.  Therefore, the onus of critically examining the response\n  falls on us.\n</p>\n<p>\n  Although AI systems today have become quite impressive at certain\n  tasks, they are still known to produce output that would be a\n  mistake to rely on.  Even if AI systems improve to the point of\n  producing reliable output with a high degree of likelihood, due to\n  their inherent stochastic nature, there would still be a small\n  likelihood of producing output that contains errors.  This makes\n  them particularly dangerous when used in contexts where errors are\n  subtle but costly.  The more serious the potential consequences, the\n  higher the burden of verification should be.\n</p>\n<p>\n  In some applications such as formulating mathematical proofs or\n  developing software, we can add an automated verification layer in\n  the form of proof checker or unit tests to verify the output of AI.\n  In other cases, we must independently verify the output ourselves.\n</p>\n<h3 id=\"non-abdication-of-responsibility\">Non-Abdication of Responsibility<a href=\"#non-abdication-of-responsibility\"></a></h3>\n<p>\n  Humans must remain fully responsible for decisions involving AI and\n  accountable for the consequences arising from its use.  If a\n  negative outcome occurs as a result of following AI-generated advice\n  or decisions, it is not sufficient to say, 'the AI told us to do\n  it'.  AI systems do not choose goals, deploy themselves or bear the\n  costs of failure.  Humans and organisations do.  An AI system is a\n  tool and like any other tool, responsibility for its use rests with\n  the people who decide to rely on it.\n</p>\n<p>\n  This is easier said than done, though.  It gets especially tricky in\n  real-time applications like self-driving cars, where a human does\n  not have the opportunity to sufficiently review the decisions taken\n  by the AI system before it acts.  Requiring a human driver to remain\n  constantly vigilant does not solve the problem that the AI system\n  often acts in less time than it takes a human to intervene.  Despite\n  this rather serious limitation, we must acknowledge that if an AI\n  system fails in such applications, the responsibility for\n  investigating the failure and adding additional guardrails should\n  still fall on the humans responsible for the design of the system.\n</p>\n<p>\n  In all other cases, where there is no physical constraint that\n  prevents a human from reviewing the AI output before it is acted\n  upon, any negative consequence arising from the use of AI must fall\n  entirely on the human decision-maker.  As a general principle, we\n  should never accept 'the AI told us so' as an acceptable excuse for\n  harmful outcomes.  Yes, the AI may have produced the recommendation\n  but a human decided to follow it, so that human must be held\n  accountable.  This is absolutely critical to preventing the\n  indiscriminate use of AI in situations where irresponsible use can\n  cause significant harm.\n</p>\n<h2 id=\"conclusion\">Conclusion<a href=\"#conclusion\"></a></h2>\n<p>\n  The three laws outlined above are based on usage patterns I have\n  seen that I feel are detrimental to society.  I am hoping that with\n  these three simple laws, we can encourage our fellow humans to pause\n  and reflect on how they interact with modern AI systems, to resist\n  habits that weaken judgement or blur responsibility and to remain\n  mindful that AI is a tool we choose to use, not an authority we\n  defer to.\n</p>\n<!-- ### -->\n<p>\n  <a href=\"https://susam.net/inverse-laws-of-robotics.html\">Read on website</a> |\n  <a href=\"https://susam.net/tag/miscellaneous.html\">#miscellaneous</a>\n</p>",
  "id": "spilr"
}