{
  "title": "Writing an LLM from scratch, part 32d -- Interventions: adding attention bias",
  "link": "https://www.gilesthomas.com/2026/02/llm-from-scratch-32d-interventions-adding-attention-bias",
  "published": "Fri, 06 Feb 2026 23:55:00 +0000",
  "summary": "<p>I'm still seeing what I can do to improve the test loss for a from-scratch GPT-2 small base model, trained on code based on\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".\nThis is the third intervention I'm trying: adding bias to the attention weight matrices.</p>\n\n<p>In the code from the book, we have this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">MultiHeadAttention</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span>\n        <span class=\"bp\">self</span><span class=\"p\">,</span>\n        <span class=\"n\">d_in</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"p\">,</span>\n        <span class=\"n\">context_length</span><span class=\"p\">,</span>\n        <span class=\"n\">dropout</span><span class=\"p\">,</span>\n        <span class=\"n\">num_heads</span><span class=\"p\">,</span>\n        <span class=\"n\">qkv_bias</span><span class=\"o\">=</span><span class=\"kc\">False</span>\n    <span class=\"p\">):</span>\n        <span class=\"o\">...</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_query</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">d_in</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"n\">qkv_bias</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_key</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">d_in</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"n\">qkv_bias</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_value</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">d_in</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"n\">qkv_bias</span><span class=\"p\">)</span>\n\n        <span class=\"o\">...</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"o\">...</span>\n\n        <span class=\"n\">keys</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_key</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">queries</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_query</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">values</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W_value</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>So: we initialise the weights <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math> and <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math> as linear layers rather than\nsimple matrices of weights, and have a parameter <code>qkv_bias</code> to say whether or not we should\nadd bias to those.  In all of our trains so far we've set that to <code>False</code>.</p>\n\n<p>Why do we have this parameter, and where did it come from?</p>\n<h3 id=\"the-background\">The background</h3>\n\n<p>In Raschka's book, the use of the <code>nn.Linear</code> for these weights is introduced in section 3.4.2\nwith the wording:</p>\n\n<blockquote>\n  <p>We can improve the <code>SelfAttention_v1</code> implementation further by utilizing PyTorch's\n  <code>nn.Linear</code> layers, which effectively perform matrix multiplication when the\n  bias units are disabled.  Additionally, a significant advantage of using <code>nn.Linear</code>\n  instead of manually implementing <code>nn.Parameter(torch.rand(...))</code> is that <code>nn.Linear</code>\n  has an optimized weight initialization scheme, contributing to more stable and\n  effective model training.</p>\n</blockquote>\n\n<p>So, it's presented essentially as a way of getting better weights for our untrained\nmodel, which makes good sense in and of itself -- but, if that's the only reason,\nwhy don't we just hard-wire it to have <code>bias=False</code>?  That would be the sensible thing\nto do if the initialisation were the only reason, but clearly there's more to it\nthan that.</p>\n\n<p>Section 4.1 has a bit more information:</p>\n\n<blockquote>\n  <p><code>qkv_bias</code> determines whether to include a bias vector in the <code>Linear</code> layers\n  of the multi-head attention ... We will initially disable this, following the norms\n  of modern LLMs, but we will revisit it in chapter 6 when we load pretrained\n  GPT-2 weights from OpenAI into our model.</p>\n</blockquote>\n\n<p>That looks like a typo, as the real explanation is in chapter 5, section 5\n(page 164 in my copy), where we do indeed load the OpenAI weights:</p>\n\n<blockquote>\n  <p>OpenAI used bias vectors in the multi-head attention module's linear layers to\n  implement the query, key and value matrix computations.  Bias vectors are not\n  commonly used in LLMs anymore as they don't improve the modeling performance\n  and are thus unnecessary.</p>\n</blockquote>\n\n<p>So, that all makes sense so far.  QKV bias was part of the original GPT-2 models,\nperhaps just because it was standard at the time, inherited from something else,\nor perhaps for some other reason -- I can't find any reference to it in\n<a href=\"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">the actual paper</a>.\nBut people have found it doesn't help, so no-one uses it these days.</p>\n\n<p>But... is there some way in which an LLM of this\nspecific size, or in some other way similar to the GPT-2 small model that we're\ntraining, might in some way benefit from having bias?</p>\n\n<p>That's what this experiment is for :-)</p>\n\n<h3 id=\"parameters\">Parameters</h3>\n\n<p>One thing that occurred to me while setting this up is that we have been training\non a Chinchilla-optimal number of tokens, 20x the number of parameters.  Without\nQKV bias, we have 163,009,536 parameters, so we've been training on 3,260,190,720 tokens,\nrounded up to the nearest batch size, which is 3,260,252,160 in our current setup for\nthese experiments (per-GPU micro-batches of 12, with 8 GPUs, so a total batch size of 96).</p>\n\n<p>These extra bias terms will be parameters, though!  We're essentially making our\nmodel larger by adding them, which changes the Chinchilla calculation.  How much?</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span> <span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;vocab_size&quot;</span><span class=\"p\">:</span> <span class=\"mi\">50257</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;context_length&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1024</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;emb_dim&quot;</span><span class=\"p\">:</span> <span class=\"mi\">768</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;n_heads&quot;</span><span class=\"p\">:</span> <span class=\"mi\">12</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;n_layers&quot;</span><span class=\"p\">:</span> <span class=\"mi\">12</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;drop_rate&quot;</span><span class=\"p\">:</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span>     <span class=\"s2\">&quot;qkv_bias&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span>\n   <span class=\"o\">...</span><span class=\"p\">:</span> <span class=\"p\">}</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">gpt</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">GPTModel</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">GPTModel</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">)</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">p</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">())</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"mi\">163037184</span>\n</code></pre>\n</div>\n\n<p>OK, that's essentially nothing -- 27,648 extra total paramaters on top of 163 million.\nI make it less than two hundredths of a percentage\npoint larger!  The correct number of tokens goes up to 3,260,743,680, so if we wanted\nto be very pedantic, we're under-training.  But I feel like training on a larger dataset\nis worse in terms of comparability between the baseline and our \"intervened-on\" model\nwith QKV bias.</p>\n\n<p>So: we'll train a model with QKV bias on 3,260,252,160 tokens, accepting that it's a tiny bit less than\nChinchilla-optimal.  Let's see how it goes!</p>\n\n<h3 id=\"the-run\">The run</h3>\n\n<p>Here's the <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/main/runs/8xa100m40-qkv-bias/model.json\"><code>model.json</code> config file</a> for this train.\nRunning it gives this training chart:</p>\n\n<p><img alt=\"Training run with QKV bias\" src=\"/post-assets/llm-from-scratch-32d-interventions-adding-attention-bias/training-run-chart.png\" title=\"Training run with QKV bias\" /></p>\n\n<p>Pretty standard, though the loss spikes look less prominent than they have been in\nthe other trains.  Might QKV bias actually help with model stability in some way...?</p>\n\n<p>The train finished with these stats:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 12,329.557 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,252,160</span>\n<span class=\"go\">Throughput: 264,426 tokens/second</span>\n<span class=\"go\">Final train loss: 3.719</span>\n</code></pre>\n</div>\n\n<p>Timing-wise, pretty much indistinguishable from the baseline train's 12,243.523 seconds.  The final train\nloss looks a tad better, but we can't rely on that -- the test set loss is the important\none.</p>\n\n<p>So it was time to download it, <a href=\"https://huggingface.co/gpjt/8xa100m40-qkv-bias\">upload it to Hugging Face Hub</a>, and then on\nto the evals.</p>\n\n<h3 id=\"evals\">Evals</h3>\n\n<p>Firstly, our normal \"how should you continue <code>Every effort moves you</code>\":</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xa100m40-qkv-bias/model.json<span class=\"w\"> </span>runs/8xa100m40-qkv-bias/checkpoints/best/model.safetensors\n<span class=\"go\">Every effort moves you toward success. The right questions are asked to become your business coach and help shape the future of their</span>\n</code></pre>\n</div>\n\n<p>Not bad at all, borderline coherent!  Next, the loss on the test set:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets<span class=\"w\"> </span>runs/8xa100m40-qkv-bias/model.json<span class=\"w\"> </span>runs/8xa100m40-qkv-bias/checkpoints/best/model.safetensors\n<span class=\"go\">Fetching 4 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 1701.54it/s]</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3200/3200 [04:52&lt;00:00, 10.95it/s]</span>\n<span class=\"go\">Loss against our test dataset: 3.669</span>\n</code></pre>\n</div>\n\n<p>Well, crap!  Now that's a surprise.  Let's look at that in the context of the other interventions to see\nhow surprising that is, given Raschka's comments (which were undoubtedly backed\nup by serious research):</p>\n\n<table>\n<thead>\n<tr>\n  <th></th>\n  <th>Test set loss</th>\n  <th>Improvement vs baseline</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>8xa100m40-baseline</td>\n  <td>3.692</td>\n  <td>-</td>\n</tr>\n<tr>\n  <td>8xa100m40-gradient-clipping</td>\n  <td>3.678</td>\n  <td>0.014</td>\n</tr>\n<tr>\n  <td>8xa100m40-qkv-bias</td>\n  <td>3.669</td>\n  <td>0.023</td>\n</tr>\n<tr>\n  <td>8xa100m40-remove-dropout</td>\n  <td>3.641</td>\n  <td>0.051</td>\n</tr>\n</tbody>\n</table>\n\n<p>So, adding QKV bias actually improved our test set loss by more than gradient clipping\ndid!</p>\n\n<p>The loss spikes in the training chart look smaller than in the other trains <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup>, so, speculating\nwildly, perhaps with a model of this size, the bias stabilises things somehow?  Or perhaps\nwhat we're seeing is the model become that tiny bit smarter because it has some extra parameters\n-- albeit less than 0.02 <em>percent</em> more?</p>\n\n<p>I'm not going to spend time investigating things now, but this is a really interesting result.\nOne extra thing that does occur to me is that the direction research has taken since GPT-2 has\ndefinitely been in the direction of larger models.  The attention weight matrices are\nsized <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>d</mi><mtext>emb</mtext></msub><mi>&#x000d7;</mi><msub><mi>d</mi><mtext>emb</mtext></msub></mrow></math>, so excluding bias they have <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msubsup><mi>d</mi><mtext>emb</mtext><mn>2</mn></msubsup></mrow></math> weights\neach.  Bias adds on another <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>d</mi><mtext>emb</mtext></msub></mrow></math>.  So, as a model scales up, the attention-related\nnon-bias weights will scale quadratically -- doubling <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>d</mi><mtext>emb</mtext></msub></mrow></math> will square their number --\nwhile the bias weights will scale linearly.</p>\n\n<p>So perhaps it's just that the effect -- whatever causes it -- gets rapidly swamped\nas you scale out of toy-model territory.  That, at least, seems pretty plausible.</p>\n\n<p>One final note to self, though: these improvements are small enough that I do find\nmyself wondering whether or not it might be some kind of noise, despite the setting of\nthe random seeds I'm doing:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">42</span>\n    <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">manual_seed_all</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>I think that at the end of this, before I do a final train, it would be worth doing\nanother baseline train and measuring the test set loss again, and doing another comparison.\nIf it comes out exactly the same -- and I can bump up the number of significant figures\nin the output, it's just a formatting parameter -- then I don't need to worry.  But if\nthey vary to some degree, perhaps I'll need to update my mental model of what level of\nfinding is significant, and what isn't.</p>\n\n<h3 id=\"summing-up\">Summing up</h3>\n\n<p>I think it goes without saying that QKV bias definitely goes onto the list of interventions\nwe want to add when training our best-possible GPT-2 small-scale model, assuming that the\nrandom seed test goes well.  That surprises\nme a bit, I was expecting it to have negligible impact!  That, of course, is why it's worth\ndoing these tests.</p>\n\n<p>Next up, I think, is trying to understand how we can tweak the learning rate, and its associated\nparameters like weight decay.  This will need a bit of a deep dive, so you can expect the next\npost late next week, or perhaps even later.  I'm sure you can't wait ;-)</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>Note to self: is there some way I could quantitatively measure those?&#160;<a class=\"footnoteBackLink\" href=\"#fnref-1\" title=\"Jump back to footnote 1 in the text.\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
  "id": "/2026/02/llm-from-scratch-32d-interventions-adding-attention-bias"
}