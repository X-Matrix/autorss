{
  "title": "Writing an LLM from scratch, part 32c -- Interventions: removing dropout",
  "link": "https://www.gilesthomas.com/2026/02/llm-from-scratch-32c-interventions-removing-dropout",
  "published": "Thu, 05 Feb 2026 23:35:00 +0000",
  "summary": "<p>This is the second in my series of attempts to improve the loss on my test dataset\n-- interventions, as I'm calling them --\nfor a from-scratch GPT-2 small base model, trained on code based on\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".</p>\n\n<p>Last time around I saw <a href=\"/2026/02/llm-from-scratch-32b-interventions-gradient-clipping\">what gradient clipping can do</a> --\nit improved loss over <a href=\"/2026/02/llm-from-scratch-32a-interventions-baseline-model\">the baseline</a>\nby 0.014, bringing it down from 3.692 to 3.678.  Not much, but it's something!</p>\n\n<p>This time, I wanted to see what happened if we trained without dropout.  Would removing it make\nthe test loss worse, or better?</p>\n<h3 id=\"background\">Background:</h3>\n\n<p>In a blog post last summer about\n<a href=\"https://magazine.sebastianraschka.com/i/170506328/21-removing-dropout\">architectural advances in LLMs since GPT-2</a>,\nSebastian Raschka wrote:</p>\n\n<blockquote>\n  <p>Dropout (2012) is a traditional technique to prevent overfitting by randomly\n  \"dropping out\" (i.e., setting to zero) a fraction of the layer activations or\n  attention scores (Figure 3) during training. However, dropout is rarely used\n  in modern LLMs, and most models after GPT-2 have dropped it (no pun intended).</p>\n  \n  <p>I assume that dropout was originally used in GPT-2 because it was inherited\n  from the original transformer architecture. Researchers likely noticed that\n  it does not really improve LLM performance (I observed the same in my\n  small-scale GPT-2 replication runs). This is likely because LLMs are typically\n  trained for only a single epoch over massive datasets, which is in contrast to\n  the multi-hundred-epoch training regimes for which dropout was first\n  introduced. So, since LLMs see each token only once during training, there is\n  little risk of overfitting.</p>\n</blockquote>\n\n<p>That makes quite a lot of sense.  My own understanding of dropout was that it was\na bit broader than just preventing overfitting -- it seemed to me to be similar\nto the\n<a href=\"/2025/03/dropout-and-mandatory-vacation\">mandatory vacation policies that financial firms user to prevent over-dependence on individuals</a>.\nMy instinct was that having knowledge distributed across different weights in the\nmodel was good in and of itself, even beyond its benefit on multiple-epoch training.</p>\n\n<p>But it is quite a high price to pay.\nWith the training parameters we've been using we're literally discarding 10% of our calculations' results --\nattention weights, feed-forward neuron activations, and so on -- as we do the forward pass.\nIt's easy to see why it would harm training.</p>\n\n<p>Let's give it a go.</p>\n\n<h3 id=\"the-training-run\">The training run</h3>\n\n<p>The nice thing about this one is that, unlike the gradient clipping experiment,\nI didn't have to write any new code.  The dropout level was already controlled by\na setting in the <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/main/runs/8xa100m40-remove-dropout/model.json\"><code>model.json</code> file</a>,\nso by setting that to zero for this run, I could just kick it off and let it\ndo its thing while I worked on something else:</p>\n\n<p>Here's what the training run chart looked like (please disregard the stuff about\ngrad norms in the title and the axis -- I'll remove that for the next train):</p>\n\n<p><img alt=\"Training chart for zero-dropout run\" src=\"/post-assets/llm-from-scratch-32c-interventions-removing-dropout/training-chart.png\" title=\"Training chart for zero-dropout run\" /></p>\n\n<p>As you can see, we still have loss spikes, including one just after global step 20,000\nthat lasts for several checkpoint periods of 617 steps.  I imagine gradient clipping\nmight have helped with that, but I'm very deliberately testing each intervention in\nisolation.</p>\n\n<p>At the end of the training run, we got this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 11,376.067 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,252,160</span>\n<span class=\"go\">Throughput: 286,589 tokens/second</span>\n<span class=\"go\">Final train loss: 3.621</span>\n</code></pre>\n</div>\n\n<p>So, interestingly, it took 967 seconds -- about 16 minutes -- less time than the\ngradient clipping run, and about 15 minutes less than the baseline train.  So\nwhile gradient clipping added on a small amount of time (or maybe that was just noise),\ndropping dropout certainly seems to speed things up!  I guess there's quite a lot of\nwork involved in generating and applying the random masks that drop things out as we're\ndoing the forward pass.</p>\n\n<p>Anyway, with the model trained, it was time to download it,\n<a href=\"https://huggingface.co/gpjt/8xa100m40-remove-dropout\">upload it to Hugging Face Hub</a>, and run the evals.</p>\n\n<h3 id=\"evals\">Evals</h3>\n\n<p>Firstly, the smoke test, where it just needs to continue the sequence <code>Every effort moves you</code>,\nit came up with something reasonably coherent:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xa100m40-remove-dropout/model.json<span class=\"w\"> </span>runs/8xa100m40-remove-dropout/checkpoints/best/model.safetensors\n<span class=\"go\">Every effort moves you to make the world a better place.</span>\n<span class=\"go\">As an international student of the arts in the UK,</span>\n</code></pre>\n</div>\n\n<p>...but it was on the test of the loss on the training set that it was most impressive:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets/<span class=\"w\"> </span>runs/8xa100m40-remove-dropout/model.json<span class=\"w\"> </span>runs/8xa100m40-remove-dropout/checkpoints/best/model.safetensors\n<span class=\"go\">Fetching 4 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 1086.75it/s]</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3200/3200 [04:54&lt;00:00, 10.87it/s]</span>\n<span class=\"go\">Loss against our test dataset: 3.641</span>\n</code></pre>\n</div>\n\n<p>That's a bigger improvement on the baseline train's 3.692 than gradient clipping:\n0.051, which is more than three times the improvement!</p>\n\n<p>Let's start keeping a table of these:</p>\n\n<table>\n<thead>\n<tr>\n  <th></th>\n  <th>Test set loss</th>\n  <th>Improvement vs baseline</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>8xa100m40-baseline</td>\n  <td>3.692</td>\n  <td>-</td>\n</tr>\n<tr>\n  <td>8xa100m40-gradient-clipping</td>\n  <td>3.678</td>\n  <td>0.014</td>\n</tr>\n<tr>\n  <td>8xa100m40-remove-dropout</td>\n  <td>3.641</td>\n  <td>0.051</td>\n</tr>\n</tbody>\n</table>\n\n<p>Now, of course, we don't know how these different interventions combine together --\nit would be naive to think that if we did both gradient clipping and dropout\nremoval, we'd get a total loss reduction of 0.014 + 0.051 -- but, especially with that\nlong-lived loss spike in our training run -- it does feel like they might play well\ntogether.</p>\n\n<h3 id=\"wrapping-up\">Wrapping up</h3>\n\n<p>So, that's dropout covered.  Which one next?  I think a nice easy one that I should\nbe able to get done on a Friday will be adding bias to the attention weight calculations.\nLet's give that a go and see if it makes things worse or better!</p>\n\n<p>Stay tuned...</p>\n\n<p><a href=\"/2026/02/llm-from-scratch-32d-interventions-adding-attention-bias\">Here's a link to the next post in this series</a>.</p>",
  "id": "/2026/02/llm-from-scratch-32c-interventions-removing-dropout"
}