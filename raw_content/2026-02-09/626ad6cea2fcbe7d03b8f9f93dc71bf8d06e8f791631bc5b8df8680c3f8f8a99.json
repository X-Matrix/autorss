{
  "title": "Hello Deep Learning: Dropout, data augmentation, weight decay and quantisation",
  "link": "https://berthub.eu/articles/posts/dropout-data-augmentation-weight-decay/",
  "published": "Thu, 30 Mar 2023 12:00:07 +0200",
  "summary": "This page is part of the Hello Deep Learning series of blog posts. You are very welcome to improve this page via GitHub!\nIn the previous chapter we found ways to speed up our character recognition learning by a factor of 20 by using a better optimizer, and a further factor of four by cleverly using threads using a &lsquo;shared nothing architecture&rsquo;. We also learned how we can observe the development of parameters.",
  "id": "https://berthub.eu/articles/posts/dropout-data-augmentation-weight-decay/"
}