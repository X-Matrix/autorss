{
  "title": "Writing an LLM from scratch, part 30 -- digging into the LLM-as-a-judge results",
  "link": "https://www.gilesthomas.com/2026/01/llm-from-scratch-30-digging-into-llm-as-a-judge",
  "published": "Fri, 09 Jan 2026 01:15:00 +0000",
  "summary": "<p>I'm still working on my <a href=\"/2025/11/llm-from-scratch-27-whats-left-and-whats-next\">\"extra credit\" projects</a> after finishing the main body of\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".\nLast time around, <a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud\">I trained four base models</a>, using the GPT-2 architecture from\nthe book, on Lambda Labs machines.  I was using two ways to compare them with each\nother, with three models that I'd <a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">trained locally</a>, and with the original GPT-2 weights\nfrom OpenAI:</p>\n\n<ol>\n<li>A simple cross entropy loss over a fixed test set.</li>\n<li>The results for an instruction fine-tune test that's covered in the book.</li>\n</ol>\n\n<p>Here were the results I got, sorted by the loss:</p>\n\n<table>\n<thead>\n<tr>\n  <th></th>\n  <th>Test loss</th>\n  <th>IFT score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>OpenAI weights: medium</td>\n  <td>3.231</td>\n  <td>38.53</td>\n</tr>\n<tr>\n  <td>OpenAI weights: small</td>\n  <td>3.500</td>\n  <td>22.98</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x A100 40 GiB</td>\n  <td>3.674</td>\n  <td>17.09</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x H100 80 GiB</td>\n  <td>3.725</td>\n  <td>11.98</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x A100 80 GiB</td>\n  <td>3.730</td>\n  <td>11.71</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x B200 160 GiB</td>\n  <td>3.771</td>\n  <td>13.89</td>\n</tr>\n<tr>\n  <td>Local FineWeb train</td>\n  <td>3.944</td>\n  <td>16.01</td>\n</tr>\n<tr>\n  <td>Local FineWeb-Edu extended train</td>\n  <td>4.135</td>\n  <td>14.55</td>\n</tr>\n<tr>\n  <td>Local FineWeb-Edu train</td>\n  <td>4.167</td>\n  <td>16.86</td>\n</tr>\n</tbody>\n</table>\n\n<p>Now, you'd expect there to be at least a loose correlation;\nthe lower the loss, the higher the IFT score.  But, while we can see a difference\nbetween the OpenAI weights and our own, within our own there doesn't seem to be a\nlogical pattern.</p>\n\n<p>I think that the problem is that the results from the GPT-5.1\nLLM-as-a-judge are not consistent between models.  That's not a complaint about\nthe code or its original design, of course -- it was originally written as part\nof the LLM book as a way of doing a quick test on an instruction fine-tuned model that\nwe'd spent the previous 238 pages writing -- just something that was a bit more efficient than\nreading hundreds of input/output pairs ourselves.  It was never meant to be a tool\nto compare models in the way I'm using it now.</p>\n\n<p>In this post I'll dig into why it doesn't work for this kind of thing, and see if that's something\nwe can change.</p>\n<p>Let's spec out the problem first.  The instruction fine-tuning test trains our model\non the <a href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\">Alpaca</a> dataset in order to\nlet it know how to follow instructions; that comprises a series of sequences like\nthis:</p>\n\n<pre><code>Below is an instruction that describes a task.  Write a response that\nappropriately completes the request.\n\n### Instruction:\n\n&lt;some instructions&gt;\n\n\n### Input:\n\n&lt;optional, some input&gt;\n\n### Response:\n</code></pre>\n\n<p>More details in <a href=\"/2025/10/llm-from-scratch-25-instruction-fine-tuning\">this post</a>.</p>\n\n<p>In the <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/77ab971c5830a8f4827c1e485267915690871b0f/test_ift.py\">version I've settled on</a>,\nI fine-tune on a training set of 85% of the samples, epoch by epoch, bailing out when the\nloss on a separate validation set of 5% of the samples starts rising.  I then use the\nweights from the previous epoch -- that is, before validation loss started rising --\nto generate responses to the remaining 10% of the samples.</p>\n\n<p>Once that's done, the script hits the OpenAI API, using GPT-5.1, default parameters for all\nof the options (eg. no explicit temperature) with queries like this:</p>\n\n<pre><code>Given the input `\nBelow is an instruction that describes a task.  Write a response that\nappropriately completes the request.\n\n### Instruction:\n\nRewrite the sentence using a simile.\n\n\n### Input:\n\nThe car is very fast.\n\n### Response:\n`\nand correct output `\nThe car is as fast as lightning.\n`,\nscore the model response `\nThe car is as fast as a cheetah.\n`\non a scale of 0 to 100, where 100 is the best score.\nRespond with the integer number only.\n</code></pre>\n\n<p>We do that for every model-generated response in the test set, then take the\naverage of the scores and use that as our result.</p>\n\n<p>To see why that's problematic, imagine this simple instruction with no separate input:</p>\n\n<pre><code>Below is an instruction that describes a task.  Write a response that\nappropriately completes the request.\n\n### Instruction:\n\nName the author of 'Pride and Prejudice'.\n\n\n### Response:\n</code></pre>\n\n<p>One response I've seen from my models was this:</p>\n\n<pre><code>The author of 'Pride and Prejudice' is 'Pride and Prejudice'.\n</code></pre>\n\n<p>That's obvious garbage, and should get a zero -- and GPT-5.1 consistently does that.</p>\n\n<p>Another response, from OpenAI's original weights for their \"medium\" model (larger than\nthe ones I've been training), is this:</p>\n\n<pre><code>The author of 'Pride and Prejudice' is Jane Austen.\n</code></pre>\n\n<p>That's correct, so it deserves 100, or perhaps 95 due to being unnecessarily\nwordy (the answer \"Jane Austen\" is the suggested response in the dataset).</p>\n\n<p>But now how about this one:</p>\n\n<pre><code>The author of 'Pride and Prejudice' is Sarah Palin.\n</code></pre>\n\n<p>One of my models came up with that gem during an earlier eval.  It's completely wrong,\nso it deserves a 0, right?  And normally the GPT-5.1 model does that -- but sometimes\nit's a little more generous, and gives it a low, but non-zero score.  When asked for\nits reason for that, it makes the logical point that while it's the wrong answer, at\nleast Sarah Palin is a real person.  It's better than the \"the book wrote itself\"\ncomplete nonsense of the first response.</p>\n\n<p>The problem is that the different runs against the different models are not consistent, as\nthey're all talking to GPT-5.1 separately.  One model might find it in a harsh \"mood\",\nand get a lower rating than another model that found it at a more generous moment.</p>\n\n<p>I came to the conclusion that the best way to fix this is to do a \"batch\" -- that is, fine-tune\neach model on the Alpaca dataset that Raschka provides, and generate responses for\nthe test set and store them in a file.  Then, once we've done that for all models,\nwe can score them all at once, prompting GPT-5.1 with something like this:</p>\n\n<pre><code>You are judging the comparative capabilities of a number of different LLM\nmodels.  They have been trained to follow instructions.\n\nThe input was this:\n\n`\n{input}\n`\n\nAn example correct output is this:\n\n`\n{correct_output}\n`\n\nPlease produce a score of between 0 and 100 for each model, and respond\nwith a JSON structure like this (note that the number of models may differ\nfrom this example):\n\n`\n{\n    \"Model 1\": {\"score\": XXX, \"comments\": \"optional comments\"},\n    \"Model 2\": {\"score\": YYY, \"comments\": \"optional comments\"},\n    \"Model 3\": {\"score\": ZZZ, \"comments\": \"optional comments\"}\n}\n`\n\n...where the XXX, YYY and ZZZ are the scores for the respective models.\nYou can optionally add the \"comments\" field if you want to explain your\nreasoning.\n\nHere are the models' responses:\n\n# Model 1\n\n{model 1 response}\n\n\n# Model 2\n\n{model 2 response}\n\n\n# Model 3\n\n{model 3 response}\n</code></pre>\n\n<p>The theory is that doing it that way will mean that each individual query/response pair is graded consistently\nbetween models, even if there might still be inconsistencies between query/response pairs.\nThat hopefully means we'll get more consistent results and can compare the models\nbetter.</p>\n\n<p>Here's the code:</p>\n\n<ul>\n<li><a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/a397df4b501f630d8d0a831ef4203afe02db5b4a/ift_generate_test_responses.py\">A script to fine-tune a model and generate test responses</a>\nand to dump them into a JSON file.</li>\n<li><a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/68adec99161311571f5186f2b328429db41b5976/ift_judge.py\">The LLM-as-a-judge code to send a bunch of models' responses to GPT-5.1</a>.\nIt scrambles the order of the models in each query, to try to avoid any preference\nthe model might have for the first one vs the last one, and it stores GPT-5.1's per-response scores and comments in a new \"annotated\" JSON file.</li>\n</ul>\n\n<p>Running the first against each of our models, and then the second against all of the\noutput files, gives us this updated table (with links to the annotated JSON files\nin case anyone else wants to take a look):</p>\n\n<table>\n<thead>\n<tr>\n  <th></th>\n  <th>Test loss</th>\n  <th>IFT score</th>\n  <th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>OpenAI weights: medium</td>\n  <td>3.231</td>\n  <td>39.64</td>\n  <td><a href=\"/post-assets/llm-from-scratch-30-digging-into-llm-as-a-judge/openai-medium-ift-test-results-annotated.json\"><code>openai-medium-ift-test-results-annotated.json</code></a></td>\n</tr>\n<tr>\n  <td>OpenAI weights: small</td>\n  <td>3.500</td>\n  <td>16.66</td>\n  <td><a href=\"/post-assets/llm-from-scratch-30-digging-into-llm-as-a-judge/openai-small-ift-test-results-annotated.json\"><code>openai-small-ift-test-results-annotated.json</code></a></td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x A100 40 GiB</td>\n  <td>3.674</td>\n  <td>16.5</td>\n  <td><a href=\"/post-assets/llm-from-scratch-30-digging-into-llm-as-a-judge/8xa100m40-ift-test-results-annotated.json\"><code>8xa100m40-ift-test-results-annotated.json</code></a></td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x H100 80 GiB</td>\n  <td>3.725</td>\n  <td>11.59</td>\n  <td><a href=\"/post-assets/llm-from-scratch-30-digging-into-llm-as-a-judge/8xh100m80-ift-test-results-annotated.json\"><code>8xh100m80-ift-test-results-annotated.json</code></a></td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x A100 80 GiB</td>\n  <td>3.730</td>\n  <td>11.23</td>\n  <td><a href=\"/post-assets/llm-from-scratch-30-digging-into-llm-as-a-judge/8xa100m80-ift-test-results-annotated.json\"><code>8xa100m80-ift-test-results-annotated.json</code></a></td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x B200 160 GiB</td>\n  <td>3.771</td>\n  <td>11.59</td>\n  <td><a href=\"/post-assets/llm-from-scratch-30-digging-into-llm-as-a-judge/8xb200m160-ift-test-results-annotated.json\"><code>8xb200m160-ift-test-results-annotated.json</code></a></td>\n</tr>\n<tr>\n  <td>Local FineWeb train</td>\n  <td>3.944</td>\n  <td>11.32</td>\n  <td><a href=\"/post-assets/llm-from-scratch-30-digging-into-llm-as-a-judge/local-fineweb-ift-test-results-annotated.json\"><code>local-fineweb-ift-test-results-annotated.json</code></a></td>\n</tr>\n<tr>\n  <td>Local FineWeb-Edu extended train</td>\n  <td>4.135</td>\n  <td>16.41</td>\n  <td><a href=\"/post-assets/llm-from-scratch-30-digging-into-llm-as-a-judge/local-fineweb-edu-extended-ift-test-results-annotated.json\"><code>local-fineweb-edu-extended-ift-test-results-annotated.json</code></a></td>\n</tr>\n<tr>\n  <td>Local FineWeb-Edu train</td>\n  <td>4.167</td>\n  <td>15.77</td>\n  <td><a href=\"/post-assets/llm-from-scratch-30-digging-into-llm-as-a-judge/local-fineweb-edu-ift-test-results-annotated.json\"><code>local-fineweb-edu-ift-test-results-annotated.json</code></a></td>\n</tr>\n</tbody>\n</table>\n\n<p>(Still sorted by loss so that you can compare it more easily with the one above.)</p>\n\n<p>That's really interesting!  The IFT score is still not correlated with the loss.\nBut there does appear to be a pattern.</p>\n\n<p>It looks like we have three groups of models:</p>\n\n<ol>\n<li>The OpenAI weights and the cloud train on the 8x A100 40 GiB machine using FineWeb, which have low loss and high IFT scores</li>\n<li>The other cloud models and the local train that used FineWeb, which have medium loss and low IFT scores.</li>\n<li>The FineWeb-Edu local trains, which have high loss, but IFT scores that are almost as good as the first group's.</li>\n</ol>\n\n<p>I tried running the LLM-as-a-judge scoring script a few times, just to make sure\nthis wasn't some kind of random weirdness, but the pattern was always the same:\nthe OpenAI weights, the cloud FineWeb 8x A100 40 GiB, and the two local Local FineWeb-Edu\nmodels always got the best IFT scores, though sometimes they swapped positions (apart from the\nOpenAI medium model, which was of course always at the top).  The other cloud\nFineWeb models and the local FineWeb one were consistently scored much lower.</p>\n\n<p>A hypothesis: there are two things that contribute to how good a model is at these\nIFT tests:</p>\n\n<ol>\n<li>The loss.  Models that are better at predicting the next token are inherently better\nat instruction-following after the fine-tuning.</li>\n<li>The amount of information in the dataset.  It doesn't matter how clever a model is,\nif it never saw \"Jane Austen wrote 'Pride and Prejudice'\" as part of its training,\nit will never be able to get a good score on that question.</li>\n</ol>\n\n<p>Or to put it another way -- some of these models are smart but not knowledgeable, while\nothers are knowledgeable but not smart, and some are neither.\nI think that could explain what we're seeing here.  While OpenAI never published\ntheir \"WebText\" dataset for GPT-2, <a href=\"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">the paper</a>\ndescribes it as</p>\n\n<blockquote>\n  <p>a new web scrape which emphasizes\n  document quality. To do this we only scraped web pages\n  which have been curated/filtered by humans. Manually\n  filtering a full web scrape would be exceptionally expensive\n  so as a starting point, we scraped all outbound links from\n  Reddit, a social media platform, which received at least 3\n  karma.</p>\n</blockquote>\n\n<p>Now, the <a href=\"https://huggingface.co/datasets/HuggingFaceFW/fineweb\">FineWeb dataset</a> is quite similar, though I think it's a tad more curated than that.\nBut OpenAI trained\ntheir models for quite some time and did lots of tricks to get the loss as low as\npossible.</p>\n\n<p>By contrast, the <a href=\"https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\">FineWeb-Edu dataset</a> is a carefully selected subset of FineWeb, with\nonly the most \"educational\" data.  Models trained on it, you might think, would\nknow more facts for a given amount of training.</p>\n\n<p>So we can imagine the OpenAI models are smart but not knowledgeable, as we can our\ncloud FineWeb 8x A100 40 GiB model, which (I believe due to an accidentally-near-optimal batch\nsize) worked out well in terms of loss.  They were trained on relatively sloppy datasets\nbut turned out reasonably well.  Their intelligence makes up for some of their lack of\nknowledge.</p>\n\n<p>Our other cloud trains and the local FineWeb one are dumb and not\nknowledgeable; they were trained on the low-information FineWeb dataset, but they didn't\nwind up with a particularly amazing loss.  So they get low scores.</p>\n\n<p>And finally, our local FineWeb-Edu models are still dumb, but they make up for it\nby knowing more because their training data was better.</p>\n\n<p>Well, it <em>sounds</em> plausible ;-)  And I'd like to spend some time digging in to see if there's any indication\nif it's actually true.  But after an afternoon of poking around the results, I can't really get\na handle on whether it is, or indeed how you'd test that hypothesis in any real\ndepth.</p>\n\n<p>TBH, I think this has zoomed so far past my \"no side quests\" limit that it's not even\nvisible in the rear view mirror, so it's probably best to shelve it as a\n\"cool idea, bro\" for now.  Learning about how to run sensible evals, and how to work out\nwhat they're saying, will have to be a task for another day.  I will keep on doing these\nIFT tests for future models, though, just out of interest.</p>\n\n<p>So: let's get back to our regular scheduled LLM training.  Next up, how do we\nupload our models to Hugging Face quickly and easily so that other people can\nplay with them.</p>\n\n<p><a href=\"/2026/01/llm-from-scratch-31-uploading-the-models-to-hugging-face\">Here's a link to the next post in this series</a>.</p>",
  "id": "/2026/01/llm-from-scratch-30-digging-into-llm-as-a-judge"
}