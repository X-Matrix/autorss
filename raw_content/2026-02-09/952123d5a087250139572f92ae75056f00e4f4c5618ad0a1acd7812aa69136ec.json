{
  "title": "air traffic control: the IBM 9020",
  "link": "https://computer.rip/2026-01-17-air-traffic-control-9020.html",
  "published": "17 Jan 2026 00:00:00 UT",
  "summary": "<p>Previously on <em>Computers Are Bad,</em> we discussed the <a href=\"https://computer.rip/2025-05-11-air-traffic-control.html\">early history of air\ntraffic control in the United States</a>.\nThe technical demands of air traffic control are well known in computer history\ncircles because of the prominence of SAGE, but what's less well known is that\nSAGE itself was not an air traffic control system at all. SAGE was an air <em>defense</em>\nsystem, designed for the military with a specific task of ground-controlled\ninterception (GCI). There is natural overlap between air defense and air\ntraffic control: for example, both applications require correlating aircraft\nidentities with radar targets. This commonality lead the Federal Aviation Agency\n(precursor to today's FAA) to launch a joint project with the Air Force to\nadapt SAGE for civilian ATC.</p>\n<p>There are also significant differences. In general, SAGE did not provide any\nsafety functions. It did not monitor altitude reservations for uniqueness,\nit did not detect loss of separation, and it did not integrate instrument\nprocedure or terminal information. SAGE would need to gain these features to\nmeet FAA requirements, particularly given the mid-century focus on\nmid-air collisions (a growing problem, with increasing air traffic, that SAGE\ndid nothing to address).</p>\n<p>The result was a 1959 initiative called SATIN, for SAGE Air Traffic Integration.\nAround the same time, the Air Force had been working on a broader enhancement\nprogram for SAGE known as the Super Combat Center (SCC). The SCC program was\nseveral different ideas grouped together: a newer transistorized computer to\nhost SAGE, improved communications capabilities, and the relocation of Air\nDefense Direction Centers from conspicuous and vulnerable &quot;SAGE Blockhouses&quot;\nto hardened underground command centers, specified as an impressive 200 PSI\nblast overpressure resistance (for comparison, the hardened telecommunication\nfacilities of the Cold War were mostly specified for 6 or 10 PSI). </p>\n<p>At the program's apex, construction of the SCCs seemed so inevitable that the\nAir Force suspended the original SAGE project under the expectation that SCC\nwould immediately obsolete it. For example, my own Albuquerque was one of the\nlast Air Defense Sectors scheduled for installation of a SAGE computer. That\ninstallation was canceled; while a hardened underground center had never\nbeen in the cards for Albuquerque, the decision was made to otherwise build\nAlbuquerque to the newer SCC design, including the transistorized computer.\nBy the same card, the FAA's interest in a civilian ATC capability, and thus\nthe SATIN project, came to be grouped together with the SCC program as just\nanother component of SAGE's next phase of development.</p>\n<p>SAGE had originally been engineered by MIT's Lincoln Laboratory, then the\nnational center of expertise in all things radar. By the late 1950s a\nlarge portion of the Lincoln Laboratory staff were working on air defense\nsystems and specifically SAGE. Those projects had become so large that\nMIT opted to split them off into a new organization, which through some\nobscure means came to be called the MITRE Corporation. MITRE was to be a\ngeneral military R&amp;D and consulting contractor, but in its early years it\nwas essentially the SAGE company.</p>\n<p>The FAA contracted MITRE to deliver the SATIN project, and MITRE subcontracted\nsoftware to the Systems Development Corporation, originally part of RAND and\namong the ancestors of today's L3Harris. For the hardware, MITRE had long\nused IBM, who designed and built the original AN/FSQ-7 SAGE computer and\nits putative transistorized replacement, the AN/FSQ-32. MITRE began a\nseries of engineering studies, and then an evaluation program on prototype\nSATIN technology.</p>\n<p>There is a somewhat tenuous claim that you will oft see repeated, that the\nAN/FSQ-7 is the largest computer ever built. It did occupy the vast majority\nof the floorspace of the four-story buildings built around it. The power\nconsumption was around 3 MW, and the heat load required an air conditioning\nsystem at the very frontier of HVAC engineering (you can imagine that nearly\nall of that 3 MW had to be blown out of the building on a continuing basis).\nOne of the major goals of the AN/FSQ-32 was reduced size and power consumption,\nwith the lower heat load in particular being a critical requirement for\ninstallation deep underground. Of course, the &quot;deep underground&quot; part more\nthan wiped out any savings from the improved technology.</p>\n<h2>From Air Defense to Air Traffic Control</h2>\n<p>By the late 1950s, enormous spending for the rapid built-out of defense systems including\nSAGE and the air defense radar system (then the <a href=\"https://computer.rip/2021-08-26-a-permanent-solution.html\">Permanent System</a>)\nhad fatigued the national budget and Congress. The winds of the Cold War had\nonce again changed. In 1959, MITRE had begun operation of a prototype\ncivilian SAGE capability called CHARM, the CAA High Altitude Remote\nMonitor (CAA had become the FAA during the course of the CHARM effort).\nCHARM used MIT's Whirlwind computer to process high-altitude radar\ndata from the Boston ARTCC (Air Route Traffic Control Center), which it displayed to operators while\ncontinuously evaluating aircraft movements for possible conflicts. CHARM was\ndesigned for interoperability with SAGE, the ultimate goal being the addition\nof the CHARM software package to existing SAGE computers. None of that would\never happen; by the time the ball dropped for the year 1960 the Super\nCombat Center program had been almost completely canceled. SATIN, and the\nwhole idea of civilian air traffic control with SAGE, became blast damage.</p>\n<p>In 1961, the Beacon Report concluded that there was an immediate need for a\ncentralized, automated air traffic control system. Mid-air collisions had\nbecome a significant political issue, subject of congressional hearings and\nGAO reports. The FAA seemed to be failing to rise to the task of safe civilian\nATC, a perilous situation for such a new agency... and after the cancellation of\nthe SCCs, the FAA's entire plan for computerized ATC was gone.</p>\n<p>During the late 1950s and 1960s, the FAA adopted computer systems in a piecemeal\nfashion. Many enroute control centers (ARTCCs), and even some terminal facilities,\nhad some type of computer system installed. These were often custom software\nrunning on commodity computers, limited to tasks like recording flight plans and\nmaking them available to controllers at other terminals. Correlation of radar\ntargets with flight plans was generally manual, as were safety functions like\nconflict detection.</p>\n<p>These systems were limited in scale—the biggest problem\nbeing that some ARTCCs remained completely manual even in the late 1960s.\nOn the upside, they demonstrated much of the technology required, and provided\na test bed for implementation. Many of the individual technical components of\nATC were under development, particularly within IBM and Raytheon, but there\nwas no coordinated nationwide program. This situation resulted in part from a\nvery intentional decision by the FAA to grant more decision making power to\nits regional offices, a concept that was successful in some areas but in\nretrospect disastrous in others. In 1967, the Department of Transportation was\nformed as a new cabinet-level executive department. The FAA, then the Federal\nAviation Agency, was reorganized into DOT and renamed the Federal Aviation\nAdministration. The new Administration had a clear imperative from both the\nPresident and Congress: figure out air traffic control.</p>\n<p>In the late 1960s, the FAA coined a new term: the National Airspace System <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup>,\na fully standardized, nationwide system of procedures and systems that\nwould safely coordinate air traffic into the indefinite future. Automation\nof the NAS began with NAS Enroute Stage A, which would automate the ARTCCs\nthat handled high-altitude aircraft on their way between terminals. The\nremit was more or less &quot;just like SAGE but with the SATIN features,&quot; and\nwhen it came to contracting, the FAA decided to cut the middlemen and go\ndirectly to the hardware manufacturer: IBM.</p>\n<h2>The IBM 9020</h2>\n<p>It was 1967 by the time NAS Enroute Stage A was underway, nearly 20 years\nsince SAGE development had begun. IBM would thus benefit from considerable\nadvancements in computer technology in general. Chief among them was the\n1965 introduction of the System/360. S/360 was a milestone in the development\nof the computer: a family of solid-state, microcoded computers with a common\narchitecture for software and peripheral interconnection. S/360's chief\ndesigner, Gene Amdahl, was a genius of computer architecture who developed a\nparticular interest in parallel and multiprocessing systems. Soon after the\nS/360 project, he left IBM to start the Amdahl Corporation, briefly one of\nIBM's chief competitors. During his short 1960s tenure at IBM, though, Amdahl\ncontributed IBM's concept of the &quot;multisystem.&quot;</p>\n<p>A multisystem consisted of multiple independent computers that operated\ntogether as a single system. There is quite a bit of conceptual similarity\nbetween the multisystem and modern concepts like multiprocessing and\ndistributed computing, but remember that this was the 1960s, and engineers\nwere probing out the possibilities of computer-to-computer communication for\nthe first time. Some of the ideas of S/360 multisystems read as strikingly\nmodern and prescient of techniques used today (like atomic resource locking for\nperipherals and shared memory), while others are more clearly of their time\n(the general fact that S/360 multisystems tended to assign their CPUs\nexclusively to a specific task).</p>\n<p>One of the great animating tensions of 1960s computer history is the ever-moving\nfront between batch processing systems and realtime computing systems. IBM had\nits heritage manufacturing unit record data processing machines, in which a\nphysical stack of punched cards was the unit of work, and input and output\nultimately occurred between humans on two sides of a service window. IBM\ncomputers were designed around the same model: a &quot;job&quot; was entered into the\nmachine, stored until it reached the end of the queue, processed, and then\nthe output was stored for later retrieval. One could argue that all computers\nstill work this way, it's just process scheduling, but IBM had originally\nenvisioned job queuing times measured in hours rather than milliseconds.</p>\n<p>The batch model of computing was fighting a battle on multiple fronts: rising\npopularity of time-sharing systems meant servicing multiple terminals\nsimultaneously and, ideally, completing simple jobs interactively while the\nuser waited. Remote terminals allowed clerks to enter and retrieve data right\nwhere business transactions were taking place, and customers standing at\nticket counters expected prompt service. Perhaps most difficult of all,\nfast-moving airplanes and even faster-moving missiles required sub-second\ndecisions by computers in defense applications.</p>\n<p>IBM approached the FAA's NAS Enroute Stage A contract as one that required\na real-time system (to meet the short timelines necessary in air traffic\ncontrol) and a multisystem (to meet the FAA's exceptionally high uptime\nand performance requirements). They also intended to build the NAS automation\non an existing, commodity architecture to the greatest extent possible. The\nresult was the IBM 9020.</p>\n<p><img alt=\"IBM 9020 concept diagram\" src=\"https://computer.rip/f/9020/1.png\" /></p>\n<p>The 9020 is a fascinating system, exemplary of so many of the challenges and\nexcitement of the birth of the modern computer. On the one hand, a 9020 is a\nsophisticated, fault-tolerant, high-performance computer system with impressive\ndiagnostic capabilities and remarkably dynamic resource allocation. On the\nother hand, a 9020 is just six to seven S/360 computers married to each other\nwith a vibe that is more duct tape and bailing wire than aerospace aluminum\nand titanium.</p>\n<p>The first full-scale 9020 was installed in Jacksonville, Florida, late in 1967.\nAlong with prototype systems at the FAA's experimental center and at Raytheon\n(due to the 9020's close interaction with Raytheon-built radar systems), the\nearly 9020 computers served as development and test platforms for a complex\nand completely new software system written mostly in JOVIAL. JOVIAL isn't a\nparticularly well-remembered programming language, based on ALGOL with\nmodifications to better suit real-time computer systems. The Air Force was\ninvesting extensively in real-time computing capabilities for air defense and\nJOVIAL was, for practical purposes, an Air Force language.</p>\n<p>It's not completely clear to me why IBM selected JOVIAL for enroute stage A,\nbut we can make an informed guess. There were very few high-level programming\nlanguages that were suitable for real-time use at all in the 1960s, and JOVIAL\nhad been created by Systems Development Corporation (the original SAGE\nsoftware vendor) and widely used for both avionics and air defense. The SCC\nproject, if it had been completed, would likely have involved rewriting large\nparts of SAGE in JOVIAL. For that reason, JOVIAL had been used for some of the\nFAA's earlier ATC projects including SATIN. At the end of the day, JOVIAL was\nprobably an irritating (due to its external origin) but obvious choice for IBM.</p>\n<p>More interesting than the programming language is the architecture of the 9020.\nIt is, fortunately, well described in various papers and a special issue of\n<em>IBM Systems Journal</em>. I will simplify IBM's description of the architecture to\nbe more legible to a modern reader who hasn't worked for IBM for a decade.</p>\n<p>Picture this: seven IBM S/360 computers, of various models, are connected to a\ncommon address and memory bus used for interaction with storage. These computers\nare referred to as Compute Elements and I/O Control Elements, forming two pools\nof machines dedicated to two different sets of tasks. Also on that bus are\nsomething like 10 Storage Elements, specialized machines that function like\nmemory controllers with additional features for locking, prioritization, and\ndiagnostics. These Storage Elements provide either 131 kB or about 1 MB of\nmemory each; due to various limitations the maximum possible memory capacity\nof a 9020 is about 3.4 MB, not all of which is usable at any given time due to\nredundancy.</p>\n<p>At least three Compute Elements, and up to four, serve as the general-purpose\npart of the system where the main application software is executed. Three\nI/O Control Elements existed mostly as &quot;smart&quot; controllers for peripherals\nconnected to their channels, the IBM parlance for what we might now call an\nexpansion bus.</p>\n<p>The 9020 received input from a huge number of sources\n(radar digitizers, teletypes at airlines and flight service stations,\ncontroller workstations, other ARTCCs). Similarly, it sent output to most of\nthese endpoints as well. All of these communications channels, with perhaps the\nexception of the direct 9020-to-9020 links between ARTCCs, were very slow even\nby the standards of the time. The I/O Control Elements each used two of their\nhigh-speed channels for interconnection with display controllers (discussed\nlater) and tape drives in the ARTCC, while the third high-speed channel connected\nto a multiplexing system called the Peripheral Adapter Module that connected\nthe computer to dozens of peripherals in the ARTCC and leased telephone lines to\nradar stations, offices, and other ATC sites.</p>\n<p>Any given I/O Control Element had a full-time job of passing data between\nperipherals and storage elements, with steps to validate and preprocess data.\nIn addition to ATC-specific I/O devices, the Control Elements also used their\nPeripheral Adapter Modules to communicate with the System Console. The System\nConsole is one of the most unique properties of the 9020, and one of the\nachievements of which IBM seems most proud.</p>\n<p>Multisystem installations of S/360s were not necessarily new, but the 9020 was\none of the first attempts to present a cluster of S/360s as a single unified\nmachine. The System Console manifested that goal. It was, on first glance, not\nthat different from the operator's consoles found on each of the individual\nS/360 machines. It was much more than that, though: it was the operator's console\nfor all seven of them. During normal 9020 operation, a single operator at the\nsystem console could supervise all components of the system through alarms and\nmonitors, interact with any element of the system via a teletypewriter terminal,\nand even manually interact with the shared storage bus for troubleshooting\nand setup. The significance of the System Console's central control was such that\nthe individual S/360 machines, when operating as part of the Multisystem, disabled\ntheir local operator's consoles entirely.</p>\n<p>One of the practical purposes of the System Console was to manage partitioning of\nthe system. A typical 9020 had three compute elements and three I/O control\nelements, an especially large system could have a fourth compute element for added\ncapacity. The system was sized to produce 50% redundancy during peak traffic. In\nother words, a 9020 could run the full normal ATC workload on just two of the\ncompute elements and two of the I/O control elements. The remaining elements could\nbe left in a &quot;standby&quot; state in which the multisystem would automatically bring\nthem online if one of the in-service elements failed, and this redundancy\nmechanism was critical to meeting the FAA's reliability requirement. You could\nalso use the out-of-service elements for other workloads, though.</p>\n<p>For example, you could remove one of the S/360s from the multisystem and then\noperate it manually or run &quot;offline&quot; software. An S/360 operating this way is\ndescribed as &quot;S/360 compatibility mode&quot; in IBM documentation, since it reduces\nthe individual compute element to a normal standalone computer. IBM developed\nan extensive library of diagnostic tools that could be run on elements in standby\nmode, many of which were only slight modifications of standard S/360 tools.\nYou could also use the offline machines in more interesting ways, by bringing\nup a complete ATC software chain running on a smaller number of elements. For\ntraining new controllers, for example, one compute element and one I/O control\nelement could be removed from the multisystem and used to form a separate\npartition of the machine that operated on recorded training data. This partition\ncould have its own assigned peripherals and storage area and largely operate as\nif it were a complete second 9020.</p>\n<h2>Multisystem Architecture</h2>\n<p>You probably have some questions about how IBM achieved these multisystem\ncapabilities, given the immature state of operating systems design at the time.\nThe 9020 used an operating system derived from OS/360 MVT, an advanced form of\nOS/360 with a multitasking capability that was state-of-the-art in the mid-1960s\nbut nonetheless very limited and with many practical problems. Fortunately, IBM\nwas not exactly building a general-purpose machine, but a dedicated system with\none function. This allowed the software to be relatively simple.</p>\n<p>The core of the 9020 software system is called the control program, which is similar to\nwhat we would call a scheduler today. During routine operation of the 9020,\nany of the individual computers might begin execution of the control program at any\ntime—typically either because the computer's previous task was complete (along\nthe lines of cooperative multitasking) or because an interrupt had been received\n(along the lines of preemptive multitasking). To meet performance and timing\nrequirements, especially with the large number of peripherals involved, the 9020\nextensively used interrupts which could either be generated and handled within a\nspecific machine or sent across the entire multisystem bus.</p>\n<p>The control program's main function is to choose the next task to execute. Since it can\nbe started on any machine at any time, it must be reentrant. The fact that all of\nthe machines have shared memory simplifies the control program's task, since it has\ndirect access to all of the running programs. Shared memory also added the complexity that\nthe control program has to implement locking and conflict detection to ensure that it\ndoesn't start the same task on multiple machines at once, or start multiple tasks\nthat will require interaction with the same peripheral.</p>\n<p>You might wonder about how, exactly, the shared memory was implemented. The\nstorage elements were not complete computers, but did implement features to\nprevent conflicts between simultaneous access by two machines, for example.\nBy necessity, all of the memory management used for the multisystem is quite\nsimple. Access conflicts were resolved by choosing one machine and making the\nother wait until the next bus cycle. Each machine had a &quot;private&quot; storage area,\ncalled the preferential storage area. A register on each element contained an offset\nadded to all memory addresses that ensured the preferential storage areas did\nnot overlap. Beyond that, all memory had to be acquired by calling system\nsubroutines provided by the control program, so that the control program could\nmanage memory regions. Several different types of memory allocations were\navailable for different purposes, ranging from arbitrary blocks for internal\nuse by programs to shared buffer areas that multiple machines could use to\nqueue data for an I/O Control Element to send elsewhere.</p>\n<p>At any time during execution of normal programs, an interrupt could be generated\nindicating a problem with the system (IBM gives the examples of a detection of\nhigh temperature or loss of A/C power in one of the compute elements). Whenever\nthe control program began execution, it would potentially detect other error\nconditions using its more advanced understanding of the state of tasks. For\nexample, the control program might detect that a program has exited abnormally,\nor that allocation of memory has failed, or an I/O operation has timed out\nwithout completing. All of these situations constitute operational errors, and\nresult in the Control Program ceding execution to the Operational Error Analysis\nProgram or OEAP.</p>\n<p>The OEAP is where error-handling logic lives, but also a surprising portion of\nthe overall control of the multisystem. The OEAP begins by performing self-diagnosis.\nWhatever started the OEAP, whether the control program or a hardware interrupt,\nis expected to leave some minimal data on the nature of the failure in a register.\nThe OEAP reads that register and then follows an automated data-collection procedure\nthat could involve reading other registers on the local machine, requesting registers\nfrom other machines, and requesting memory contents from storage elements. Based\non the diagnosis, the OEAP has different options: some errors are often transient (like\ncommunications problems), so the OEAP might do nothing and simply allow the control\nprogram to start the task again.</p>\n<p>On the other hand, some errors could indicate a\nserious problem with a component of the system, like a storage element that is no\nlonger responding to read and write operations in its address range. In those\nmore critical cases, the OEAP will rewrite configuration registers on the various\nelements of the system and then reset them—and on initialization, the configuration\nregisters will cause them to assume new states in terms of membership in the\nmultisystem. In this way, the OEAP is capable of recovering from &quot;solid&quot; hardware\nfailures by simply reconfiguring the system to no longer use the failed hardware.\nMost of the time, that involves changing the failed element's configuration from &quot;online&quot;\nto &quot;offline,&quot; and choosing an element in &quot;online standby&quot; and changing its configuration\nto &quot;online.&quot; During the next execution of the control program, it will start tasks on\nthe newly &quot;online&quot; element, and the newly &quot;offline&quot; element may as well have never\nexisted.</p>\n<p>The details are, of course, a little more complex. In the case of a failed storage\nelement, for example, there's a problem of memory addresses. The 9020 multisystem\ndoesn't have virtual memory in the modern sense, addresses are more or less absolute\n(ignoring some logical addressing available for specific types of memory allocations).\nThat means that if a storage element fails, any machines which have been using\nmemory addresses within that element will need to have a set of registers for\nmemory address offsets reconfigured and then execution reset. Basically, by changing\noffsets, the OEAP can &quot;remap&quot; the memory in use by a compute or I/O control element\nto a different storage element. Redundancy is also built in to the software design\nto make these operations less critical. For example, some important parts of memory\nare stored in duplicate with an offset between the two copies large enough to ensure\nthat they will never fall on the same physical storage element.</p>\n<p>So far we have only really talked about the &quot;operational error&quot; part, though, and\nnot the &quot;analysis.&quot; In the proud tradition of IBM, the 9020 was designed from the\nground up for diagnosis. A considerable part of IBM's discussion of the architecture\nof the Control Program, for example, is devoted to its &quot;timing analysis&quot; feature.\nThat capability allows the Control Program to commit to tape a record of when each\ntask began execution, on which element, and how long it took. The output is a set\nof start and duration times, with task metadata, remarkably similar to what we\nwould now call a &quot;span&quot; in distributed tracing. Engineers used these records to\nanalyze the performance of the system and more accurately determine load limits\nsuch as the number of in-air flights that could be simultaneously tracked. Of\ncourse, details of the time analysis system remind us that computers of this\nera were very slow: the resolution on task-start timestamps was only 1/2 second,\nalthough durations were recorded at the relatively exact 1/60th of a second.</p>\n<p>That was just the control program, though, and the system's limited ability to\nwrite timing analysis data (which, even on the slow computers, tended to be\nproduced faster than the tape drives could write it and so had to fit within a\nbuffer memory area for practical purposes) meant that it was only enabled as\nneeded. The OEAP provided long-term analysis of the performance of the entire\nmachine. Whenever the OEAP was invoked, even if it determined that a problem\nwas transient or &quot;soft&quot; and took no action, it would write statistical records\nof the nature of the error and the involved elements. When the OEAP detected an\nunusually large number of soft errors from the same physical equipment, it\nwould automatically reconfigure the system to remove that equipment from service\nand then generate an alarm.</p>\n<p>Alerts generated by the OEAP were recorded by a printer connected to the System\nConsole, and indicated by lights on the System Console. A few controls on the\nSystem Console allowed the operator manual intervention when needed, for example\nto force a reconfiguration.</p>\n<p>One of the interesting aspects of the OEAP is where it runs. The 9020 multisystem\nis truly a distributed one in that there is no &quot;leader.&quot; The control program, as\nwe have discussed, simply starts on whichever machine is looking for work. In\npractice, it may sometimes run simultaneously on multiple machines, which is\nacceptable as it implements precautions to prevent stepping on its own toes.</p>\n<p>This model is a little more complex for the OEAP, because of the fact that it\ndeals specifically with failures. Consider a specific failure scenario: loss of\npower. IBM equipped each of the functional components of the 9020 with a\nbattery backup, but they only rate the battery backup for 5.5 seconds of\noperation. That isn't long enough for a generator to reliably pick up the load,\nso this isn't a UPS as we would use today. It's more of a dying gasp system:\nthe computer can &quot;know&quot; that it has lost power and continue to operate long\nenough stabilize the state of the system for faster resumption.</p>\n<p>When a compute element or I/O control element loses power, an\ninterrupt is generated within that single machine that starts the OEAP. The\nOEAP performs a series of actions, which include generating an interrupt\nacross the entire system to trigger reconfiguration (it is possible, even\nlikely given the physical installations, that the power loss is isolated to\nthe single machine) and resetting task states in the control program so that\nthe machine's tasks can be restarted elsewhere. The OEAP also informs the\nsystem console and writes out records of what has happened. Ideally, this all\ncompletes in 5.5 seconds while battery power remains reliable.</p>\n<p>In the real world, there could be problems that lead to slow OEAP execution,\nor the batteries could fail to make it for long enough, or for that matter the\ncompute element could encounter some kind of fundamentally different problem.\nThe fact that the OEAP is executing on a machine means that something has gone\nwrong, and so until the OEAP completes analysis, the machine that it is running\non should be considered suspect. The 9020 resolves this contradiction through\nteamwork: beginning of OEAP execution on any machine in the total system generates\nan interrupt that starts the OEAP on other machines in a &quot;time-down&quot; mode. The\n&quot;time-down&quot; OEAPs wait for a random time interval and then check the shared\nmemory to see if the original OEAP has marked its execution as completed. If not,\nthe first OEAP to complete its time-down timer will take over OEAP execution and\nattempt to complete diagnostics from afar. That process can, potentially, repeat\nmultiple times: in some scenario where two of the three compute elements have\nfailed, the remaining third element will eventually give up on waiting for the\nfirst two and run the OEAP itself. In theory, <em>someone</em> will eventually diagnose\nevery problem. IBM asserts that system recovery should always complete within\n30 seconds.</p>\n<p>Let's work a couple of practical examples, to edify our understanding of the\nControl Program and OEAP. Say that a program running on a Compute Element sets\nup a write operation for an I/O Control Element, which formats and sends the\ndata to a Peripheral Adapter Module which attempts to send it to an offsite\nperipheral (say an air traffic control tower teleprinter) but fails. A timer\nthat tracks the I/O operation will eventually fail, triggering the OEAP on the\nI/O control element running the task. The OEAP reads out the error register on\nits new home, discovers that it is an I/O problem related to a PAM, and then\nspeaks over the channel to request the value of state registers from the PAM.\nThese registers contain flags for various possible states of peripheral connections,\nand from these the OEAP can determine that sending a message has failed because\nthere was no response. These types of errors are often transient, due to\ntelephone network trouble or bad luck, so the OEAP increments counters for\nfuture reference, looks up the application task that tried to send the message\nand changes its state to incomplete, clears registers on the PAM and I/O\ncontrol element, and then hands execution back to the Control Program. The\nControl Program will most likely attempt to do the exact same thing over\nagain, but in the case of a transient error, it'll probably work this time.</p>\n<p>Consider a more severe case, where the Control Program starts a task on a\nCompute Element that simply never finishes. A timer runs down to detect\nthis condition, and an interrupt at the end of the timer starts the Control\nProgram, which checks the state and discovers the still-unfinished task.\nThrowing its hands in the air, the Control Program sets some flags in the\nerror register and hands execution to the OEAP. The OEAP starts on the same\nmachine, but also interrupts other machines to start the OEAP in time-down\nmode in case the machine is too broken to complete error handling. It then\nreads the error register and examines other registers and storage contents.\nDetermining that some indeterminate problem has occurred with the Compute\nElement, the OEAP triggers what IBM confusingly calls a &quot;logout&quot; but we\nmight today call a &quot;core dump&quot; (ironically an old term that was more\nappropriate in this era). The &quot;logout&quot; entails copying the contents of\nall of the registers and counters to the preferential storage area and then\ndirecting, via channel, one of the tape drives to write it all to a tape kept\nready for this purpose—the syslog of its day. Once that's complete, the\nOEAP will reset the Compute Element and hand back to the Control Program\nto try again... unless counters indicate that this same thing has happened\nrecently. In that case, the OEAP will update the configuration register on\nthe running machine to change its status to offline, and choose a machine\nin online-standby. It will write to that machine's register, changing its\nstatus to online. A final interrupt causes the Control Program to start on\nboth machines, taking them into their new states.</p>\n<p>Lengthy FAA procedure manuals described what would happen next. These are\nunfortunately difficult to obtain, but from IBM documentation we know that\nbasic information on errors was printed for the system operator. The system\noperator would likely then use the system console to place the suspicious\nelement in &quot;test&quot; mode, which completely isolates it to behave more or less\nlike a normal S/360. At that point, the operator could use one of the tape\ndrives attached to the problem machine to load IBM's diagnostic library and\nperform offline troubleshooting. The way the tape drives are hooked up to\nspecific machines is important; in fact, since the OEAP is fairly large, it\nis only stored in one copy on one Storage Element. The 9020 requires that one\nof the tape drives always have a &quot;system tape&quot; ready with the OEAP itself,\nand low-level logic in the elements allows the OEAP to be read from the\nready-to-go system tape in case the storage element that contains it fails to\nrespond.</p>\n<p>A final interesting note about the OEAP is a clever optimization called &quot;problem\nprogram mode.&quot; During analysis and handling of an error, the OEAP can decide\nthat the critical phase of error handling has ended and the situation is no\nlonger time sensitive. For example, the OEAP might decide that no action is\nrequired except for updating statistics, which can comfortably happen with a\nslight delay. These lower-priority remaining tasks can be added to memory as &quot;normal&quot;\napplication tasks, to be run by the Control Program like any other task after\nerror handling is complete. Think of it as a deferral mechanism, to avoid the\nOEAP locking up a machine for any longer than necessary.</p>\n<p>For the sake of clarity, I'll note again an interesting fact by quoting IBM\ndirectly: &quot;OEAP has sole responsibility for maintaining the system\nconfiguration.&quot; The configuration model of the 9020 system is a little\nunintuitive to me. Each machine has its own configuration register that\ntells it what its task is and whether it is online or offline (or one of\nseveral states in between like online-standby). The OEAP reconfigures the\nsystem by running on any one machine and writing the configuration registers\nof both the machine it's running on, and all of the other machines via the\nshared bus. Most reconfigurations happen because the OEAP has detected a\nproblem and is working around it, but if the operator manually reconfigures\nthe system (for example to facilitate testing or training), they also do so\nby triggering an interrupt that leads the Control Program to start the OEAP.\nThe System Console has buttons for this, along with toggles to set up a sort\nof &quot;main configuration register&quot; that determines how the OEAP will try to\nset up the system.</p>\n<h2>The Air Traffic Control Application</h2>\n<p>This has become a fairly long article by my norms, and I haven't even really\ntalked about air traffic control that much. Well, here it comes: the application\nthat actually ran on the 9020, which seems to have had no particular name,\nbesides perhaps Central Computing Complex (although this seems to have been\nadopted mostly to differentiate it from the Display Complex, discussed soon).</p>\n<p>First, let's talk about the hardware landscape of the ARTCC and the 9020's role.\nAn ARTCC handles a number of sectors, say around 30. Under the 9020 system,\neach of these sectors has three controllers associated with it, called the R, D,\nand A controllers. The R controller is responsible for monitoring and interpreting\nthe radar, the D controller for managing flight plans and flight strips, and the\nA controller is something of a generalist who assists the other two. The three\npeople sit at something like a long desk, made up of the R, D, and A consoles\nside by side.</p>\n<p><img alt=\"Sector control consoles\" src=\"https://computer.rip/f/9020/2.png\" /></p>\n<p>The R console is the most recognizable to modern eyes, as its centerpiece is a\n22&quot; CRT plan-view radar display. The plan-view display (PVD) of the 9020 system\nis <em>significantly</em> more sophisticated than the SAGE PVD on which it is modeled.\nMost notably, the 9020 PVD is capable of displaying text and icons. No longer\ndoes a controller use a light gun to select a target for a teleprinter to\nidentify; the &quot;data blocks&quot; giving basic information on a flight were actually\nshown on the PVD next to the radar contact. A trackball and a set of buttons\neven allowed the controller to select targets to query for more information or\nupdate flight data. This was quite a feat of technology\neven in 1970, and in fact one that the 9020 was not capable of. Well, it was\nactually capable of it, but not initially.</p>\n<p>The original NAS stage A architecture separated the air traffic control data\nfunction and radar display function into two completely separate systems. The\nformer was contracted to IBM, the latter to Raytheon, due to their significant\nexperience building similar systems for the military. Early IBM 9020\ninstallations sat alongside a Raytheon 730 Display Channel, a very specialized\nsystem that was nearly as large as the 9020. The Display Channel's role was to\nreceive radar contact data and flight information in digital form from the 9020,\nand convert it into drawing instructions sent over a high-speed serial connection\nto each individual PVD. A single Display Channel was responsible for up to 60\nPVDs. Further complicating things, sector workstations were reconfigurable to\nhandle changing workloads. The same sector might be displayed on multiple PVDs,\nand where sectors met, PVDs often overlapped so the same contact would be\nvisible to controllers for both sectors. The Display Channel had a fairly\ncomplex task to get the right radar contacts and data blocks to the right\ndisplays, and in the right places.</p>\n<p>Later on, the FAA opted to contract IBM to build a slightly more sophisticated\nversion of the Display Channel that supported additional PVDs and provided\nbetter uptime. To meet that contract, IBM used another 9020. Some ARTCCs thus\nhad <em>two</em> complete 9020 systems, called the Central Computer Complex (CCC) and the\nDisplay Channel Complex (DCC).</p>\n<p>The PVD is the most conspicuous part of the controller console, but there's a\nlot of other equipment there, and the rest of it is directly connected to the\n9020 (CCC). At the R controller's position, a set of &quot;hotkeys&quot; allow for\nquickly entering flight data (like new altitudes) and a computer readout device (CRD),\na CRT that displays 25x20 text for general output. For example, when a controller\nselects a target on the PVD to query for details, that query is sent to the 9020\nCCC which shows the result on the R controller's CRD above the PVD.</p>\n<p>At the D controller's position, right next door, a large rack of slots for\nflight strips (small paper strips used to logically organize flight clearances,\nstill in use today in some contexts) surrounds the D controller's CRD. The\nD controller also has a Computer Entry Device, or CED, a specialized keyboard\nthat allows the D controller to retrieve and update flight plans and clearances\nbased on requests from pilots or changes in the airspace situation. To their\nright, a modified teleprinter is dedicated to producing the flight strips\nthat they arrange in front of them. Flight strips are automatically printed when\nan aircraft enters the sector, or when the controller enters changes.\nThe A controller's position to the right of the flight strip printer is largely the same as the D controller's position,\nwith another CRD and CED that operate independently from the D controller's—valuable\nduring peak traffic.</p>\n<p>While controller consoles are the most visible peripherals of the system, they're\nfar from the only ones. Each 9020 system had an extensive set of teletypewriter\ncircuits. Some of these were local; for example, the ATC supervisor had a dedicated\nTTY where they could not only interact with flight data (to assist a sector controller\nfor example) but also interact with the status of the NAS automation itself\n(for example to query the status of a malfunctioning radar site and then remove\nit from use for PVDs).</p>\n<p>Since the 9020 was also the locus of flight planning,\nTTYs were provided in air traffic control towers, terminal radar facilities,\nand even the dispatch offices of airlines. These allowed flight plans to be\nentered into the 9020 before the aircraft was handed off to enroute control.\nFlight service stations functioned more or less as the dispatch offices for\ngeneral aviation, so they were similarly equipped with TTYs for flight plan management.\nIn many areas, military controllers at air defense sectors were also provided\nwith TTYs for convenient access to flight plans. Not least of all, each 9020\nhad high-speed leased lines to its neighboring 9020s. Flights passing from one\nARTCC to the next had their flight strip &quot;digitally passed&quot; by transmission\nfrom one 9020 to the next.</p>\n<p>A set of high-speed line printers connected to the 9020 printed diagnostic\ndata as well as summary and audit reports on air traffic. Similar audit data,\nincluding a detailed record of clearances, was written to tape drives for\nfuture reference.</p>\n<p>To organize the whole operation, IBM divided the software architecture of the\nsystem into the &quot;supervisor state&quot; and the &quot;problem state.&quot; These are reasonably\nanalogous to kernel and user space today, and &quot;problem&quot; is meant as in &quot;the\nproblem the computer solves&quot; rather than &quot;a problem has occurred.&quot; The Control\nProgram and OEAP run in the supervisor state, everything else runs after the\nControl Program has set up a machine in the Problem State and started a given\nprogram.</p>\n<p>IBM organized the application software into five modules, which they called\nthe five Programs. These are Input Processing, Flight Processing,\nRadar Processing, Output Processing, and Liaison Management. Most of these are\nfairly self-explanatory, but the list reveals the remarkably asynchronous\ndesign of the system. Consider an example, we'll say a general aviation flight\ntaking off from an airport inside of one of the ARTCC's sectors.</p>\n<p>The pilot first contacts a Flight Service Station, which uses their TTY to\nenter a flight plan into the 9020. Next, the pilot interacts with the control\ntower, which in the process of giving a takeoff clearance uses their TTY to\ninform the 9020 that the flight plan is active. They may also update the\nflight plan with the aircraft's planned movements shortly after takeoff, if\nthey have changed due to operating conditions. The Input Processing program\nhandles all of these TTY inputs, parsing them into records stored on a Storage\nElement. In case any errors occur, like an invalid entry, those are also\nwritten to the Storage Element, where the Output Processing program picks them\nup and sends an appropriate message to the originating TTY. IBM notes that\nthere were, as originally designed, about 100 types of input messages parsed\nby the input processing program.</p>\n<p>As the aircraft takes off, it is detected by a radar site (such as a Permanent\nSystem radar or Air Route Surveillance Radar) which digitally encodes the radar\ncontact (a Raytheon system) for transmission to the 9020. The Radar Processing\nprogram receives these messages, converts radial radar coordinates to the XY\nplane used by the system, correlates contacts with similar XY positions from\nmultiple radar sites into a single logical contact, and computes each contact's\napparent heading and speed to extrapolate future positions. Complicating things,\nthe 9020 went into service during the development of secondary surveillance\nradar, also known as the transponder system <sup class=\"footnote-ref\" id=\"fnref-2\"><a href=\"#fn-2\">2</a></sup>. On appropriately equipped aircraft,\nthe transponder provides altitude. The Radar Processing system makes an\naltitude determination on each aircraft, a slightly more complicated task than\nyou might expect as, at the time, only some radar systems and some transponders\nprovided altitude information. The Radar Processing program thus had to track\nif it had altitude information at all and, if so, where from. In the mean time,\nthe Radar Processing program tracked the state of the radar sites and reported\nany apparent trouble (such as loss of data or abnormal data) to the supervisor.</p>\n<div class=\"beg\">\n<hr />\n<p>I put a lot of time into writing this, and I hope that you enjoy reading\nit. If you can spare a few dollars, consider <a href=\"https://ko-fi.com/jbcrawford\">supporting me on\nko-fi</a>. You'll receive an occasional extra,\nsubscribers-only post, and defray the costs of providing artisanal, hand-built\nworld wide web directly from Albuquerque, New Mexico.</p>\n<hr />\n</div>\n<p>The Flight Processing program periodically evaluates all targets from the Radar\nProcessing program against all filed flight plans, correlating radar targets\nwith filed flight plans, calculating navigational deviations, and predicting\nfuture paths. Among other outputs, the Flight Processing program generated\nup-to-date flight strips for each aircraft and predicted their arrival times\nat each flight plan fix for controller's planning purposes. The Flight\nProcessing program hosted a set of rules used for safety protections, such as\nseparation distances. This capability was fairly minimal during the 9020's\noriginal development, but was enhanced over time.</p>\n<p>The Output Processing program had two key roles. First, it handled data that\nwas specifically queued for it because of a reactive need to send data to a\ngiven output. For example, if someone made a data entry error or a controller\nqueried for a specific aircraft's flight plan, the Input Processing program\nplaced the resulting data in memory, where the Output Processing program would &quot;find it&quot;\nto format and send to the correct device. The Output Processing program also\ncontinuously prepared common outputs like flight data blocks and radar\nstation status messages that were formatted once to a common memory buffer\nto be sent to many devices in bulk. For example, a new flight strip for an\naircraft would be formatted and stored once, and then sent in sequence to\nevery controller position with a relation to that aircraft.</p>\n<h2>Legacy</h2>\n<p>The 9020 is just one corner of the evolution of air traffic control during the\n1960s and 1970s, a period that also saw the introduction of secondary radar\nfor civilian flights and the first effort to automate the role of flight service\nstations. These topics quickly spiral out into others: unlike the ARTCCs of the\ntime, the flight service stations dealt extensively with weather and interacted\nwith both FAA and National Weather Service teletype networks and computer systems.\nAn early effort to automate the flight service function involved the use of a\n<a href=\"https://computer.rip/2025-08-25-teletext-in-north-america.html\">teletext system</a> originally developed for agricultural use as a &quot;flight\nbriefing terminal.&quot; That wasn't the agricultural teletext system in Kentucky\nthat I discussed, but a different one, in Kansas. Fascinating things everywhere\nyou look!</p>\n<p>This article has already become long, though, and so we'll have to save plenty\nfor later. To round things out, let's consider the fate of the 9020. SAGE is\nknown not only for its pioneering role in the computing art, but because of its\nremarkably long service life, roughly from 1958 to 1984. The 9020 was almost\n20 years younger than SAGE, and indeed outlived it, but not by much. In 1982, IBM announced\nthe IBM 3083, a newer implementation of the Enhanced S/370 architecture that\nwas directly descended from S/360 but with greatly improved I/O capabilities.\nIn 1986, the FAA accepted a new 3083-based system called &quot;HOST.&quot; Over the\nfollowing three years, all of the 9020 CCCs were replaced by HOST systems.</p>\n<p>The 9020 was not to be forgotten so easily, though. First, the HOST project\nwas mostly limited to hardware modernization or &quot;rehosting.&quot; The HOST 3083\ncomputers ran most of the same application code as the original 9020 system,\nincorporating many enhancements made over the intervening decades.</p>\n<p>Second, there is the case of the Display Channel Complex. Once again, because\nof the complexity of the PVD subsystem the FAA opted to view it as a separate\nprogram. While an effort was started to replace the 9020 DCCs alongside the\n9020 CCCs, it encountered considerable delays and was ultimately canceled.\nThe 9020 DCCs remained in service controlling PVDs until the ERAM Stage A\nproject replaced the PVD system entirely in the 1990s.</p>\n<p>While IBM's efforts to market the 9020 overseas generally failed, a 9020 CCC\nsystem (complete with simplex test machine) was sold to the UK Civil Aviation Authority for use in the London Air\nTraffic Centre. This 9020 remained in service until 1990, and perhaps because\nof its singularity and unusually long life, it is better remembered as a\nhistoric object. <a href=\"https://atchistory.wordpress.com/2023/05/24/latcc-engineering-sado-9020-and-many-other-activities-still-under-construction/\">There are photos</a>.</p>\n<p><img alt=\"Figure from study of typical 9020 message volume\" src=\"https://computer.rip/f/9020/3.png\" /></p>\n<div class=\"footnotes\">\n<ol>\n<li id=\"fn-1\">\n<p>The term National Airspace System (NAS) is still in use today, but is now\nmore of a concept than a physical thing. The NAS is the totality of the\nregulations, procedures, and communications systems used in air traffic control.\nDuring the NAS Enroute Stage A project, IBM and the FAA both seem to have used\n&quot;NAS&quot; to describe the ARTCC computer system as a physical object, although I think\nit was debatable even then whether or not this was an appropriate use of the\nterm. One of the difficulties in researching the history of civilian air traffic\ncontrol is that the FAA seems to have been particularly bad about names. &quot;NAS\nEnroute Stage A&quot; is not very wieldy but is one of the only terms that unambiguously\nrefers to the late-'60s, early-'70s IBM 9020-based ARTCC system, and even then\nit is confusing with the later enroute automation modernization (ERAM) program,\ncomplete with its own stage A. I refer to the ARTCC automation system simply as\n&quot;the IBM 9020&quot; even though this is incorrect (consider for example that the\ncomplete system often involved a display subsystem built by Raytheon), and you\nwill find contemporary references to it as &quot;NAS,&quot; &quot;NAS stage A,&quot; &quot;NAS automation,&quot;\netc.<a class=\"footnote\" href=\"#fnref-1\">&#8617;</a></p></li>\n<li id=\"fn-2\">\n<p>One of the responsibilities of the 9020 was the assignment of non-overlapping\ntransponder codes as well.<a class=\"footnote\" href=\"#fnref-2\">&#8617;</a></p></li>\n</ol>\n</div>",
  "id": "https://computer.rip/2026-01-17-air-traffic-control-9020.html"
}