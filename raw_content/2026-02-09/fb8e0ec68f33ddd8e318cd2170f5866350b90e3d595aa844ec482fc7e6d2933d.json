{
  "title": "Fed 24 Years of My Blog Posts to a Markov Model",
  "link": "https://susam.net/fed-24-years-of-posts-to-markov-model.html",
  "published": "Sat, 13 Dec 2025 00:00:00 +0000",
  "summary": "<p>\n  Yesterday I shared a little program called <em>Mark V. Shaney\n  Junior</em> at\n  <a href=\"https://github.com/susam/mvs\">github.com/susam/mvs</a>.  It\n  is a minimal implementation of a Markov text generator inspired by\n  the legendary Mark V. Shaney program from the 1980s.  Mark V. Shaney\n  was a synthetic Usenet user that posted messages to various\n  newsgroups using text generated by a Markov model.  See the\n  Wikipedia article\n  <a href=\"https://en.wikipedia.org/wiki/Mark_V._Shaney\">Mark\n  V. Shaney</a> for more details about it.  In this post, I will\n  discuss my implementation of the model, explain how it works and\n  share some of the results produced by it.\n</p>\n<h2 id=\"contents\">Contents<a href=\"#contents\"></a></h2>\n<ul>\n  <li><a href=\"#recreational-programming\">Recreational Programming</a></li>\n  <li><a href=\"#gibberish\">Gibberish</a></li>\n  <li><a href=\"#markov-property\">The Markov Property</a></li>\n  <li><a href=\"#some-more-gibberish\">Some More Gibberish</a></li>\n</ul>\n<h2 id=\"recreational-programming\">Recreational Programming<a href=\"#recreational-programming\"></a></h2>\n<p>\n  The program I shared yesterday has only about\n  <a href=\"https://github.com/susam/mvs/blob/main/mvs\">30 lines of\n  Python</a> and favours simplicity over efficiency.  Even if you have\n  never worked with Markov models before, as long as you know some\n  Python programming, I am quite confident that it will take you less\n  than 20 minutes to understand the whole program and make complete\n  sense of it.  I also offer an explanation\n  <a href=\"#the-markov-property\">further below</a> in this post.\n</p>\n<p>\n  As a hobby, I often engage in exploratory programming where I write\n  computer programs not to solve a specific problem but simply to\n  explore a particular idea or topic for the sole purpose of\n  recreation.  I must have written small programs to explore Markov\n  chains for various kinds of state spaces over a dozen times by now.\n  Every time, I just pick my last experimental code and edit it to\n  encode the new state space I am exploring.  That's usually my\n  general approach to such one-off programs.  I have hundreds of tiny\n  little experimental programs lying on my disk at any given time.\n</p>\n<p>\n  Once in a while, I get the itch to take one of those exploratory\n  programs, give it some finishing touches, wrap it up in a nice Git\n  repo along with a <code>README.md</code>, <code>CHANGES.md</code>\n  and the whole shebang and share it on\n  <a href=\"https://github.com/susam\">github.com/susam</a> and\n  <a href=\"https://codeberg.org/susam\">codeberg.org/susam</a>.  The\n  Mark V. Shaney Junior program that I shared yesterday happened to be\n  one such exercise.\n</p>\n<h2 id=\"gibberish\">Gibberish<a href=\"#gibberish\"></a></h2>\n<p>\n  If you scroll down the\n  <a href=\"https://github.com/susam/mvs#readme\">README</a> of this\n  project, you'll find some nice examples of the gibberish produced by\n  this program.  The first few examples there are the result of\n  training the model on <em>A Christmas Carol</em> by Charles Dickens,\n  one of my favourite authors.  It is often said that Dickens never\n  used fewer words when more would suffice.  So I thought there\n  couldn't be a better piece of text when it comes to testing out my\n  tiny Markov model.  I'll not reproduce the generated text examples\n  here for the sake of brevity.  If you are interested to take a look,\n  just head over to the\n  <a href=\"https://github.com/susam/mvs#gibberish\">Gibberish</a>\n  section of the README.\n</p>\n<p>\n  Soon after sharing the project, I wondered what kind of gibberish it\n  would produce if I fed all 24 years of my blog\n  <a href=\"pages.html\">posts and pages</a> into the program.\n  Well, here's one of the results:\n</p>\n<pre><samp>$ <kbd>./mvs &lt; susam.txt</kbd>\nwhile a query replace operation is approved by the user. The above\nvariable defines the build job. It can be incredibly useful while\nworking on assembly language and machine code. In fact, all internal\nresources like the result to refine the search prompt changes from\nbck-i-search: to fwd-i-search:. Now type C-SPC (i.e. ctrl+space) to\nset a mark causes Emacs to use 32-bit registers like EBP, ESP,\netc. Thus the behaviour is undefined. Such code may behave differently\nwhen compiled with the readily available GNU tools like the shape\nof 8. Flipping \"P\" horizontally makes it a proper quine: cat $0</samp></pre>\n<p>\n  This is the text that comes out after the program consumes over 200\n  posts consisting of about 200,000 words.  My blog also has\n  a <a href=\"comments/\">comments section</a> with over 500\n  comments consisting of about 40,000 words.  All comments were\n  excluded while training the model.  Here is another output example:\n</p>\n<pre><samp>$ <kbd>./mvs &lt; susam.txt</kbd>\nenjoy asking \"what happens if\" and then type M-x zap-up-to-char RET\nb. The buffer for this specific video, the actual fare for 8.3 km and\n11 are all written from scratch. No prior knowledge is expected to\nslow down in future. For now, I will add a statement like x =\n0.999..., the inner corner square as discussed in the code segment\ninto the REPL window. Unlike Slimv, Vlime can work with and debug\nexecutable files, it can be more convenient. M-x: Execute Extended\nCommand The key sequence M-q invokes the command cat and type TAB to\nindent the current</samp></pre>\n<p>\n  Here is a particularly incoherent but amusing one:\n</p>\n<pre><samp>$ <kbd>./mvs &lt; susam.txt</kbd>\nThen open a new Lisp source file and the exact answer could harm\nstudents' self-esteem. Scientists have arbitrarily assumed that an\nintegral domain. However, the string and comment text. To demonstrate\nhow a build job can trigger itself, pass input to standard output or\nstandard error), Eshell automatically runs the following command in\nVim and Emacs will copy the message length limit of 512 characters,\netc. For example, while learning to play the game between normal mode\nto move the point is on an old dictionary lying around our house and\nthat is moving to the small and supportive community</samp></pre>\n<p>\n  No, I have never said anywhere that opening a Lisp source file could\n  harm anyone's self-esteem.  The text generator has picked up the\n  'Lisp source file' phrase from my\n  <a href=\"lisp-in-vim.html\">Lisp in Vim</a> post and the\n  'self-esteem' bit from the <a href=\"from-perl-to-pi.html\">From Perl\n  to Pi</a> post.\n</p>\n<h2 id=\"markov-property\">The Markov Property<a href=\"#markov-property\"></a></h2>\n<p>\n  By default, this program looks at trigrams (all sequences of three\n  adjacent words) and creates a map where the first two words of the\n  trigram are inserted as the key and the third word is appended to\n  its list value.  This map is the model.  In this way, the model\n  captures each pair of adjacent words along with the words that\n  immediately follow each pair.  The text generator first chooses a\n  key (a pair of words) at random and selects a word that follows.  If\n  there are multiple followers, it picks one uniformly at random.  It\n  then repeats this process with the most recent pair of words,\n  consisting of one word from the previous pair and the word that was\n  just picked.  It continues to do this until it can no longer find a\n  follower or a fixed word limit (100 by default) is reached.  That is\n  pretty much the whole algorithm.  There isn't much more to it.  It\n  is as simple as it gets.  For that reason, I often describe a simple\n  Markov model like this as the 'hello, world' for language models.\n</p>\n<p>\n  If the same trigram occurs multiple times in the training data, the\n  model records the follower word (the third word) multiple times in\n  the list associated with the key (the first two words).  This\n  representation can be optimised, of course, by keeping frequencies\n  of the follower words rather than duplicating them in the list, but\n  that is left as an exercise to the reader.  In any case, when the\n  text generator chooses a follower for a given pair of words, a\n  follower that occurs more frequently after that pair has a higher\n  probability of being chosen.  In effect, the next word is sampled\n  based only on the previous two words and not on the full history of\n  the generated text.  This memoryless dependence on the current state\n  is what makes the generator Markov.  Formally, for a discrete-time\n  stochastic process, the Markov property can be expressed as\n\n  \\[\n    P(X_{n+1} \\mid X_n, X_{n-1}, \\ldots, X_1) = P(X_{n+1} \\mid X_n).\n  \\]\n\n  where \\( X_n \\) represents the \\( n \\)th state.  In our case, each\n  state \\( X_n \\) is a pair of words \\( (w_{n-1}, w_{n}) \\) but the\n  state space could just as well consist of other objects, such as a\n  pair of characters, pixel values or musical notes.  The sequence of\n  states \\( (X_1, X_2, \\dots) \\) visited by the program forms a Markov\n  chain.  The left-hand side of the equation denotes the conditional\n  distribution of the next state \\( X_{n+1} \\) given the entire\n  history of states \\( X_1, X_2, \\dots, X_n, \\) while the right-hand\n  side conditions only on the current state \\( X_n.  \\)  When both are\n  equal, it means that the probability of the next state depends only\n  on the current state and not on the earlier states.  This is the\n  Markov property.  It applies to the text generation process only,\n  not the training data.  The training data is used only to estimate\n  the transition probabilities of the model.\n</p>\n<h2 id=\"some-more-gibberish\">Some More Gibberish<a href=\"#some-more-gibberish\"></a></h2>\n<p>\n  In 2025, given the overwhelming popularity of large language models\n  (LLMs), Markov models like this look unimpressive.  Unlike LLMs, a\n  simple Markov model cannot capture global structure or long-range\n  dependencies within the text.  It relies entirely on local word\n  transition statistics.  Also, these days, one hardly needs a Markov\n  model to generate gibberish; social media provides an ample supply.\n  Nevertheless, I think the simplicity of its design and\n  implementation serves as a good entry point into language models.\n</p>\n<p>\n  In my implementation, the number of words in the key of the map can\n  be set via command line arguments.  By default, it is 2 as described\n  above.  This value is also known as the order of the model.  So by\n  default the order is 2.  If we increase it to, say, 3 or 4, the\n  generated text becomes a little more coherent.  Here is one such\n  example:\n</p>\n<pre><samp>$ <kbd>./mvs 4 &lt; susam.txt</kbd>\nIt is also possible to search for channels by channel names. For\nexample, on Libera Chat, to search for all channels with 'python' in\nits name, enter the IRC command: /msg alis list python. Although I\nhave used Libera Chat in the examples above, there are plenty of\ninfinite fields, so they must all be integral domains too. Consider\nthe field of rational numbers Q. Another quick way to arrive at this\nfact is to observe that when one knight is placed on a type D square,\nonly two positions for the second knight such that the two knights\nattack</samp></pre>\n<p>\n  Except for a couple of abrupt and meaningless transitions, the text\n  is mostly coherent.  We need to be careful about not increasing the\n  order too much.  In fact, if we increase the order of the model to\n  5, the generated text becomes very dry and factual because it begins\n  to quote large portions of the blog posts verbatim.  Not much fun\n  can be had with that.\n</p>\n<p>\n  Before I end this post, let me present one final example where I ask\n  it to generate text from an initial prompt:\n</p>\n<pre><samp>$ <kbd>./mvs 2 100 'Finally we' &lt; susam.txt</kbd>\nFinally we divide this number by a feed aggregrator for Emacs-related\nblogs. The following complete key sequences describe the effects of\nprevious evaluations shall have taken a simple and small to contain\nbad content. This provides an interactive byte-compiled Lisp function\nin MATLAB and GNU bash 5.1.4 on Debian is easily reproducible in\nWindows XP. Older versions might be able to run that server for me it\nplayed a significant burden on me as soon as possible. C-u F: Visit\nthe marked files or directories in the sense that it was already\ninitiated and we were to complete the proof.</samp></pre>\n<p>\n  Apparently, this is how I would sound if I ever took up speaking\n  gibberish!\n</p>\n<!-- ### -->\n<p>\n  <a href=\"https://susam.net/fed-24-years-of-posts-to-markov-model.html\">Read on website</a> |\n  <a href=\"https://susam.net/tag/python.html\">#python</a> |\n  <a href=\"https://susam.net/tag/programming.html\">#programming</a> |\n  <a href=\"https://susam.net/tag/technology.html\">#technology</a>\n</p>",
  "id": "mgtfy"
}