{
  "title": "Writing an LLM from scratch, part 32a -- Interventions: training a baseline model",
  "link": "https://www.gilesthomas.com/2026/02/llm-from-scratch-32a-interventions-baseline-model",
  "published": "Wed, 04 Feb 2026 01:45:00 +0000",
  "summary": "<p>I'm rounding out my series of posts on\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\"\nby seeing how I could train the <em>best</em> base model I can from scratch on my own hardware.\nI started by <a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">training one in two days on my RTX 3090</a>,\nand found that while it was a decent little model, it wasn't as good as the original\nGPT-2 small, either in terms of the loss it got on my test dataset, or\nin terms of how good it was at following instruction prompts after fine-tuning on them.  I decided\nthat I wanted to see what levers I could pull -- dropout, attention weight biases, and so\non -- to make it better.</p>\n\n<p>For that, I didn't want to have my PC tied up for days at a time with multiple long training runs,\nso I <a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud\">learned how to train faster in the cloud</a>.\nThat led to some <a href=\"/2026/01/llm-from-scratch-30-digging-into-llm-as-a-judge\">refinements in the prompt-following test I was using</a>,\nand I also spent a bit of time on a side quest getting the various models I'd trained <a href=\"/2026/01/llm-from-scratch-31-models-on-hugging-face\">onto Hugging Face Hub</a>.</p>\n\n<p>Now it's time to try the various \"interventions\", as I'll call them -- the levers to\npull to see if I can make the model better.  This post is\nto recap what they are, and to describe what I did to establish a baseline model to\ncompare to.</p>\n<h3 id=\"the-interventions\">The interventions</h3>\n\n<p>I listed a number of possible interventions at the end of the RTX 3090 post; I'm not going\nto do them all, but for completeness, here's the full list:</p>\n\n<ul>\n<li>The amount of training data.  I'm not going to dig into this one; it looks like it\ndoes help, but the returns diminish rapidly, so I think that in order to get any serious\nimprovement we'd need to train for much more than two days locally.  In the one\n\"extended training\" test I did, I managed to get the loss down from 4.167 to 4.135, which\nwas... less-than-inspiring.</li>\n<li>The number of epochs.  I'm going to stick to single-epoch training -- that is, I'll\ntrain on a single pass through an amount of non-repeating data chosen to take 48 hours to handle\non my local machine.</li>\n<li>The bias on the <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math> and <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math> matrices.  This one definitely sounds worth looking\ninto -- easy, as it's just a change to a config flag, and makes the model more like the\noriginal GPT-2.  I'll give that a go.  <a href=\"/2026/02/llm-from-scratch-32d-interventions-adding-attention-bias\">Here's the post</a></li>\n<li>Dropout.  I've read that for single-epoch training, dropout doesn't help (which\ndoesn't quite work with <a href=\"/2025/03/dropout-and-mandatory-vacation\">my mental model</a> of what it's for, but\ndoes sound plausible).  Worth a look!  <a href=\"/2026/02/llm-from-scratch-32c-interventions-removing-dropout\">Here's the post</a>.</li>\n<li>The learning rate, and weight decay.  The values I've used for these are basically copypasta\nfrom the book.  I think I should learn to understand these and try to optimise them a bit.</li>\n<li>The precision.  I'm using <a href=\"https://docs.pytorch.org/docs/stable/amp.html\">AMP</a>, which means that\nsome calculations are done in 16-bit rather than 32-bit, and calling\n<a href=\"https://docs.pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\"><code>set_float32_matmul_precision</code></a>\nwith <code>\"high\"</code> to let PyTorch choose to use the GPU's tensor cores, which use TF32, a kind of \"32-bit float lite\" (see the\n<a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">post on the local train</a> for details).\nThose both (at least potentially) reduce the precision of\nthe train below what you'd get if you trained with full-fat <code>float32</code>.  Would reverting\nthat be worth the longer train time?  I should probably at least poke at that.</li>\n<li>The batch size.  I've already, in effect, tried playing with that.  The different\ncloud machines I played with had different amounts of per-GPU VRAM, so supported\ndifferent per-GPU micro-batch sizes.  So I wound up trying batch sizes from\n512 (the same as the original GPT-2 was trained with) down to 104 in the cloud,\nplus my local trains with a batch size of 6.  I did a rough-and-ready calculation\n<a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud#the-results\">at the end of the cloud training post</a>\nwhere I estimated that the ideal batch size might be something like 97.  So, probably\nnot worth much more investigation.</li>\n<li>Exploding gradients.  In one of my local trains, and in three out of the four cloud\ntrains, I had sudden spikes in both training and validation loss.  It generally took\nquite a bit of training -- maybe 10-15% of training time -- to get back on track after\nsome of these, so we had what could be seen as wasted time in the training runs.\nExploding gradients can be fixed by gradient clipping, which is relatively easy to\ndo.  Definitely worth investigating!  <a href=\"/2026/02/llm-from-scratch-32b-interventions-gradient-clipping\">Here's the post on that</a>.</li>\n</ul>\n\n<p>I'm going to work through each of those apart from the first two and the batch size (and will retrospectively add links to\nthe list above when I do), trying a train with just that intervention and nothing else, on\na cloud machine.  Once that's done, I'll bake all of the things that helped into\nthe training loop, and do another local train -- with\n<a href=\"https://www.hopsworks.ai/dictionary/gradient-accumulation\">gradient accumulation</a> to make\nthe batch size match the cloud instances'.</p>\n\n<p>The cloud machine size that I decided to use for this was the one that came out the\nmost cost-effective (and due to its VRAM size, had the best loss) in my earlier\ncloud training test: an 8x A100 machine with 40 GiB VRAM per GPU.</p>\n\n<p>But first, we need a baseline model.</p>\n\n<h3 id=\"why-a-new-baseline\">Why a new baseline?</h3>\n\n<p>I've already done a train on an 8x A100 40 GiB machine -- why do we need a new one?</p>\n\n<p>In my cloud training post, I came to the conclusion that the cost in terms of training\ntime of running a periodic validation loop as we trained was not really worth it, at\nleast in this case.  Two of the biggest reasons to have validation during training\nare to work out when you're overfitting on a multi-epoch train, and to see how your model\ncan handle datasets that it has not been trained on.</p>\n\n<p>In a single-epoch train like this,\nyou're not going to overfit -- every sample it sees will be new to it -- and the training\nloss itself is over samples it's not been trained on at the time it was calculated, for the\nsame reason (though of course it will be trained on them as soon as we do the backward\npass starting with that loss).</p>\n\n<p>Of course, it's not perfect --\na big benefit of the validation loss is that it's over the <em>same</em> held-back dataset\non every run -- and there are arguments for keeping it (albeit, perhaps doing full\nruns less frequently than I was).  But for these experiments, I decided that I'd\nsimply drop it.</p>\n\n<p>I also wanted to introduce a consistent random seed at the start of the training\nloop.  I didn't have that in my cloud trains, and of course if we want to have solid\nresults on whether each intervention really does improve matters, then we need one\nso that we can be sure they're all starting from the same point.</p>\n\n<p>Both of those meant that I couldn't use the earlier train on the 8x A100 40 GiB machine\nas a baseline; I'd need a new one, introducing those two changes: no validation during the\ntraining run (using training loss as a proxy), and setting a random seed at the start\nfor reproducibility.</p>\n\n<p>So: what was the baseline train going to look like?</p>\n\n<h3 id=\"creating-the-baseline\">Creating the baseline</h3>\n\n<p>The first step was to <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/5175b84262a95b0c0353d784ed9bb6161e9ee882\">strip out the validation code</a>\nand to replace it with code that just took periodic checkpoints, keeping track of\nwhich one had the best average training loss over the period since the previous one.  Next, I\ndecided to <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/9ed8cfd5ff40cde47c8c49a730f0571285469cf8\">plot on the training chart</a> that is generated during the run not just\nthe training loss, but also an indicator of the maximum and minimum training loss\nover all of the steps in that period.  Then I <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/6a7e5867f86882050fee058cdf9c5880a0f3fcb1\">added the random seed</a>,\nwhich I set to 42.</p>\n\n<p>A couple of bugfixes, and we were left with <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/tree/dc341a246c13190409d0158765e340d8292a29ae\">this version of the code</a>.</p>\n\n<p>One thing to highlight: in the <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/dc341a246c13190409d0158765e340d8292a29ae/runs/8xa100m40-baseline/train.json\"><code>train.json</code></a>\nfile that specifies the various training parameters, I set the per-GPU micro-batch\nsize to 12 rather than the 13 I'd used on this size of machine earlier.\nTwo reasons for that:</p>\n\n<p>Firstly, I'm going to want to do a local run with gradient\naccumulation later, using all of the helpful interventions.\nWith gradient accumulation, you do a number of steps with batches that you can fit\ninto your memory, but you don't update the gradients each time.  After a number of those,\nyou do one big update based on the accumulated gradients -- hence the name.  The full\nbatch is all of those smaller batches taken together.</p>\n\n<p>If I want that to closely match the cloud train, I'll want the accumulated batches to\nbe the same size as each global batch in the cloud.</p>\n\n<p>Now, on my local machine, I can fit a batch of 6 into VRAM.  So that means that the\nfull batch needs to be divisible by 6 <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup>.  On the cloud train, with a micro-batch of 13 and 8 GPUs, we had an overall batch\nsize of 104 in the previous train.  104 is not divisible by 6: no joy.  But with a micro-batch size of\n12, we have an overall batch of <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>12</mn><mi>&#x000d7;</mi><mn>8</mn><mo>&#x0003d;</mo><mn>96</mn></mrow></math>, which means we'd be able to do gradient\naccumulation and do a parameter update every <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>96</mn><mi>&#x000f7;</mi><mn>6</mn><mo>&#x0003d;</mo><mn>16</mn></mrow></math> steps.</p>\n\n<p>Secondly, while my estimate of the ideal overall batch size was based on a rather arbitrary\nbit of curve-fitting, it did say that 97 was the ideal size.  So it could be interesting\nto see whether it did help!</p>\n\n<p>So, having coded that up and set up the configuration, it was time to run it.</p>\n\n<p>Here's the training chart it came up with:</p>\n\n<p><img alt=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-32a-interventions-baseline-model/baseline-training-run-chart.png\" title=\"Baseline training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>Note the loss spikes at around global steps 4,200, 13,000 and 23,000.  Those are important, I'll explain why later.</p>\n\n<p>The training run reported this at the end:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 12,243.523 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,252,160</span>\n<span class=\"go\">Throughput: 266,284 tokens/second</span>\n<span class=\"go\">Final train loss: 3.743</span>\n</code></pre>\n</div>\n\n<p>So it took about 3h24m to train, even less than we expected from the previous\ncloud experiments' estimates of how long it would take excluding validation.  About\nUS$35 in cost.</p>\n\n<p><a href=\"https://huggingface.co/gpjt/8xa100m40-baseline\">Here is the model on Hugging Face Hub</a>.</p>\n\n<p>Let's see how it looks.</p>\n\n<h3 id=\"evals\">Evals</h3>\n\n<p>For these intervention posts, I won't run the instruction-following tests, as they\ncan only be run against a batch of models in one go to get results that\n<a href=\"/2026/01/llm-from-scratch-30-digging-into-llm-as-a-judge\">are consistent with each other</a>.</p>\n\n<p>But the smoke test -- how does it complete the sequence <code>Every effort moves you</code> is worthwhile:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xa100m40-baseline/model.json<span class=\"w\"> </span>runs/8xa100m40-baseline/checkpoints/best/model.safetensors\n<span class=\"go\">Every effort moves you in on a good cause.</span>\n<span class=\"go\">If it doesn’t work you would like to join the</span>\n</code></pre>\n</div>\n\n<p>Looks good!  Reasonably coherent.</p>\n\n<p>Now we can find the loss on our held-back test set:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch (main)$ </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets/<span class=\"w\"> </span>runs/8xa100m40-baseline/model.json<span class=\"w\"> </span>runs/8xa100m40-baseline/checkpoints/best/model.safetensors\n<span class=\"go\">Fetching 4 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 990.57it/s]</span>\n<span class=\"go\">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3200/3200 [04:53&lt;00:00, 10.91it/s]</span>\n<span class=\"go\">Loss against our test dataset: 3.692</span>\n</code></pre>\n</div>\n\n<p>That's a bit worse than the 3.674 we got for the original cloud train.  Either\nthe calculations of the optimal batch size I did were not quite right (entirely likely,\nthey were very ad-hoc) or the model weights we started with, given the random seed\nwe're using, just happened to lead us in a slightly worse direction (also plausible).  Either way, it's\nin line with what we expected, and is still better than the test loss of 3.725 that we\ngot with the second-best machine in the cloud comparison post (the 8x H100 80 GiB with a global batch size of 216).</p>\n\n<p>So: we have a solid baseline model -- before we wrap up, let's consider those spikes in\nthe loss that I called out in the training chart.</p>\n\n<h3 id=\"the-loss-spikes\">The loss spikes</h3>\n\n<p>Random spikes in the loss are a Bad Thing, right?  Certainly they're a bad thing for\na train in general, especially if you don't know for sure what's causing them.  But my\nworking assumption has been that they're caused by exploding gradients -- for some\nspecific sample in the dataset, the gradients have gone up to some insanely high value,\nand we've had a bad update to our parameters as a result.  It hasn't completely knocked the model back to its starting\npoint, but it does take some time to recover, so we lose the benefit of some of our\ntraining.</p>\n\n<p>If that is the case -- and it's not just something like a batch happening to have stuff\nthat's wildly different to the rest of the training data, or something weird in the optimiser\n-- then gradient clipping is the solution.  I wanted to see if it\nwould help the model quality in general, but of course if we hadn't had any loss spikes\nin this baseline train it would have been hard to see if that was the case!</p>\n\n<p>So I\nwas very glad to see them here, as if there had been none I would either have had\nto do a gradient clipping experiment with no real expectation of it helping -- or do\nanother baseline train with a different random seed in the hope that that caused some\nspikes, which would have cost another US$35.</p>\n\n<p>All in all, it was good to see them there, as it sets us up well for that experiment.</p>\n\n<h3 id=\"wrapping-up\">Wrapping up</h3>\n\n<p>So, we've trained a baseline model that we can make changes to -- the interventions\nI listed at the start -- and get a pretty reliable understanding of whether or not\nthey help the quality of the final model.  With that in place, we're in a good position\nto start running those intervention tests!</p>\n\n<p>Given the loss spike situation in that chart, I think that a solid first one to go\nfor -- even though it was the last in that list at the top of this post -- is gradient\nclipping.  Where are those loss spikes coming from, and if it's exploding gradients, what happens if we limit the\ndamage they do with gradient clipping?</p>\n\n<p>Stay tuned!  I've already done the training run for that (while I wrote this one up),\nso I should be able to post about it tomorrow.</p>\n\n<p><a href=\"/2026/02/llm-from-scratch-32b-interventions-gradient-clipping\">Here's a link to the next post in this series</a>.</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>Well, you could potentially do something with batches of different sizes, but\nthat would be fiddly.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-1\" title=\"Jump back to footnote 1 in the text.\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
  "id": "/2026/02/llm-from-scratch-32a-interventions-baseline-model"
}