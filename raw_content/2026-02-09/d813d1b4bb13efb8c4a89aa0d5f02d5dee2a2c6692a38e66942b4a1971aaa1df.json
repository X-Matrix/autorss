{
  "title": "Quoting Andrej Karpathy",
  "link": "https://simonwillison.net/2026/Jan/31/andrej-karpathy/#atom-everything",
  "published": "2026-01-31T21:44:02+00:00",
  "summary": "<blockquote cite=\"https://twitter.com/karpathy/status/2017703360393318587\"><p>Originally in 2019, GPT-2 was trained by OpenAI on 32 TPU v3 chips for 168 hours (7 days), with $8/hour/TPUv3 back then, for a total cost of approx. $43K. It achieves 0.256525 CORE score, which is an ensemble metric introduced in the DCLM paper over 22 evaluations like ARC/MMLU/etc.</p>\n<p>As of the last few improvements merged into nanochat (many of them originating in modded-nanogpt repo), I can now reach a higher CORE score in 3.04 hours (~$73) on a single 8XH100 node. This is a 600X cost reduction over 7 years, i.e. the cost to train GPT-2 is falling approximately 2.5X every year.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/karpathy/status/2017703360393318587\">Andrej Karpathy</a></p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/andrej-karpathy\">andrej-karpathy</a>, <a href=\"https://simonwillison.net/tags/gpt-2\">gpt-2</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a></p>",
  "id": "https://simonwillison.net/2026/Jan/31/andrej-karpathy/#atom-everything"
}