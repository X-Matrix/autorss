{
  "title": "Writing an LLM from scratch, part 29 -- using DistributedDataParallel to train a base model from scratch in the cloud",
  "link": "https://www.gilesthomas.com/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud",
  "published": "Wed, 07 Jan 2026 20:40:00 +0000",
  "summary": "<p>I'm carrying on with my <a href=\"/2025/11/llm-from-scratch-27-whats-left-and-whats-next\">\"extra credit\" projects</a> after finishing the main body of\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\".\nHaving proven that I could <a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">train a GPT-2 small scale base model from scratch\non my RTX 3090</a>\nin 48 hours, I wanted to try training it on a multi-GPU machine on Lambda Labs.\nThere are two benefits I see in doing that:</p>\n\n<ol>\n<li>I can learn what you need to change in a simple single-GPU training loop to\nmake it multi-GPU.</li>\n<li>If I can get the training time for a full base model down from 48 hours\nto something more manageable (and hopefully not too expensive) -- then I\ncan try a few experiments to see how I can improve the quality of the trained\nmodel.  I have\n<a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch#but-why-is-our-model-worse-than-openais\">a bunch of ideas</a>\nabout why my own base model wasn't as good as\nthe original OpenAI one, and it would be good to know which (if any) of them\nare right.</li>\n</ol>\n\n<p>In addition, I wanted to see if anything unexpected dropped out of it; after all,\nthere were four different sizes of machines that I wanted to try, so I'd be doing\nfour from-scratch trains on the same dataset.  Does the machine size affect the quality\nof the model in some way?</p>\n\n<p>Here's what happened.  As with the last post, this is a set of tidied-up lab notes,\nso you can see the full journey.  There's a lot to it!  I was considering splitting\nit into multiple posts -- \"writing the code\", \"building the datasets\", \"running the trains\" --\nbut they're interleaved.  Each train taught me something about how to structure the code\nto make it easier to use, so the code kept changing.</p>\n\n<p>So I think it's worth documenting\nthe process as it really was.  If at some point I want to write a how-to document on\nporting single-GPU code to multi-GPU, I'll be able to mine this for resources, and in\nthe meantime, hopefully this will be of use to readers -- even if it's just at the level of\n\"I got this error message, how do I fix it?\"</p>\n\n<p>Anyway, once again I don't want to bury the lede, so: after spending US$215.16 on various\ntrains on various servers, I was able to find that a reasonably\ncheap instance on Lambda Labs, with 8x A100 GPUs, each of which has 40 GiB of VRAM,\nis the sweet spot for this particular 163M-parameter, ~Chinchilla-optimal single-epoch run.  They can train the model in less than\nfour hours, they happen to be the right size for batches that minimise loss (more on that later),\nand can do that train for about US$35, excluding validation.</p>\n\n<p>If you'd like to read the gory details of what I did, then read on -- but if you\nprefer, you can jump straight to <a href=\"#the-results\">the results</a>.</p>\n<h3 id=\"which-multi-gpu-technique\">Which multi-GPU technique?</h3>\n\n<p>Back when I was <a href=\"/fine-tuning\">messing around with fine-tuning LLMs</a> using the Hugging Face ecosystem\n-- <a href=\"https://huggingface.co/docs/transformers/en/index\">their \"Transformers\" library</a> and so on --\none of the experiments I did was to <a href=\"/2024/05/fine-tuning-3\">fine-tune a 0.5B Qwen model on an 8x GPU machine</a>.\nAs part of that, I came across\n<a href=\"https://huggingface.co/docs/transformers/en/perf_train_gpu_many\">this excellent HF page summarising different kinds of multi-GPU training techniques</a>.\nThe three that are relevant are:</p>\n\n<ol>\n<li>DataParallel (DP).  With this:\n<ul>\n<li>The default GPU (normally <code>gpu0</code>) is in charge of the process.  It gets a batch\nof data, divides it up into per-GPU \"micro-batches\", and sends each of those to\na thread for each of the other GPUs.</li>\n<li>It then sends an up-to-date version of the model to each GPU.</li>\n<li>Next, all of the per-GPU threads do a forward pass on their replica using their specific micro-batch, and send their outputs to the thread for the default GPU.</li>\n<li>The default GPU thread aggregates all of those outputs (similarly to how the losses\nacross all of our batches and the prefix sequences\n<a href=\"/2025/10/llm-from-scratch-20-starting-training-cross-entropy-loss\">are aggregated in the normal single-GPU case</a>)\nto work out an overall loss.</li>\n<li>It then does a backward pass.  This will start on the default GPU, as the\naggregation step is the first thing that it will come to when going backwards\nthrough the steps that came up with that overall loss.  However, it will then\ncome to operations that happened on the other GPUs and those are (somehow)\nparallelised.</li>\n<li>Once that is done, each GPU has gradients that represent how their copies of the model\ncontributed to the overall loss.</li>\n<li>Finally, they send those gradients back to the default GPU, which combines them\n(I think of this as just being an average, though I gather it's more complex)\nand applies them, producing an updated model.</li>\n<li>Then the process repeats; the updated model on the default GPU will be sent\nto the other GPUs in the second step of the next iteration.</li>\n</ul></li>\n<li>DistributedDataParallel (DDP).  This does less work on the default GPU and does less copying around.\nEach GPU has its own process (rather than thread), and is essentially responsible for its own training loop.\nRight at the very start, the default GPU's process sends the model to all of the others.\nThen all processes go into their training loop:\n<ul>\n<li>Firstly, each one\nworks out its own micro-batch (which means you need to have code to make sure\nthat the datasets are properly split across the GPUs)</li>\n<li>Each model does its own forward pass, then its own backward pass, working out its\nown independent gradients.</li>\n<li>As it comes up with those gradients, it broadcasts them to a \"reducer\",\nwhich handles the aggregation.  This is done in a distributed way -- there's not\njust one reducer handling everything.</li>\n<li>When all models have completed the backward pass, the reducer has a set of\ncombined gradients, which is visible from the per-GPU processes.</li>\n<li>Each GPU process does its own optimizer step using those combined gradients.</li>\n<li>That means that there's no model copy required -- each GPU has applied\nthe same gradient update, so they already have in-sync models, assuming\neverything went well.</li>\n</ul></li>\n<li>ZeRO.  This is a much more complex system, and I went into how it works\n<a href=\"/2024/05/fine-tuning-4\">in this blog post</a>.</li>\n</ol>\n\n<p>Now, from what I understand, due to all of the copying around of models, plus\nthe issues inherent with the GIL in Python, DDP is actually better than DP despite being\nmore complicated -- and more flexible!  Per Hugging Face:</p>\n\n<blockquote>\n  <p>DDP is recommended because it reduces communication overhead between GPUs, efficiently utilizes each GPU, and scales to more than one machine.</p>\n</blockquote>\n\n<p>It might be a while before I want to try multi-machine training, but it would be\nawesome to have code that's ready to do that without needing any extra work.</p>\n\n<p>Now, how to implement it?</p>\n\n<h3 id=\"implementing-ddp-for-our-model\">Implementing DDP for our model.</h3>\n\n<p>Hugging Face have a library called <a href=\"https://huggingface.co/docs/accelerate/index\">Accelerate</a>,\nwhich does everything for you:</p>\n\n<blockquote>\n  <p>Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code!</p>\n</blockquote>\n\n<p>That does sound very useful, but I worry that by using it I won't learn as much.\nIt also rather ties you in to the HF ecosystem.\nThat's not necessarily a bad thing -- I enjoyed using their stuff in my fine-tuning\nproject -- but I'm trying for a somewhat lower-level view in this series.</p>\n\n<p>So, let's use the PyTorch-native stuff.  There's a <a href=\"https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html\">\"getting started\" tutorial</a>,\nso we can follow that.</p>\n\n<p>It has two options for running using DDP, one with a bit of extra setup code --\nthe first example, under \"Basic Use Case\" -- and one that uses <code>torchrun</code> to make\nthings easier.  The second sounds best.</p>\n\n<p>The code changes actually look really simple; given a normal single-GPU training script,\nyou need to do some setup at the start:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.distributed</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">dist</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">torch.nn.parallel</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">DistributedDataParallel</span> <span class=\"k\">as</span> <span class=\"n\">DDP</span>\n\n<span class=\"c1\"># ...</span>\n\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">set_device_index</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">&quot;LOCAL_RANK&quot;</span><span class=\"p\">]))</span>\n    <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">current_accelerator</span><span class=\"p\">()</span>\n    <span class=\"n\">backend</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">get_default_backend_for_device</span><span class=\"p\">(</span><span class=\"n\">acc</span><span class=\"p\">)</span>\n    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">init_process_group</span><span class=\"p\">(</span><span class=\"n\">backend</span><span class=\"p\">)</span>\n    <span class=\"n\">rank</span> <span class=\"o\">=</span> <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">get_rank</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Start running basic DDP example on rank </span><span class=\"si\">{</span><span class=\"n\">rank</span><span class=\"si\">}</span><span class=\"s2\">.&quot;</span><span class=\"p\">)</span>\n    <span class=\"c1\"># create model and move it to GPU with id rank</span>\n    <span class=\"n\">device_id</span> <span class=\"o\">=</span> <span class=\"n\">rank</span> <span class=\"o\">%</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">device_count</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...then wrap the model itself in a <code>DDP</code> object, which is what you actually do the\ntrain on:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ToyModel</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device_id</span><span class=\"p\">)</span>\n    <span class=\"n\">ddp_model</span> <span class=\"o\">=</span> <span class=\"n\">DDP</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">device_ids</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">device_id</span><span class=\"p\">])</span>\n</code></pre>\n</div>\n\n<p>...and a bit of teardown at the end:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">destroy_process_group</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>The way to look at this is that <code>torchrun</code> will spin off one process per GPU, each\nrunning exactly the same code.  They have a \"rank\", which is an integer saying which\nof the per-GPU processes they are -- 0 for GPU 0, 1 for GPU 1, and so on.  There's\na bit of a gotcha here, though -- you can see that we're looking at an environment\nvariable called <code>LOCAL_RANK</code> at the start, but we then get a (non-\"local\") <code>rank</code>\nvariable from <code>torch.distributed</code> a bit later on.  This is due to the multi-machine\npossibilities with DDP -- if you have multiple machines, then the local rank will\nbe \"which GPU on the machine does this process relate to\", but there will also be a\n\"global\" rank, which is unique across all machines.  This distinction won't matter\nthat much during this one-machine test, but it's worth keeping in mind if we want to\nkeep the code in a shape where it could potentially scale to multiple machines.</p>\n\n<p>Anyway, after the processes are spun up, they will do their training, and the synchronisation\nand passing around of gradients during the backward pass will all happen invisibly\nin the background, so when we do our <code>optimizer.step()</code>, it will have the full set\nof gradients.</p>\n\n<p>Now that means that we'll presumably also need to use the rank -- that is, which of the <em>n</em> per-GPU\nprocesses the current code is running in -- when selecting which dataset items to\ntrain on.  More about that later.</p>\n\n<p>Let's start writing some code!  I'll use a <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch\">new repo</a>,\ninto which I can put just the code needed for this train.  I'll also structure\nit a little better than last time, with separate \"runs\", each of which has a model\nconfig and training parameters, and will later on have its own checkpoints.  You can\nthink of these as being one per machine size that I'm trying out -- I'll create a run directory for\neach one.</p>\n\n<p><a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/0c3462c71874d83511eaabdf18fe8a91a7c31c2b/ddp_train.py\">Here's a first cut</a>,\nsimply loading up a model config from a run's directory, using it to create the\nmodel, and then doing the wrapping above -- no training at all.  Running it with <code>torchrun</code> (and <code>uv</code>, as\nI'm using that for all new projects):</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>torchrun<span class=\"w\"> </span>ddp_train.py<span class=\"w\"> </span>original\nOn<span class=\"w\"> </span>rank<span class=\"w\"> </span><span class=\"m\">0</span>.\n</code></pre>\n</div>\n\n<p>Promising.  Now, unfortunately we only have one GPU locally, and the code assumes\nthat it's one process per GPU (I believe that's a hard limitation for PyTorch's\nDDP), so running with <code>--nproc_per_node=2</code> blows up.  So we can't do an in-depth\ntest locally.</p>\n\n<p>But at least we know that the basic infra is there and working.</p>\n\n<p>Now let's move the other training code from the single-GPU script into that file, pretty much blindly.\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/2e781226034bbd2a7f0a9204f8f9c0bd55bac261/ddp_train.py\">This is the result</a> --\nit's doing almost nothing beyond what the last train did, apart from wrapping the model\nin a <code>DDP</code> object -- the only other changes are to use this \"runs\" directory that\nwe've introduced.</p>\n\n<p>As a quick hack, we should try running it.  It does a validation and checkpoint before it starts,\nand we can make that happen quickly by hacking the validation loop to only\ndo a couple of iterations:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">for</span> <span class=\"n\">val_inputs</span><span class=\"p\">,</span> <span class=\"n\">val_targets</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"n\">val_ds</span><span class=\"p\">[:</span><span class=\"mi\">2</span><span class=\"p\">]):</span>\n</code></pre>\n</div>\n\n<p>(Foreshadowing: that hack will come back to haunt us later!)</p>\n\n<p>Running that, then hitting control-C after the validation completes, and it looks OK:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>torchrun<span class=\"w\"> </span>ddp_train.py<span class=\"w\"> </span>original\nOn<span class=\"w\"> </span>rank<span class=\"w\"> </span><span class=\"m\">0</span>.\nStarting<span class=\"w\"> </span>training<span class=\"w\"> </span>at<span class=\"w\"> </span>dataset<span class=\"w\"> </span>offset<span class=\"w\"> </span><span class=\"m\">0</span>\n<span class=\"w\">  </span><span class=\"m\">0</span>%<span class=\"p\">|</span><span class=\"w\">                                                                                                                                          </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/530630<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>Validation/checkpoint\n<span class=\"m\">100</span>%<span class=\"p\">|</span>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">2</span>/2<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.95it/s<span class=\"o\">]</span>\nContinuing<span class=\"w\"> </span>training█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">2</span>/2<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.96it/s<span class=\"o\">]</span>\n<span class=\"w\">  </span><span class=\"m\">0</span>%<span class=\"p\">|</span><span class=\"w\">                                                                                                                              </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">18</span>/530630<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:06&lt;<span class=\"m\">45</span>:20:54,<span class=\"w\">  </span><span class=\"m\">3</span>.25it/s<span class=\"o\">]</span>^CW1203<span class=\"w\"> </span><span class=\"m\">18</span>:34:11.363000<span class=\"w\"> </span><span class=\"m\">471545</span><span class=\"w\"> </span>torch/distributed/elastic/agent/server/api.py:725<span class=\"o\">]</span><span class=\"w\"> </span>Received<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>death<span class=\"w\"> </span>signal,<span class=\"w\"> </span>shutting<span class=\"w\"> </span>down<span class=\"w\"> </span>workers\nW1203<span class=\"w\"> </span><span class=\"m\">18</span>:34:11.364000<span class=\"w\"> </span><span class=\"m\">471545</span><span class=\"w\"> </span>torch/distributed/elastic/multiprocessing/api.py:908<span class=\"o\">]</span><span class=\"w\"> </span>Sending<span class=\"w\"> </span>process<span class=\"w\"> </span><span class=\"m\">471607</span><span class=\"w\"> </span>closing<span class=\"w\"> </span>signal<span class=\"w\"> </span>SIGINT\n<span class=\"w\">  </span><span class=\"m\">0</span>%<span class=\"p\">|</span><span class=\"w\">                                                                                                                              </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">18</span>/530630<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:07&lt;<span class=\"m\">57</span>:44:53,<span class=\"w\">  </span><span class=\"m\">2</span>.55it/s<span class=\"o\">]</span>\n\nAborted!\n</code></pre>\n</div>\n\n<p>...and we have what look like solid checkpoints:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>ls<span class=\"w\"> </span>-lrt<span class=\"w\"> </span>runs/original/checkpoints/\ntotal<span class=\"w\"> </span><span class=\"m\">4</span>\nlrwxrwxrwx<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\">   </span><span class=\"m\">27</span><span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">3</span><span class=\"w\"> </span><span class=\"m\">18</span>:34<span class=\"w\"> </span>latest<span class=\"w\"> </span>-&gt;<span class=\"w\"> </span>20251203Z183404-iteration-0\nlrwxrwxrwx<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\">   </span><span class=\"m\">27</span><span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">3</span><span class=\"w\"> </span><span class=\"m\">18</span>:34<span class=\"w\"> </span>best<span class=\"w\"> </span>-&gt;<span class=\"w\"> </span>20251203Z183404-iteration-0\ndrwxr-xr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">3</span><span class=\"w\"> </span><span class=\"m\">18</span>:34<span class=\"w\"> </span>20251203Z183404-iteration-0\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>ls<span class=\"w\"> </span>-lrth<span class=\"w\"> </span>runs/original/checkpoints/20251203Z183404-iteration-0/\ntotal<span class=\"w\"> </span><span class=\"m\">1</span>.9G\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\"> </span>670M<span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">3</span><span class=\"w\"> </span><span class=\"m\">18</span>:34<span class=\"w\"> </span>model.safetensors\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\"> </span><span class=\"m\">1</span>.4K<span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">3</span><span class=\"w\"> </span><span class=\"m\">18</span>:34<span class=\"w\"> </span>scaler.pt\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\"> </span><span class=\"m\">1</span>.3G<span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">3</span><span class=\"w\"> </span><span class=\"m\">18</span>:34<span class=\"w\"> </span>optimizer.pt\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\">  </span><span class=\"m\">105</span><span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">3</span><span class=\"w\"> </span><span class=\"m\">18</span>:34<span class=\"w\"> </span>meta.json\n</code></pre>\n</div>\n\n<p>However, loading one of those checkpoints fails:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>torchrun<span class=\"w\"> </span>ddp_train.py<span class=\"w\"> </span>original<span class=\"w\"> </span>best\nOn<span class=\"w\"> </span>rank<span class=\"w\"> </span><span class=\"m\">0</span>.\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\"> </span>Traceback<span class=\"w\"> </span><span class=\"o\">(</span>most<span class=\"w\"> </span>recent<span class=\"w\"> </span>call<span class=\"w\"> </span>last<span class=\"o\">)</span>:\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">   </span>File<span class=\"w\"> </span><span class=\"s2\">&quot;/home/giles/Dev/ddp-base-model-from-scratch/ddp_train.py&quot;</span>,<span class=\"w\"> </span>line<span class=\"w\"> </span><span class=\"m\">229</span>,<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>&lt;module&gt;\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span>main<span class=\"o\">()</span>\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span>~~~~^^\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">   </span>File<span class=\"w\"> </span><span class=\"s2\">&quot;/home/giles/Dev/ddp-base-model-from-scratch/.venv/lib/python3.13/site-packages/click/core.py&quot;</span>,<span class=\"w\"> </span>line<span class=\"w\"> </span><span class=\"m\">1485</span>,<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>__call__\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span><span class=\"k\">return</span><span class=\"w\"> </span>self.main<span class=\"o\">(</span>*args,<span class=\"w\"> </span>**kwargs<span class=\"o\">)</span>\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">            </span>~~~~~~~~~^^^^^^^^^^^^^^^^^\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">   </span>File<span class=\"w\"> </span><span class=\"s2\">&quot;/home/giles/Dev/ddp-base-model-from-scratch/.venv/lib/python3.13/site-packages/click/core.py&quot;</span>,<span class=\"w\"> </span>line<span class=\"w\"> </span><span class=\"m\">1406</span>,<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>main\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span><span class=\"nv\">rv</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span>self.invoke<span class=\"o\">(</span>ctx<span class=\"o\">)</span>\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">   </span>File<span class=\"w\"> </span><span class=\"s2\">&quot;/home/giles/Dev/ddp-base-model-from-scratch/.venv/lib/python3.13/site-packages/click/core.py&quot;</span>,<span class=\"w\"> </span>line<span class=\"w\"> </span><span class=\"m\">1269</span>,<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>invoke\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span><span class=\"k\">return</span><span class=\"w\"> </span>ctx.invoke<span class=\"o\">(</span>self.callback,<span class=\"w\"> </span>**ctx.params<span class=\"o\">)</span>\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">            </span>~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">   </span>File<span class=\"w\"> </span><span class=\"s2\">&quot;/home/giles/Dev/ddp-base-model-from-scratch/.venv/lib/python3.13/site-packages/click/core.py&quot;</span>,<span class=\"w\"> </span>line<span class=\"w\"> </span><span class=\"m\">824</span>,<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>invoke\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span><span class=\"k\">return</span><span class=\"w\"> </span>callback<span class=\"o\">(</span>*args,<span class=\"w\"> </span>**kwargs<span class=\"o\">)</span>\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">   </span>File<span class=\"w\"> </span><span class=\"s2\">&quot;/home/giles/Dev/ddp-base-model-from-scratch/ddp_train.py&quot;</span>,<span class=\"w\"> </span>line<span class=\"w\"> </span><span class=\"m\">211</span>,<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>main\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span>train_ds_offset,<span class=\"w\"> </span><span class=\"nv\">best_loss</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span>load_checkpoint<span class=\"o\">(</span>\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">                                  </span>~~~~~~~~~~~~~~~^\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">         </span>run_dir,<span class=\"w\"> </span>checkpoint,<span class=\"w\"> </span>model,<span class=\"w\"> </span>optimizer,<span class=\"w\"> </span>scaler\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">         </span>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span><span class=\"o\">)</span>\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span>^\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">   </span>File<span class=\"w\"> </span><span class=\"s2\">&quot;/home/giles/Dev/ddp-base-model-from-scratch/checkpointing.py&quot;</span>,<span class=\"w\"> </span>line<span class=\"w\"> </span><span class=\"m\">16</span>,<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>load_checkpoint\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span>model.load_state_dict<span class=\"o\">(</span>load_file<span class=\"o\">(</span>checkpoint_dir<span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"s2\">&quot;model.safetensors&quot;</span><span class=\"o\">))</span>\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span>~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">   </span>File<span class=\"w\"> </span><span class=\"s2\">&quot;/home/giles/Dev/ddp-base-model-from-scratch/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py&quot;</span>,<span class=\"w\"> </span>line<span class=\"w\"> </span><span class=\"m\">2629</span>,<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>load_state_dict\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span>raise<span class=\"w\"> </span>RuntimeError<span class=\"o\">(</span>\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span>...&lt;<span class=\"m\">3</span><span class=\"w\"> </span>lines&gt;...\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">     </span><span class=\"o\">)</span>\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\"> </span>RuntimeError:<span class=\"w\"> </span>Error<span class=\"o\">(</span>s<span class=\"o\">)</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>loading<span class=\"w\"> </span>state_dict<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>GPTModel:\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">    </span>Missing<span class=\"w\"> </span>key<span class=\"o\">(</span>s<span class=\"o\">)</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>state_dict:<span class=\"w\"> </span><span class=\"s2\">&quot;tok_emb.weight&quot;</span>,<span class=\"w\"> </span><span class=\"s2\">&quot;pos_emb.weight&quot;</span>,<span class=\"w\"> </span><span class=\"s2\">&quot;trf_blocks.0.att.mask&quot;</span>,<span class=\"w\"> </span><span class=\"s2\">&quot;trf_blocks.0.att.W_query.weight&quot;</span>,\n...\n<span class=\"o\">[</span>rank0<span class=\"o\">]</span>:<span class=\"w\">    </span>Unexpected<span class=\"w\"> </span>key<span class=\"o\">(</span>s<span class=\"o\">)</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>state_dict:<span class=\"w\"> </span><span class=\"s2\">&quot;module.final_norm.scale&quot;</span>,<span class=\"w\"> </span><span class=\"s2\">&quot;module.final_norm.shift&quot;</span>,<span class=\"w\"> </span><span class=\"s2\">&quot;module.out_head.weight&quot;</span>,<span class=\"w\"> </span><span class=\"s2\">&quot;module.pos_emb.weight&quot;</span>,<span class=\"w\"> </span><span class=\"s2\">&quot;module.tok_emb.weight&quot;</span>\n...\n</code></pre>\n</div>\n\n<p>It turns out that the problem is this code when we save it:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>            <span class=\"n\">save_checkpoint</span><span class=\"p\">(</span>\n                <span class=\"n\">run_dir</span><span class=\"p\">,</span>\n                <span class=\"sa\">f</span><span class=\"s2\">&quot;iteration-</span><span class=\"si\">{</span><span class=\"n\">ix</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">,</span>\n                <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">scaler</span><span class=\"p\">,</span>\n                <span class=\"n\">avg_train_loss</span><span class=\"p\">,</span> <span class=\"n\">val_loss</span><span class=\"p\">,</span>\n                <span class=\"n\">ix</span><span class=\"p\">,</span>\n                <span class=\"n\">is_best</span>\n            <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>The <code>model</code> that we're saving is the <code>DDP</code> wrapper around our model; my guess is that\nit does actually include all of the weights for the model, hence the correct-looking\nsize for the checkpoint file, but they're renamed --\nthe <code>DDP</code> wrapper sees the underlying model as something called <code>module</code>, so (for example)\n<code>tok_emb.weight</code> would be called <code>module.tok_emb.weight</code>.</p>\n\n<p>Fixing that, with this diff:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">diff</span> <span class=\"o\">--</span><span class=\"n\">git</span> <span class=\"n\">a</span><span class=\"o\">/</span><span class=\"n\">ddp_train</span><span class=\"o\">.</span><span class=\"n\">py</span> <span class=\"n\">b</span><span class=\"o\">/</span><span class=\"n\">ddp_train</span><span class=\"o\">.</span><span class=\"n\">py</span>\n<span class=\"n\">index</span> <span class=\"mf\">7418851..963</span><span class=\"n\">fbf7</span> <span class=\"mi\">100644</span>\n<span class=\"o\">---</span> <span class=\"n\">a</span><span class=\"o\">/</span><span class=\"n\">ddp_train</span><span class=\"o\">.</span><span class=\"n\">py</span>\n<span class=\"o\">+++</span> <span class=\"n\">b</span><span class=\"o\">/</span><span class=\"n\">ddp_train</span><span class=\"o\">.</span><span class=\"n\">py</span>\n<span class=\"o\">@@</span> <span class=\"o\">-</span><span class=\"mi\">137</span><span class=\"p\">,</span><span class=\"mi\">12</span> <span class=\"o\">+</span><span class=\"mi\">137</span><span class=\"p\">,</span><span class=\"mi\">13</span> <span class=\"o\">@@</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">train</span><span class=\"p\">(</span>\n         <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">ix</span> <span class=\"o\">%</span> <span class=\"n\">VAL_AND_CHECKPOINT_INTERVAL</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"ow\">or</span> <span class=\"p\">(</span><span class=\"n\">ix</span> <span class=\"o\">==</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_ds</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">):</span>\n             <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Validation/checkpoint&quot;</span><span class=\"p\">)</span>\n             <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n<span class=\"o\">+</span>            <span class=\"n\">base_model</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">module</span>\n             <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">inference_mode</span><span class=\"p\">(),</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n                 <span class=\"n\">val_losses</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n                 <span class=\"k\">for</span> <span class=\"n\">val_inputs</span><span class=\"p\">,</span> <span class=\"n\">val_targets</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"n\">val_ds</span><span class=\"p\">):</span>\n                     <span class=\"n\">val_inputs</span> <span class=\"o\">=</span> <span class=\"n\">val_inputs</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">long</span><span class=\"p\">)</span>\n                     <span class=\"n\">val_targets</span> <span class=\"o\">=</span> <span class=\"n\">val_targets</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">long</span><span class=\"p\">)</span>\n<span class=\"o\">-</span>                    <span class=\"n\">val_logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">val_inputs</span><span class=\"p\">)</span>\n<span class=\"o\">+</span>                    <span class=\"n\">val_logits</span> <span class=\"o\">=</span> <span class=\"n\">base_model</span><span class=\"p\">(</span><span class=\"n\">val_inputs</span><span class=\"p\">)</span>\n                     <span class=\"n\">val_losses</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span>\n                         <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">val_logits</span><span class=\"p\">,</span> <span class=\"n\">val_targets</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n                     <span class=\"p\">)</span>\n<span class=\"o\">@@</span> <span class=\"o\">-</span><span class=\"mi\">160</span><span class=\"p\">,</span><span class=\"mi\">7</span> <span class=\"o\">+</span><span class=\"mi\">161</span><span class=\"p\">,</span><span class=\"mi\">7</span> <span class=\"o\">@@</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">train</span><span class=\"p\">(</span>\n             <span class=\"n\">save_checkpoint</span><span class=\"p\">(</span>\n                 <span class=\"n\">run_dir</span><span class=\"p\">,</span>\n                 <span class=\"sa\">f</span><span class=\"s2\">&quot;iteration-</span><span class=\"si\">{</span><span class=\"n\">ix</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">,</span>\n<span class=\"o\">-</span>                <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">scaler</span><span class=\"p\">,</span>\n<span class=\"o\">+</span>                <span class=\"n\">base_model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">scaler</span><span class=\"p\">,</span>\n                 <span class=\"n\">avg_train_loss</span><span class=\"p\">,</span> <span class=\"n\">val_loss</span><span class=\"p\">,</span>\n                 <span class=\"n\">ix</span><span class=\"p\">,</span>\n                 <span class=\"n\">is_best</span>\n</code></pre>\n</div>\n\n<p>...sorts it out -- we can load our checkpoints again.  Here's <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/7d1f189a0174e1a88d6f21ce2d4b0b88ab2965f7/ddp_train.py\">the updated file</a>.</p>\n\n<p>I think we're going to have to revisit checkpointing and validation again; we don't\nwant to do it in all of our processes, probably only on global rank 0, and we'll need to\nsomehow synchronise everything so that the other processes don't carry on training\nwhile we're doing it.</p>\n\n<p>But before we get on to that, there are a couple of other things to change.\nAt the top of the file we're defining some\nconstants that look wrong:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">BATCH_SIZE</span> <span class=\"o\">=</span> <span class=\"mi\">6</span>\n<span class=\"n\">SEQ_LENGTH</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span>\n<span class=\"n\">VAL_AND_CHECKPOINT_INTERVAL</span> <span class=\"o\">=</span> <span class=\"mi\">2000</span>\n</code></pre>\n</div>\n\n<h3 id=\"sequence-length\">Sequence length</h3>\n\n<p>We'll handle the dumbest of these first; it was actually silly that in the old\ncode we had a constant for sequence length.  We're using the context\nlength of the model for that, so it's duplicated information.  Let's get it\nfrom the <code>model_conf</code>:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">diff</span> <span class=\"o\">--</span><span class=\"n\">git</span> <span class=\"n\">a</span><span class=\"o\">/</span><span class=\"n\">ddp_train</span><span class=\"o\">.</span><span class=\"n\">py</span> <span class=\"n\">b</span><span class=\"o\">/</span><span class=\"n\">ddp_train</span><span class=\"o\">.</span><span class=\"n\">py</span>\n<span class=\"n\">index</span> <span class=\"mi\">963</span><span class=\"n\">fbf7</span><span class=\"o\">.</span><span class=\"mf\">.77</span><span class=\"n\">a62ae</span> <span class=\"mi\">100644</span>\n<span class=\"o\">---</span> <span class=\"n\">a</span><span class=\"o\">/</span><span class=\"n\">ddp_train</span><span class=\"o\">.</span><span class=\"n\">py</span>\n<span class=\"o\">+++</span> <span class=\"n\">b</span><span class=\"o\">/</span><span class=\"n\">ddp_train</span><span class=\"o\">.</span><span class=\"n\">py</span>\n<span class=\"o\">@@</span> <span class=\"o\">-</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">15</span> <span class=\"o\">+</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">14</span> <span class=\"o\">@@</span> <span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">gpt</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">GPTModel</span>\n\n\n <span class=\"n\">BATCH_SIZE</span> <span class=\"o\">=</span> <span class=\"mi\">6</span>\n<span class=\"o\">-</span><span class=\"n\">SEQ_LENGTH</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span>\n <span class=\"n\">VAL_AND_CHECKPOINT_INTERVAL</span> <span class=\"o\">=</span> <span class=\"mi\">2000</span>\n\n\n <span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">BigTrainDataset</span><span class=\"p\">(</span><span class=\"n\">Dataset</span><span class=\"p\">):</span>\n\n<span class=\"o\">-</span>    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">all_tokens</span><span class=\"p\">):</span>\n<span class=\"o\">-</span>        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">xs</span> <span class=\"o\">=</span> <span class=\"n\">all_tokens</span><span class=\"p\">[:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">BATCH_SIZE</span><span class=\"p\">,</span> <span class=\"n\">SEQ_LENGTH</span><span class=\"p\">)</span>\n<span class=\"o\">-</span>        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ys</span> <span class=\"o\">=</span> <span class=\"n\">all_tokens</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">BATCH_SIZE</span><span class=\"p\">,</span> <span class=\"n\">SEQ_LENGTH</span><span class=\"p\">)</span>\n<span class=\"o\">+</span>    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">all_tokens</span><span class=\"p\">,</span> <span class=\"n\">seq_length</span><span class=\"p\">):</span>\n<span class=\"o\">+</span>        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">xs</span> <span class=\"o\">=</span> <span class=\"n\">all_tokens</span><span class=\"p\">[:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">BATCH_SIZE</span><span class=\"p\">,</span> <span class=\"n\">seq_length</span><span class=\"p\">)</span>\n<span class=\"o\">+</span>        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ys</span> <span class=\"o\">=</span> <span class=\"n\">all_tokens</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">BATCH_SIZE</span><span class=\"p\">,</span> <span class=\"n\">seq_length</span><span class=\"p\">)</span>\n\n     <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__getitem__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">ix</span><span class=\"p\">):</span>\n         <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">ix</span><span class=\"p\">],</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ys</span><span class=\"p\">[</span><span class=\"n\">ix</span><span class=\"p\">])</span>\n<span class=\"o\">@@</span> <span class=\"o\">-</span><span class=\"mi\">37</span><span class=\"p\">,</span><span class=\"mi\">9</span> <span class=\"o\">+</span><span class=\"mi\">36</span><span class=\"p\">,</span><span class=\"mi\">10</span> <span class=\"o\">@@</span> <span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">BigTrainDataset</span><span class=\"p\">(</span><span class=\"n\">Dataset</span><span class=\"p\">):</span>\n         <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">xs</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n\n\n<span class=\"o\">-</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">load_dataset</span><span class=\"p\">(</span><span class=\"n\">run_dir</span><span class=\"p\">,</span> <span class=\"n\">split</span><span class=\"p\">):</span>\n<span class=\"o\">+</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">load_dataset</span><span class=\"p\">(</span><span class=\"n\">run_dir</span><span class=\"p\">,</span> <span class=\"n\">split</span><span class=\"p\">,</span> <span class=\"n\">seq_length</span><span class=\"p\">):</span>\n     <span class=\"k\">return</span> <span class=\"n\">BigTrainDataset</span><span class=\"p\">(</span>\n<span class=\"o\">-</span>        <span class=\"n\">load_file</span><span class=\"p\">(</span><span class=\"n\">run_dir</span> <span class=\"o\">/</span> <span class=\"s2\">&quot;datasets&quot;</span> <span class=\"o\">/</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">split</span><span class=\"si\">}</span><span class=\"s2\">.safetensors&quot;</span><span class=\"p\">)[</span><span class=\"s2\">&quot;tokens&quot;</span><span class=\"p\">]</span>\n<span class=\"o\">+</span>        <span class=\"n\">load_file</span><span class=\"p\">(</span><span class=\"n\">run_dir</span> <span class=\"o\">/</span> <span class=\"s2\">&quot;datasets&quot;</span> <span class=\"o\">/</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">split</span><span class=\"si\">}</span><span class=\"s2\">.safetensors&quot;</span><span class=\"p\">)[</span><span class=\"s2\">&quot;tokens&quot;</span><span class=\"p\">],</span>\n<span class=\"o\">+</span>        <span class=\"n\">seq_length</span><span class=\"p\">,</span>\n     <span class=\"p\">)</span>\n\n\n<span class=\"o\">@@</span> <span class=\"o\">-</span><span class=\"mi\">205</span><span class=\"p\">,</span><span class=\"mi\">8</span> <span class=\"o\">+</span><span class=\"mi\">205</span><span class=\"p\">,</span><span class=\"mi\">8</span> <span class=\"o\">@@</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">main</span><span class=\"p\">(</span><span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">checkpoint</span><span class=\"p\">):</span>\n\n     <span class=\"n\">scaler</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">GradScaler</span><span class=\"p\">()</span>\n\n<span class=\"o\">-</span>    <span class=\"n\">train_ds</span> <span class=\"o\">=</span> <span class=\"n\">load_dataset</span><span class=\"p\">(</span><span class=\"n\">run_dir</span><span class=\"p\">,</span> <span class=\"s2\">&quot;train&quot;</span><span class=\"p\">)</span>\n<span class=\"o\">-</span>    <span class=\"n\">val_ds</span> <span class=\"o\">=</span> <span class=\"n\">load_dataset</span><span class=\"p\">(</span><span class=\"n\">run_dir</span><span class=\"p\">,</span> <span class=\"s2\">&quot;validation&quot;</span><span class=\"p\">)</span>\n<span class=\"o\">+</span>    <span class=\"n\">train_ds</span> <span class=\"o\">=</span> <span class=\"n\">load_dataset</span><span class=\"p\">(</span><span class=\"n\">run_dir</span><span class=\"p\">,</span> <span class=\"s2\">&quot;train&quot;</span><span class=\"p\">,</span> <span class=\"n\">model_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;context_length&quot;</span><span class=\"p\">])</span>\n<span class=\"o\">+</span>    <span class=\"n\">val_ds</span> <span class=\"o\">=</span> <span class=\"n\">load_dataset</span><span class=\"p\">(</span><span class=\"n\">run_dir</span><span class=\"p\">,</span> <span class=\"s2\">&quot;validation&quot;</span><span class=\"p\">,</span> <span class=\"n\">model_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;context_length&quot;</span><span class=\"p\">])</span>\n\n     <span class=\"k\">if</span> <span class=\"n\">checkpoint</span><span class=\"p\">:</span>\n         <span class=\"n\">train_ds_offset</span><span class=\"p\">,</span> <span class=\"n\">best_loss</span> <span class=\"o\">=</span> <span class=\"n\">load_checkpoint</span><span class=\"p\">(</span>\n</code></pre>\n</div>\n\n<p>...and <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/017dd79a4a3c05c7a2dc31189c9ff71cf164fa64/ddp_train.py\">here's the updated file</a>.\nThat was nice and simple.</p>\n\n<h3 id=\"batch-size\">Batch size</h3>\n\n<p>The code that we have specifies the batch size for each GPU -- that is, with <code>6</code>, we'll\nhave six sequences in each batch on each one.  Like I mentioned earlier, that's called a \"micro-batch\" in distributed\ntraining like this <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup> -- a per-GPU batch, as opposed to the overall global size across all\nGPUs -- so we could just rename it, and then we'd have <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>6</mn><mi>&#x000d7;</mi><msub><mi>n</mi><mtext>gpus</mtext></msub></mrow></math> as\na global batch size.</p>\n\n<p>However, it feels to me like this is a useful metaparameter to be able to tweak\nfrom outside the code.  I can see machines with per-GPU VRAM varying from 40 GiB to\n160 GiB on Lambda Labs, and pretty clearly that will mean there will be a varying\nlargest micro-batch size on each type.  So this is something we'll want to configure\non a per-run basis, so let's add a new <code>train.json</code> file to our run config,\nload that up, and pass it through.</p>\n\n<p>That's a simple enough fix; no need to note the diff, but <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/3e1879a8eadade783c1a6dd9f621b8cf2ae76be7/ddp_train.py\">here's the code</a>.</p>\n\n<h3 id=\"validationcheckpoint-interval\">Validation/checkpoint interval</h3>\n\n<p>This one we'll need to think about.  The size of our validation set is based on\nwhat one process running on my local RTX 3090 can validate in five minutes, and the interval (for which I\nfairly arbitrarily put 2000 in the code when copying it across) was calibrated\nfor roughly every half-hour.  Those numbers in turn were aimed at the 44 hours\nof training time I expected locally.</p>\n\n<p>For this train, we'll (hopefully!) be taking significantly less time.  We'll have\neight GPUs, so naively that's 5.5 hours of train time, and each will have more VRAM,\nso we should be able to bump up the batch size and potentially get even faster than that.  Depending on which kind of cards\nwe're using, they may be faster, too -- I found that an A100 is slower (with the same batch size) than the\nRTX 3090 in my fine-tuning experiments, but the H100 and B200 are likely faster.</p>\n\n<p>I think this is another thing for the train config; we should have the validation\ninterval (in terms of iterations) and the number of batches to do for validation.</p>\n\n<p><a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/fec0f54e4f0061f5ce93c85c4649b11f6ed7b77d/ddp_train.py\">Here's the updated code</a>.</p>\n\n<h3 id=\"datasets\">Datasets</h3>\n\n<p>Now, let's move on to the dataset.  With the code as it is right now, all of our\nper-GPU processes are using this code to iterate over the same dataset:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">for</span> <span class=\"n\">ix</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">train_ds_offset</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_ds</span><span class=\"p\">))):</span>\n</code></pre>\n</div>\n\n<p>That means that they'll all be training on the same data; the synchronisation that is\nhappening \"magically\" in the background means that they'll all train on the first item,\nwork out gradients, and step their optimiser -- so they'll essentially (modulo randomness)\nhave the same updates.  Pretty pointless!   What we want is for each of the <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>n</mi></mrow></math> per-GPU processes\nto train on <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>1</mn><mo>&#x0002f;</mo><mi>n</mi></mrow></math> of the data.</p>\n\n<p>We have two useful helpers in <a href=\"https://docs.pytorch.org/docs/stable/distributed.html\"><code>torch.distributed</code></a>:</p>\n\n<ul>\n<li><p><code>get_rank</code>, which gets the global rank of this process.  In our one-machine case, it returns 0 for the process on <code>gpu0</code>, 1 for the one on <code>gpu1</code>, and so on.\nWe're already using it in that setup code we looked at earlier:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">rank</span> <span class=\"o\">=</span> <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">get_rank</span><span class=\"p\">()</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Start running basic DDP example on rank </span><span class=\"si\">{</span><span class=\"n\">rank</span><span class=\"si\">}</span><span class=\"s2\">.&quot;</span><span class=\"p\">)</span>\n<span class=\"c1\"># create model and move it to GPU with id rank</span>\n<span class=\"n\">device_id</span> <span class=\"o\">=</span> <span class=\"n\">rank</span> <span class=\"o\">%</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">device_count</span><span class=\"p\">()</span>\n</code></pre>\n</div></li>\n<li><p><code>get_world_size</code>, which tells us how many GPU processes there are (globally -- it would be across all machines if we had more than one)</p></li>\n</ul>\n\n<p>So, the simplest thing to do is to use the world size as a step, and the rank as an offset:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">rank</span> <span class=\"o\">=</span> <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">get_rank</span><span class=\"p\">()</span>\n<span class=\"n\">world_size</span> <span class=\"o\">=</span> <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">get_world_size</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">ix</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">train_ds_offset</span> <span class=\"o\">+</span> <span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_ds</span><span class=\"p\">),</span> <span class=\"n\">world_size</span><span class=\"p\">)):</span>\n</code></pre>\n</div>\n\n<p><a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/86ea49d0e576e4dec3b5ca9b4fcd8ce8ad176a3f/ddp_train.py\">Here's the code with that</a>.</p>\n\n<h3 id=\"validation-and-checkpointing-only-on-rank-0\">Validation and checkpointing only on rank 0</h3>\n\n<p>Now, remember that the same code is running for every one of our per-GPU processes.\nThat means that all of them will do the training with forward and backward passes, and\ntheir own optimiser steps, all synchronised by PyTorch DDP magic.  But they will\nalso do their own validations -- which is kind of pointless -- and they'll also try to\nsave their own checkpoints, which would be messy because they could quite easily interfere\nwith each other; after all, all of the processes are running on the same machine and\nwould be writing to the same filesystem.</p>\n\n<p>So, as a first cut, let's just wrap an <code>if rank == 0</code> around the eval and checkpointing\nstuff -- we change this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">ix</span> <span class=\"o\">%</span> <span class=\"n\">validation_interval</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"ow\">or</span> <span class=\"p\">(</span><span class=\"n\">ix</span> <span class=\"o\">==</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_ds</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">):</span>\n</code></pre>\n</div>\n\n<p>...to this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">if</span> <span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"p\">((</span><span class=\"n\">ix</span> <span class=\"o\">%</span> <span class=\"n\">validation_interval</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"ow\">or</span> <span class=\"p\">(</span><span class=\"n\">ix</span> <span class=\"o\">==</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_ds</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)):</span>\n</code></pre>\n</div>\n\n<p>That line is getting bit long, so let's break it apart a bit:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">is_eval_iter</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"p\">(</span><span class=\"n\">ix</span> <span class=\"o\">%</span> <span class=\"n\">validation_interval</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"ow\">or</span> <span class=\"p\">(</span><span class=\"n\">ix</span> <span class=\"o\">==</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_ds</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n<span class=\"k\">if</span> <span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"n\">is_eval_iter</span><span class=\"p\">:</span>\n</code></pre>\n</div>\n\n<p>That looks OK, but there's an extra wrinkle: all of the processes are running\nthe same code, so while the rank zero one will do the eval, the others will continue\nthrough the script, so they will go\nright back around our loop and start training on the next batches -- which is bad.\nWe want our processes to be proceeding in lockstep, iteration-by-iteration.</p>\n\n<p>Luckily, the solution is simple: the <code>barrier</code> function in <code>torch.distributed</code>\nbasically says \"stop here until all of our processes have reached this point\".</p>\n\n<p>So we can use two of those -- one before the eval loop, to make sure that all of the\nprocesses have finished their training part of the iteration before we do the eval on\nrank zero,\nand one after the eval, so that the non-rank-zero processes will wait.</p>\n\n<p>One bit of complexity -- we want to do those barriers only if it's a eval iteration, but\nwe want to do them for all processes.  So we have to break up the <code>if</code> statement, and we\nwind up with this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>       <span class=\"n\">is_eval_iter</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n            <span class=\"p\">(</span><span class=\"n\">ix</span> <span class=\"o\">%</span> <span class=\"n\">validation_interval</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n            <span class=\"ow\">or</span> <span class=\"p\">(</span><span class=\"n\">ix</span> <span class=\"o\">==</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_ds</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"n\">is_eval_iter</span><span class=\"p\">:</span>\n            <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">barrier</span><span class=\"p\">()</span>\n\n            <span class=\"k\">if</span> <span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Validation/checkpoint&quot;</span><span class=\"p\">)</span>\n                <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n\n                <span class=\"n\">base_model</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">module</span>\n                <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">inference_mode</span><span class=\"p\">(),</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">autocast</span><span class=\"p\">(</span><span class=\"n\">device_type</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n                    <span class=\"n\">val_losses</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n                    <span class=\"k\">for</span> <span class=\"n\">val_inputs</span><span class=\"p\">,</span> <span class=\"n\">val_targets</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"n\">val_ds</span><span class=\"p\">[:</span><span class=\"n\">validation_batches</span><span class=\"p\">]):</span>\n                        <span class=\"n\">val_inputs</span> <span class=\"o\">=</span> <span class=\"n\">val_inputs</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">long</span><span class=\"p\">)</span>\n                        <span class=\"n\">val_targets</span> <span class=\"o\">=</span> <span class=\"n\">val_targets</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">long</span><span class=\"p\">)</span>\n                        <span class=\"n\">val_logits</span> <span class=\"o\">=</span> <span class=\"n\">base_model</span><span class=\"p\">(</span><span class=\"n\">val_inputs</span><span class=\"p\">)</span>\n                        <span class=\"n\">val_losses</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span>\n                            <span class=\"n\">calculate_loss</span><span class=\"p\">(</span><span class=\"n\">val_logits</span><span class=\"p\">,</span> <span class=\"n\">val_targets</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n                        <span class=\"p\">)</span>\n                    <span class=\"n\">val_loss</span> <span class=\"o\">=</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">val_losses</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">val_losses</span><span class=\"p\">)</span>\n\n                <span class=\"k\">if</span> <span class=\"n\">best_loss</span> <span class=\"ow\">is</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">val_loss</span> <span class=\"o\">&lt;</span> <span class=\"n\">best_loss</span><span class=\"p\">:</span>\n                    <span class=\"n\">is_best</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n                    <span class=\"n\">best_loss</span> <span class=\"o\">=</span> <span class=\"n\">val_loss</span>\n                <span class=\"k\">else</span><span class=\"p\">:</span>\n                    <span class=\"n\">is_best</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n\n                <span class=\"n\">avg_train_loss</span> <span class=\"o\">=</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">train_losses</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_losses</span><span class=\"p\">)</span>\n                <span class=\"n\">train_losses</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n                <span class=\"n\">save_checkpoint</span><span class=\"p\">(</span>\n                    <span class=\"n\">run_dir</span><span class=\"p\">,</span>\n                    <span class=\"sa\">f</span><span class=\"s2\">&quot;iteration-</span><span class=\"si\">{</span><span class=\"n\">ix</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">,</span>\n                    <span class=\"n\">base_model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">scaler</span><span class=\"p\">,</span>\n                    <span class=\"n\">avg_train_loss</span><span class=\"p\">,</span> <span class=\"n\">val_loss</span><span class=\"p\">,</span>\n                    <span class=\"n\">ix</span><span class=\"p\">,</span>\n                    <span class=\"n\">is_best</span>\n                <span class=\"p\">)</span>\n                <span class=\"n\">generate_training_chart</span><span class=\"p\">(</span><span class=\"n\">run_dir</span><span class=\"p\">)</span>\n\n                <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Continuing training&quot;</span><span class=\"p\">)</span>\n\n            <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">barrier</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>That seems to work OK (<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/a44b90088ca6373be6749cb512e4221ebb9c67b6/ddp_train.py\">code here</a>),\nbut it does give a warning:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">UserWarning: barrier(): using the device under current context. You can specify ``device_id`` in ``init_process_group`` to mute this warning.</span>\n</code></pre>\n</div>\n\n<p>So, we want to pass the device ID in when we call <code>init_process_group</code>.  Let's dig into\nthat a bit.</p>\n\n<h3 id=\"revisiting-the-init-code\">Revisiting the init code</h3>\n\n<p>Here's the copypasta that I took from the PyTorch tutorial earlier in this post:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">set_device_index</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">&quot;LOCAL_RANK&quot;</span><span class=\"p\">]))</span>\n    <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">current_accelerator</span><span class=\"p\">()</span>\n    <span class=\"n\">backend</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">get_default_backend_for_device</span><span class=\"p\">(</span><span class=\"n\">acc</span><span class=\"p\">)</span>\n    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">init_process_group</span><span class=\"p\">(</span><span class=\"n\">backend</span><span class=\"p\">)</span>\n    <span class=\"n\">rank</span> <span class=\"o\">=</span> <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">get_rank</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;On rank </span><span class=\"si\">{</span><span class=\"n\">rank</span><span class=\"si\">}</span><span class=\"s2\">.&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">device_id</span> <span class=\"o\">=</span> <span class=\"n\">rank</span> <span class=\"o\">%</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">device_count</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>Let's dig into what that is doing.</p>\n\n<p>The <code>LOCAL_RANK</code> environment variable is being set by <code>torchrun</code> to 0, 1, 2, etc as\nappropriate to tell us which process we are on this machine.  So the first line is telling PyTorch\nto <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.accelerator.device_index.html\">use the device with that index for this process</a>.</p>\n\n<p>The next line is <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.accelerator.current_accelerator.html\">getting the current accelerator</a> --\nthat is, an object that represents which acceleration hardware we're using in\nthis process.</p>\n\n<p>I think that the best way to see the combination of these two lines is that the first\nsays \"use <code>gpu0</code>\" (or 1, or 2, or...), and then the second says \"get the object\ndescribing the GPU you're using right now\".  So it's a slightly indirect way of\ngetting the object containing the details of the GPU in question.</p>\n\n<p>Next, we call <a href=\"https://docs.pytorch.org/docs/stable/distributed.html#torch.distributed.get_default_backend_for_device\"><code>torch.distributed.get_default_backend_for_device</code></a>.\nA backend in this context is an abstraction of whatever system the device in question\nis programmed using -- in the case of an Nvidia GPU, it would be some kind of thing\nthat encapsulates CUDA.</p>\n\n<p>Once that's done, we call <a href=\"https://docs.pytorch.org/docs/2.9/distributed.html#torch.distributed.init_process_group\"><code>torch.distributed.init_process_group</code></a>,\npassing in the backend that we're using.  We're saying \"initialise the internal\ndata structures for <code>torch.distributed</code> so that they're all set up properly to work\nwith the backend we specified\".</p>\n\n<p>After that, we can do stuff like getting the global rank with <code>dist.get_rank</code> and so on, because <code>torch.distributed</code>\nhas been properly initialized.  Presumably at this point we're talking to any other machines\nin a multi-machine cluster, so we can find out what our world size is and that kind of thing.</p>\n\n<p>That extra line at the end, to get the <code>device_id</code>:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">device_id</span> <span class=\"o\">=</span> <span class=\"n\">rank</span> <span class=\"o\">%</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">device_count</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...actually looks erroneous to me.  All of our code is assuming one process per GPU.\nSo I think we can just use the <code>LOCAL_RANK</code> there as well.</p>\n\n<p>Let's rewrite it like this (with some useful comments):</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"c1\"># Which of the one-per-GPU processes are we?</span>\n    <span class=\"n\">rank</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">&quot;LOCAL_RANK&quot;</span><span class=\"p\">])</span>\n\n    <span class=\"c1\"># Set ourselves up to use the GPU with ID ``rank``</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">set_device_index</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Get the accelerator object associated with that GPU,</span>\n    <span class=\"c1\"># and the associated backend object (eg. ``nccl`` for CUDA):</span>\n    <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">current_accelerator</span><span class=\"p\">()</span>\n    <span class=\"n\">backend</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">get_default_backend_for_device</span><span class=\"p\">(</span><span class=\"n\">acc</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Initialize torch.distributed; set the device ID explicitly</span>\n    <span class=\"c1\"># to avoid warnings in ``dist.barrier``</span>\n    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">init_process_group</span><span class=\"p\">(</span><span class=\"n\">backend</span><span class=\"p\">,</span> <span class=\"n\">device_id</span><span class=\"o\">=</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;On rank </span><span class=\"si\">{</span><span class=\"n\">rank</span><span class=\"si\">}</span><span class=\"s2\">.&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">GPTModel</span><span class=\"p\">(</span><span class=\"n\">model_conf</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>That seems to work well!  <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/0f17f4fc455031020400ac3ccc7d41e3dc8d44dc/ddp_train.py\">Here's the code</a>.\nHowever, I ran it past ChatGPT (largely to validate my understanding of what was\ngoing on), and it highlighted something slightly misleading about it.</p>\n\n<p>Right now, we're training on a single node, with one process per GPU.  But again, one\nof the neat-o things about this DDP stuff is that it should be able to scale to multiple\nnodes.</p>\n\n<p>Now, remember that <code>LOCAL_RANK</code> is just the rank of the current process on the specific node that it's\nrunning on -- hence the name.  If we had two machines, each with 8 GPUs, then\nthere would be a process with rank zero on each of them.</p>\n\n<p>The \"real\" rank -- that is, across all machines -- is the one that you can get from\n<code>dist.get_rank</code> once it has been initialised.  One of the things it does during that\ninitialisation is to talk to all of the other nodes and work that kind of thing out\n-- which of the local rank zero processes across all of the machines is the global rank zero\nprocess.</p>\n\n<p>So we need to use the local rank when working out which GPU we should be running on\nand so on, but we should not treat it as a global rank.</p>\n\n<p>That's actually quite fine in this case, as we're calling <code>dist.get_rank</code> inside\nthe training loop when we actually need to use the global one (when indexing into\nthe dataset, or when deciding if we're the process that should be doing evals and\ncheckpoints).  The only place where we might be confusing matters is in that\nprint, which is not important anyway, as the training loop also prints out its rank.</p>\n\n<p>So, let's tweak it a little more for clarity:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"c1\"># Which of the one-per-GPU processes are we on this machine?</span>\n    <span class=\"n\">local_rank</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">&quot;LOCAL_RANK&quot;</span><span class=\"p\">])</span>\n\n    <span class=\"c1\"># Set ourselves up to use the GPU with the ID that matches our local rank</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">set_device_index</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Get the accelerator object associated with that GPU,</span>\n    <span class=\"c1\"># and the associated backend object (eg. ``nccl`` for CUDA):</span>\n    <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">accelerator</span><span class=\"o\">.</span><span class=\"n\">current_accelerator</span><span class=\"p\">()</span>\n    <span class=\"n\">backend</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">get_default_backend_for_device</span><span class=\"p\">(</span><span class=\"n\">acc</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Initialize torch.distributed; set the device ID explicitly</span>\n    <span class=\"c1\"># to avoid warnings in ``dist.barrier``</span>\n    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">init_process_group</span><span class=\"p\">(</span><span class=\"n\">backend</span><span class=\"p\">,</span> <span class=\"n\">device_id</span><span class=\"o\">=</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>\n\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">GPTModel</span><span class=\"p\">(</span><span class=\"n\">model_conf</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>That seems to work well!  <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/5ae5ea2aa38cd5f62b609e2bf33c8a45d5b642d4/ddp_train.py\">Here's the code</a>.</p>\n\n<p>Time to run it past ChatGPT to see if I've made any dumb errors.  Turns out that\n(unsurprisingly) I have...</p>\n\n<h3 id=\"checkpointing-revisited\">Checkpointing, revisited</h3>\n\n<p>Let's go back to our code that decides whether or not it's an iteration where we need\nto do a validation run and a checkpoint:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>        <span class=\"n\">is_eval_iter</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n            <span class=\"p\">(</span><span class=\"n\">ix</span> <span class=\"o\">%</span> <span class=\"n\">validation_interval</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n            <span class=\"ow\">or</span> <span class=\"p\">(</span><span class=\"n\">ix</span> <span class=\"o\">==</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_ds</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>The problem is that our index <code>ix</code> is different in the different processes!  Remember, we have\nthis in order to pick out the correct training items:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">for</span> <span class=\"n\">ix</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">train_ds_offset</span> <span class=\"o\">+</span> <span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_ds</span><span class=\"p\">),</span> <span class=\"n\">world_size</span><span class=\"p\">)):</span>\n</code></pre>\n</div>\n\n<p>So let's think about it; in the first run through the loop, with 8 GPUs, we would have</p>\n\n<ul>\n<li><code>ix</code> = 0 for the process with rank 0</li>\n<li><code>ix</code> = 1 for the process with rank 1</li>\n<li>...</li>\n<li><code>ix</code> = 7 for the process with rank 7</li>\n</ul>\n\n<p>In the next run through the loop, we'd have:</p>\n\n<ul>\n<li><code>ix</code> = 8 for the process with rank 0</li>\n<li><code>ix</code> = 9 for the process with rank 1</li>\n<li>...</li>\n<li><code>ix</code> = 15 for the process with rank 7</li>\n</ul>\n\n<p>So <code>is_eval_iter</code> will give different results for each process.   That might not\nsound like the end of the world -- <code>ix % validation_interval</code> will only be zero for\none of them, so long as <code>validation_interval</code> is larger than the number of GPUs --\nbut remember that our validation code looks like this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>        <span class=\"k\">if</span> <span class=\"n\">is_eval_iter</span><span class=\"p\">:</span>\n            <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">barrier</span><span class=\"p\">()</span>\n\n            <span class=\"k\">if</span> <span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"c1\"># do the validation and checkpointing</span>\n\n            <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">barrier</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>Now, if different processes have different values for <code>is_eval_iter</code>, then\n<code>dist.barrier()</code> will only be called in the one(s) for which it is <code>True</code>.  But\n<code>dist.barrier()</code> means \"wait until all processes have reached this barrier\".  So the\nones that call it will lock up completely until other processes get there, and everything\nwill at best get out-of-sync, and at worst will lock up completely.</p>\n\n<p>I think that the problem here is that I'm conflating two things: the index of the global step -- that is, one\niteration across all GPUs -- and the dataset element that we want to use.  In the original\none-GPU case that made, sense; iteration 0 was on dataset element 0, iteration 1 was on element 1,\nand so on.  But now the offset into the dataset, and the global step, are quite different things.</p>\n\n<p>This is quite deeply embedded in the code, but\nwe can fix it!</p>\n\n<p>Let's start off by changing our checkpoint code, just to rename things.  It keeps track\nof a variable called <code>train_ds_offset</code>, our offset into the training dataset, and uses\nthat both to index into the dataset, and to work out how far through the train we are.\nThe latter is a much better thing to store in a checkpoint, so instead of\nsaving <code>train_ds_offset</code>, we'll store (and restore) <code>global_step</code>.  Basically, just\na rename so that the variables and stored JSON match the new reality.\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/c8702169afb44082a4571b0179e1ad4a2c43fb3d/checkpointing.py\">Here's the updated code</a>.</p>\n\n<p>Now we need to make a number of minor changes to the training loop just to match that\nrename of the value that we're checkpointing (eg. for the code to generate the training\nchart) but the most important change is to our loop.  Instead of iterating over our dataset with\na step and and offset so that we can index into it, we firstly work out how many\nglobal steps there will be:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">total_global_steps</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_ds</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">world_size</span>\n</code></pre>\n</div>\n\n<p>...then we iterate from our initial global step -- zero if we're starting a fresh\ntrain, or whatever global step we were on in a loaded checkpoint plus one if we're\ndoing a continued train from a checkpoint -- up to the <code>total_global_steps</code>:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">for</span> <span class=\"n\">global_step</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">start_global_step</span><span class=\"p\">,</span> <span class=\"n\">total_global_steps</span><span class=\"p\">)):</span>\n</code></pre>\n</div>\n\n<p>That means that we need to use the global step, the world size, and our current rank\nto work out which dataset item we should be training on for this process at this global\nstep.  Let's say that we have eight processes; on the 0th global step, we should have\nrank 0 training on dataset item 0, rank 1 on item 1, and so on.  On the next global step,\nrank 0 should train on item 8, rank 1 on 9, and so on.  So:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">targets</span> <span class=\"o\">=</span> <span class=\"n\">train_ds</span><span class=\"p\">[</span><span class=\"n\">global_step</span> <span class=\"o\">*</span> <span class=\"n\">world_size</span> <span class=\"o\">+</span> <span class=\"n\">rank</span><span class=\"p\">]</span>\n</code></pre>\n</div>\n\n<p>That's actually much more elegant than the earlier code, and seems to work fine.\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/c8702169afb44082a4571b0179e1ad4a2c43fb3d/ddp_train.py\">Here it is</a>.</p>\n\n<p>Phew, glad to have caught that before I started spending money on machines -- it would\nhave been confusing if everything locked up.  Thanks, ChatGPT!</p>\n\n<h3 id=\"slicing-the-validation-dataset\">Slicing the validation dataset</h3>\n\n<p>Another thing that raised by ChatGPT is about the validation.  We don't want to validate\nacross all of the validation dataset -- we're using a number from the <code>train.json</code>.  I have this code:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">for</span> <span class=\"n\">val_inputs</span><span class=\"p\">,</span> <span class=\"n\">val_targets</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"n\">val_ds</span><span class=\"p\">[:</span><span class=\"n\">validation_batches</span><span class=\"p\">]):</span>\n</code></pre>\n</div>\n\n<p>This looked like a nice, quick way to get the first <code>validation_batches</code> elements of\nthe validation dataset.  But ChatGPT told me it would raise.  It didn't, though -- why?</p>\n\n<p>The problem is that I had <code>validation_batches</code> set to <code>2</code> in my training config for testing.\nStepping through what that slice does, when we run <code>val_ds[:validation_batches]</code>:</p>\n\n<ul>\n<li><p>Python calls the <code>__getitem__</code> on the dataset, passing in a <code>slice</code> object as <code>ix</code>,\nso this code is called with it:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__getitem__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">ix</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">xs</span><span class=\"p\">[</span><span class=\"n\">ix</span><span class=\"p\">],</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ys</span><span class=\"p\">[</span><span class=\"n\">ix</span><span class=\"p\">])</span>\n</code></pre>\n</div></li>\n<li><p>Now, because that code doesn't do anything clever with <code>slice</code>s, they're passed straight\ndown to the tensors that make up <code>self.xs</code> and <code>self.ys</code>.  So it's actually equivalent\nto this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">xs</span><span class=\"p\">[:</span><span class=\"n\">validation_batches</span><span class=\"p\">],</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ys</span><span class=\"p\">[:</span><span class=\"n\">validation_batches</span><span class=\"p\">]</span>\n</code></pre>\n</div></li>\n<li><p>Or, to rewrite the whole loop (omitting the <code>tqdm</code> for clarity):</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">for</span> <span class=\"n\">val_inputs</span><span class=\"p\">,</span> <span class=\"n\">val_targets</span> <span class=\"ow\">in</span> <span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">xs</span><span class=\"p\">[:</span><span class=\"n\">validation_batches</span><span class=\"p\">],</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ys</span><span class=\"p\">[:</span><span class=\"n\">validation_batches</span><span class=\"p\">]):</span>\n    <span class=\"o\">...</span>\n</code></pre>\n</div></li>\n<li><p>So, the first time through the loop, we try to bind our loop variables like this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">val_inputs</span><span class=\"p\">,</span> <span class=\"n\">val_targets</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">xs</span><span class=\"p\">[:</span><span class=\"n\">validation_batches</span><span class=\"p\">]</span>\n</code></pre>\n</div>\n\n<p>That is clearly wrong!  It's equivalent to this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">val_inputs</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">xs</span><span class=\"p\">[:</span><span class=\"n\">validation_batches</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">val_targets</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">xs</span><span class=\"p\">[:</span><span class=\"n\">validation_batches</span><span class=\"p\">][</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n</code></pre>\n</div>\n\n<p>...with code to blow up if <code>self.xs[:validation_batches]</code> has more than two elements --\nthe normal Python \"ValueError: too many values to unpack\"</p></li>\n<li>But if <code>validation_batches</code> is set to 2, which it happened to be in my case, then\nit will silently fail -- our first eval loop will get the first X from the validation\nset as <code>val_inputs</code>, and the second X as <code>val_targets</code>.</li>\n</ul>\n\n<p>Nasty!  AI code review certainly helped me dodge a bullet on that one.</p>\n\n<p>Let's fix it, it's not a big change: we can just do this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">for</span> <span class=\"n\">val_ix</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">validation_batches</span><span class=\"p\">)):</span>\n        <span class=\"n\">val_inputs</span><span class=\"p\">,</span> <span class=\"n\">val_targets</span> <span class=\"o\">=</span> <span class=\"n\">val_ds</span><span class=\"p\">[</span><span class=\"n\">val_ix</span><span class=\"p\">]</span>\n</code></pre>\n</div>\n\n<p>...and that works!  So <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/87d15bfc4ff9187077b09293922df525fa42f425/ddp_train.py\">here's the code now</a>.</p>\n\n<h3 id=\"back-to-the-datasets\">Back to the datasets</h3>\n\n<p>So, I think we have one final issue, which is the training and validation datasets.  In our single-GPU\ntrain, we worked out ahead of time how much of FineWeb (or FineWeb-Edu) to train on --\nthe Chinchilla-optimal number -- and\ngenerated a dataset that contained a round number of 6-sequence, 1024-token batches\nthat was the smallest such round number that was larger than our target.  We also worked\nout exactly how large (in terms of batches) our validation dataset needed to be so that\neach validation run would take five minutes.</p>\n\n<p>There was one big issue with that system; when I decided to do an \"extended\" train\non more of the FineWeb-Edu dataset, in order to see whether I could get the loss down further,\nI had to do some nasty hackery in order to generate a new one.\nSo it would be nice to not have that problem this time around.</p>\n\n<p>Additionally, we're likely to be tweaking the batch size quite a lot in this experiment\nwhile we find what the appropriate level is to fit onto the cloud GPUs, and also varying\nhow much validation we do -- and additionally,\nwe have the world size to worry about.</p>\n\n<p>I think that the best way to give us the flexibility we need will be to pre-convert\nthe complete FineWeb and FineWeb-Edu datasets into the format we need -- each sequence\nin the dataset converted to GPT-2 tokens, and then those sequences concatenated together,\nwith the <code>&lt;|endoftext|&gt;</code> token 50257 separating them.</p>\n\n<p>It would be good to properly nail down the validation dataset at the same time.  So\nwe can have a script that loads up the original dataset as downloaded from Hugging Face,\nsplits it into 99% train, 1% validation, does the conversion, and then saves them as\nsafetensors files.</p>\n\n<p>If we use <code>uint16</code> for those (which is just large enough for our 50,257-token vocab), we can fit the ~10B tokens in each dataset's train split into 20 GiB of\ndisk.  Not too bad.</p>\n\n<p>But there will still be the issue of getting them onto our cloud machines.  Let's generate\nthe data, and then work out how to handle that.</p>\n\n<p>I tried initially with <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/aede6fbca2bf5ca518995ef39e58103e4c2dae15/prepare_datasets.py\">the code I used last time, adapted to run through the entire dataset</a>.\nIt does the 99%/1% train/validation\nsplit, and then for each of those generates a single massive tensor of tokens like this:</p>\n\n<ul>\n<li>Zoom through the records in the dataset in batches of 1,000.</li>\n<li>For each batch:\n<ul>\n<li>Tokenising each batch, so we get a list of lists of tokens.</li>\n<li>Convert that list of lists into a single list <code>&lt;|endoftext|&gt;</code> tokens separating each item.</li>\n<li>Convert that list into a PyTorch <code>uint16</code> tensor.</li>\n<li>Add the tensor to a <code>results</code> list.</li>\n</ul></li>\n<li>After that's all done, use <code>torch.cat</code> to convert the <code>results</code> list into a single\ntensor, and then save that with <code>safetensors</code>.</li>\n</ul>\n\n<p>It <em>almost</em> worked!  To my surprise, it got all the way to the end, and only\nblew up with an out-of-memory error when it was trying to save the result -- and it did that completely silently,\nso I thought it had worked right up until I tried to check the file on disk to see\nhow large it was, and it wasn't there.</p>\n\n<p>The obvious tweak: set the <code>results</code> list to <code>None</code> just after the <code>torch.cat</code>, to\nfree up the memory it's using.  Given that it was the save that triggered the OOM,\nyou'd think that that would be enough -- but it turned out not to be so.</p>\n\n<p>Rather than mess around with this for much longer, I just decided to add on 128 GiB\nof swap to my machine temporarily:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>sudo<span class=\"w\"> </span>dd<span class=\"w\"> </span><span class=\"k\">if</span><span class=\"o\">=</span>/dev/zero<span class=\"w\"> </span><span class=\"nv\">of</span><span class=\"o\">=</span>./swap<span class=\"w\"> </span><span class=\"nv\">bs</span><span class=\"o\">=</span>1G<span class=\"w\"> </span><span class=\"nv\">count</span><span class=\"o\">=</span><span class=\"m\">128</span>\n<span class=\"o\">[</span>sudo<span class=\"o\">]</span><span class=\"w\"> </span>password<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>giles:\n<span class=\"m\">128</span>+0<span class=\"w\"> </span>records<span class=\"w\"> </span><span class=\"k\">in</span>\n<span class=\"m\">128</span>+0<span class=\"w\"> </span>records<span class=\"w\"> </span>out\n<span class=\"m\">137438953472</span><span class=\"w\"> </span>bytes<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">137</span><span class=\"w\"> </span>GB,<span class=\"w\"> </span><span class=\"m\">128</span><span class=\"w\"> </span>GiB<span class=\"o\">)</span><span class=\"w\"> </span>copied,<span class=\"w\"> </span><span class=\"m\">63</span>.1124<span class=\"w\"> </span>s,<span class=\"w\"> </span><span class=\"m\">2</span>.2<span class=\"w\"> </span>GB/s\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>sudo<span class=\"w\"> </span>chmod<span class=\"w\"> </span><span class=\"m\">0600</span><span class=\"w\"> </span>./swap\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>sudo<span class=\"w\"> </span>mkswap<span class=\"w\"> </span>./swap\nSetting<span class=\"w\"> </span>up<span class=\"w\"> </span>swapspace<span class=\"w\"> </span>version<span class=\"w\"> </span><span class=\"m\">1</span>,<span class=\"w\"> </span><span class=\"nv\">size</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">128</span><span class=\"w\"> </span>GiB<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">137438949376</span><span class=\"w\"> </span>bytes<span class=\"o\">)</span>\nno<span class=\"w\"> </span>label,<span class=\"w\"> </span><span class=\"nv\">UUID</span><span class=\"o\">=</span>693d72a1-871d-4ab8-b0c8-b383b435ca8f\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>sudo<span class=\"w\"> </span>swapon<span class=\"w\"> </span>./swap\n</code></pre>\n</div>\n\n<p>...and that was enough to make it run.  So I've now generated pre-tokenised,\npre-concatenated train and validation sets for both FineWeb and FineWeb-Edu:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>ls<span class=\"w\"> </span>-lrth<span class=\"w\"> </span>fineweb-prepared/\ntotal<span class=\"w\"> </span>20G\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\"> </span>196M<span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">4</span><span class=\"w\"> </span><span class=\"m\">21</span>:02<span class=\"w\"> </span>validation.safetensors\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\">  </span>20G<span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">4</span><span class=\"w\"> </span><span class=\"m\">21</span>:20<span class=\"w\"> </span>train.safetensors\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>ls<span class=\"w\"> </span>-lrth<span class=\"w\"> </span>fineweb-edu-prepared/\ntotal<span class=\"w\"> </span>19G\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\"> </span>192M<span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">4</span><span class=\"w\"> </span><span class=\"m\">22</span>:43<span class=\"w\"> </span>validation.safetensors\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\">  </span>19G<span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">4</span><span class=\"w\"> </span><span class=\"m\">22</span>:59<span class=\"w\"> </span>train.safetensors\n</code></pre>\n</div>\n\n<p>Now, thinking about how to get it up to the Lambda Labs machines.  I have normal\n1 Gb residential broadband, so conceivably I could upload 20 GiB in about\n200 seconds.  But that's assuming that there's no network congestion, so I would\nexpect it to take longer.  The LL machines are quite expensive, and I don't want to\nwaste money keeping them up while I'm just uploading data.</p>\n\n<p>There are possibilities here:</p>\n\n<ol>\n<li>I can upload the datasets to Hugging Face; their network connection will be better\nthan mine, so I can just pay the price in time of uploading everything from home once, and\nthen I can download them faster from  HF to LL.  That also has the benefit of meaning\nthat after this experiment I can safely delete the local files, but then download\nthem again if I need them.  And if anyone else wants to repro this experiment,\nthe data will be easily available to them.</li>\n<li>Lambda Labs have persistent filesystems that you can use.  They cost $0.20/GB/month,\nso that would be about $5/month for all of my datasets.  So I could upload the data\nto a cheap instance with a persistent filesystem mounted, shut down that instance\nbut keep the filesystem, and then mount it on each machine I use to run tests. .</li>\n</ol>\n\n<p>I think the best option is to use option (1), but with the option of also doing (2).\nThe HF dataset will still take time to download to LL, even over the faster network\nconnection.  That might not be a problem -- but if it is, I download it once on a cheap\ninstance and use a persistent disk too.  Essentially I'd be using\nthe persistent disk as a \"cache\", and still get the benefits of the easily-shareable\ndatasets on Hugging Face.</p>\n\n<p>So, that decided, let's find out how we can upload a whacking great 20 GiB safetensors\nfile as a dataset on Hugging Face.</p>\n\n<h3 id=\"putting-the-datasets-on-hugging-face\">Putting the datasets on Hugging Face.</h3>\n\n<p>It turns out that resources like datasets on HF are just Git repositories using\nthe LFS (Large File System) plugin to be able to handle, well, large files.\nConveniently, given that I'm using <code>uv</code> to manage my project, there's\na <a href=\"https://huggingface.co/docs/huggingface_hub/main/en/guides/cli#using-uv\">plugin</a> that\nallows me to use their CLI tools with minimal effort, so:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uvx<span class=\"w\"> </span>hf<span class=\"w\"> </span>auth<span class=\"w\"> </span>login\n\n<span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">      </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">      </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">      </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>\n<span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">        </span>_<span class=\"p\">|</span><span class=\"w\">          </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">            </span>_<span class=\"p\">|</span><span class=\"w\">        </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">        </span>_<span class=\"p\">|</span>\n<span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">      </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">        </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>\n<span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">      </span>_<span class=\"p\">|</span><span class=\"w\">        </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">        </span>_<span class=\"p\">|</span>\n<span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">      </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span><span class=\"w\">      </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">      </span>_<span class=\"p\">|</span><span class=\"w\">        </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span><span class=\"w\">    </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span><span class=\"w\">  </span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>_<span class=\"p\">|</span>\n\n<span class=\"w\">    </span>To<span class=\"w\"> </span>log<span class=\"w\"> </span><span class=\"k\">in</span>,<span class=\"w\"> </span><span class=\"sb\">``</span>huggingface_hub<span class=\"sb\">``</span><span class=\"w\"> </span>requires<span class=\"w\"> </span>a<span class=\"w\"> </span>token<span class=\"w\"> </span>generated<span class=\"w\"> </span>from<span class=\"w\"> </span>https://huggingface.co/settings/tokens<span class=\"w\"> </span>.\nEnter<span class=\"w\"> </span>your<span class=\"w\"> </span>token<span class=\"w\"> </span><span class=\"o\">(</span>input<span class=\"w\"> </span>will<span class=\"w\"> </span>not<span class=\"w\"> </span>be<span class=\"w\"> </span>visible<span class=\"o\">)</span>:\nAdd<span class=\"w\"> </span>token<span class=\"w\"> </span>as<span class=\"w\"> </span>git<span class=\"w\"> </span>credential?<span class=\"w\"> </span><span class=\"o\">[</span>y/N<span class=\"o\">]</span>:<span class=\"w\"> </span>n\nToken<span class=\"w\"> </span>is<span class=\"w\"> </span>valid<span class=\"w\"> </span><span class=\"o\">(</span>permission:<span class=\"w\"> </span>write<span class=\"o\">)</span>.\nThe<span class=\"w\"> </span>token<span class=\"w\"> </span><span class=\"sb\">``</span><span class=\"o\">[</span>REDACTED<span class=\"o\">]</span><span class=\"sb\">``</span><span class=\"w\"> </span>has<span class=\"w\"> </span>been<span class=\"w\"> </span>saved<span class=\"w\"> </span>to<span class=\"w\"> </span>/home/giles/.cache/huggingface/stored_tokens\nYour<span class=\"w\"> </span>token<span class=\"w\"> </span>has<span class=\"w\"> </span>been<span class=\"w\"> </span>saved<span class=\"w\"> </span>to<span class=\"w\"> </span>/home/giles/.cache/huggingface/token\nLogin<span class=\"w\"> </span>successful.\nThe<span class=\"w\"> </span>current<span class=\"w\"> </span>active<span class=\"w\"> </span>token<span class=\"w\"> </span>is:<span class=\"w\"> </span><span class=\"sb\">``</span><span class=\"o\">[</span>REDACTED<span class=\"o\">]</span><span class=\"sb\">``</span>\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uvx<span class=\"w\"> </span>hf<span class=\"w\"> </span>repo<span class=\"w\"> </span>create<span class=\"w\"> </span>fineweb-gpt2-tokens<span class=\"w\"> </span>--repo-type<span class=\"w\"> </span>dataset\nSuccessfully<span class=\"w\"> </span>created<span class=\"w\"> </span>gpjt/fineweb-gpt2-tokens<span class=\"w\"> </span>on<span class=\"w\"> </span>the<span class=\"w\"> </span>Hub.\nYour<span class=\"w\"> </span>repo<span class=\"w\"> </span>is<span class=\"w\"> </span>now<span class=\"w\"> </span>available<span class=\"w\"> </span>at<span class=\"w\"> </span>https://huggingface.co/datasets/gpjt/fineweb-gpt2-tokens\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uvx<span class=\"w\"> </span>hf<span class=\"w\"> </span>repo<span class=\"w\"> </span>create<span class=\"w\"> </span>fineweb-edu-gpt2-tokens<span class=\"w\"> </span>--repo-type<span class=\"w\"> </span>dataset\nSuccessfully<span class=\"w\"> </span>created<span class=\"w\"> </span>gpjt/fineweb-edu-gpt2-tokens<span class=\"w\"> </span>on<span class=\"w\"> </span>the<span class=\"w\"> </span>Hub.\nYour<span class=\"w\"> </span>repo<span class=\"w\"> </span>is<span class=\"w\"> </span>now<span class=\"w\"> </span>available<span class=\"w\"> </span>at<span class=\"w\"> </span>https://huggingface.co/datasets/gpjt/fineweb-edu-gpt2-tokens\n</code></pre>\n</div>\n\n<p>Both datasets show up on my profile page on Hugging Face, so that's looking good.\nNow it's time to try to upload the data.  We'll need to install Git's LFS support\nfirst:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>git<span class=\"w\"> </span>lfs<span class=\"w\"> </span>install\nUpdated<span class=\"w\"> </span>Git<span class=\"w\"> </span>hooks.\nGit<span class=\"w\"> </span>LFS<span class=\"w\"> </span>initialized.\n</code></pre>\n</div>\n\n<p>Now let's try the FineWeb one first:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>git<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://huggingface.co/datasets/gpjt/fineweb-gpt2-tokens\nCloning<span class=\"w\"> </span>into<span class=\"w\"> </span><span class=\"s1\">'fineweb-gpt2-tokens'</span>...\nremote:<span class=\"w\"> </span>Enumerating<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">3</span>,<span class=\"w\"> </span><span class=\"k\">done</span>.\nremote:<span class=\"w\"> </span>Total<span class=\"w\"> </span><span class=\"m\">3</span><span class=\"w\"> </span><span class=\"o\">(</span>delta<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"o\">)</span>,<span class=\"w\"> </span>reused<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"o\">(</span>delta<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"o\">)</span>,<span class=\"w\"> </span>pack-reused<span class=\"w\"> </span><span class=\"m\">3</span><span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">)</span>\nUnpacking<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">3</span>/3<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"m\">1</span>.17<span class=\"w\"> </span>KiB<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">1</span>.17<span class=\"w\"> </span>MiB/s,<span class=\"w\"> </span><span class=\"k\">done</span>.\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span><span class=\"nb\">cd</span><span class=\"w\"> </span>fineweb-gpt2-tokens\ngiles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>cp<span class=\"w\"> </span>../fineweb-prepared/train.safetensors<span class=\"w\"> </span>.\ngiles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>cp<span class=\"w\"> </span>../fineweb-prepared/validation.safetensors<span class=\"w\"> </span>.\ngiles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>cat<span class=\"w\"> </span>&gt;<span class=\"w\"> </span>meta.json<span class=\"w\"> </span><span class=\"s\">&lt;&lt; 'EOF'</span>\n<span class=\"s\">{</span>\n<span class=\"s\">  &quot;description&quot;: &quot;FineWeb 10BT tokenized with GPT-2 BPE (tiktoken). uint16 safetensors, single long sequence with ``&lt;|endoftext|&gt;`` separators.&quot;,</span>\n<span class=\"s\">  &quot;token_dtype&quot;: &quot;uint16&quot;,</span>\n<span class=\"s\">  &quot;files&quot;: {</span>\n<span class=\"s\">    &quot;train&quot;: &quot;train.safetensors&quot;,</span>\n<span class=\"s\">    &quot;validation&quot;: &quot;validation.safetensors&quot;</span>\n<span class=\"s\">  }</span>\n<span class=\"s\">}</span>\n<span class=\"s\">EOF</span>\ngiles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>git<span class=\"w\"> </span>add<span class=\"w\"> </span>.\ngiles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>git<span class=\"w\"> </span>commit<span class=\"w\"> </span>-am<span class=\"s2\">&quot;First cut, added GPT-2 tokens&quot;</span>\n<span class=\"o\">[</span>main<span class=\"w\"> </span>3af6ef2<span class=\"o\">]</span><span class=\"w\"> </span>First<span class=\"w\"> </span>cut,<span class=\"w\"> </span>added<span class=\"w\"> </span>GPT-2<span class=\"w\"> </span>tokens\n<span class=\"w\"> </span><span class=\"m\">3</span><span class=\"w\"> </span>files<span class=\"w\"> </span>changed,<span class=\"w\"> </span><span class=\"m\">14</span><span class=\"w\"> </span>insertions<span class=\"o\">(</span>+<span class=\"o\">)</span>\n<span class=\"w\"> </span>create<span class=\"w\"> </span>mode<span class=\"w\"> </span><span class=\"m\">100644</span><span class=\"w\"> </span>meta.json\n<span class=\"w\"> </span>create<span class=\"w\"> </span>mode<span class=\"w\"> </span><span class=\"m\">100644</span><span class=\"w\"> </span>train.safetensors\n<span class=\"w\"> </span>create<span class=\"w\"> </span>mode<span class=\"w\"> </span><span class=\"m\">100644</span><span class=\"w\"> </span>validation.safetensors\ngiles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>git<span class=\"w\"> </span>push\nUsername<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"s1\">'https://huggingface.co'</span>:<span class=\"w\"> </span>gpjt\nPassword<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"s1\">'https://gpjt@huggingface.co'</span>:\nUsername<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"s1\">'https://huggingface.co'</span>:<span class=\"w\"> </span>gpjtB/s\nPassword<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"s1\">'https://gpjt@huggingface.co'</span>:\nbatch<span class=\"w\"> </span>response:\nYou<span class=\"w\"> </span>need<span class=\"w\"> </span>to<span class=\"w\"> </span>configure<span class=\"w\"> </span>your<span class=\"w\"> </span>repository<span class=\"w\"> </span>to<span class=\"w\"> </span><span class=\"nb\">enable</span><span class=\"w\"> </span>upload<span class=\"w\"> </span>of<span class=\"w\"> </span>files<span class=\"w\"> </span>&gt;<span class=\"w\"> </span>5GB.\nRun<span class=\"w\"> </span><span class=\"s2\">&quot;hf lfs-enable-largefiles ./path/to/your/repo&quot;</span><span class=\"w\"> </span>and<span class=\"w\"> </span>try<span class=\"w\"> </span>again.\n\nerror:<span class=\"w\"> </span>failed<span class=\"w\"> </span>to<span class=\"w\"> </span>push<span class=\"w\"> </span>some<span class=\"w\"> </span>refs<span class=\"w\"> </span>to<span class=\"w\"> </span><span class=\"s1\">'https://huggingface.co/datasets/gpjt/fineweb-gpt2-tokens'</span>\n</code></pre>\n</div>\n\n<p>OK, so we need some kind of extra thing to tell it we can use large files on top of\nthe LFS stuff:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uvx<span class=\"w\"> </span>hf<span class=\"w\"> </span>lfs-enable-largefiles<span class=\"w\"> </span>.\nLocal<span class=\"w\"> </span>repo<span class=\"w\"> </span><span class=\"nb\">set</span><span class=\"w\"> </span>up<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>largefiles\n</code></pre>\n</div>\n\n<p>Right, now let's try again:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>git<span class=\"w\"> </span>push\nUsername<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"s1\">'https://huggingface.co'</span>:<span class=\"w\"> </span>gpjt\nPassword<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"s1\">'https://gpjt@huggingface.co'</span>:\nUsername<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"s1\">'https://huggingface.co'</span>:<span class=\"w\"> </span>gpjtB/s\nPassword<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"s1\">'https://gpjt@huggingface.co'</span>:\nEOFoading<span class=\"w\"> </span>LFS<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">2</span>/2<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"m\">21</span><span class=\"w\"> </span>GB<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span>B/s\nerror:<span class=\"w\"> </span>failed<span class=\"w\"> </span>to<span class=\"w\"> </span>push<span class=\"w\"> </span>some<span class=\"w\"> </span>refs<span class=\"w\"> </span>to<span class=\"w\"> </span><span class=\"s1\">'https://huggingface.co/datasets/gpjt/fineweb-gpt2-tokens'</span>\n</code></pre>\n</div>\n\n<p>Weird that it prompted for the credentials twice, but it did appear to try to do\nsomething there -- but obviously it didn't work.</p>\n\n<p>Let's see if Git over SSH is any better.</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span><span class=\"nb\">cd</span><span class=\"w\"> </span>..\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>rm<span class=\"w\"> </span>-rf<span class=\"w\"> </span>fineweb-gpt2-tokens/\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>git<span class=\"w\"> </span>clone<span class=\"w\"> </span>git@hf.co:datasets/gpjt/fineweb-gpt2-tokens\nCloning<span class=\"w\"> </span>into<span class=\"w\"> </span><span class=\"s1\">'fineweb-gpt2-tokens'</span>...\n**<span class=\"w\"> </span>WARNING:<span class=\"w\"> </span>connection<span class=\"w\"> </span>is<span class=\"w\"> </span>not<span class=\"w\"> </span>using<span class=\"w\"> </span>a<span class=\"w\"> </span>post-quantum<span class=\"w\"> </span>key<span class=\"w\"> </span>exchange<span class=\"w\"> </span>algorithm.\n**<span class=\"w\"> </span>This<span class=\"w\"> </span>session<span class=\"w\"> </span>may<span class=\"w\"> </span>be<span class=\"w\"> </span>vulnerable<span class=\"w\"> </span>to<span class=\"w\"> </span><span class=\"s2\">&quot;store now, decrypt later&quot;</span><span class=\"w\"> </span>attacks.\n**<span class=\"w\"> </span>The<span class=\"w\"> </span>server<span class=\"w\"> </span>may<span class=\"w\"> </span>need<span class=\"w\"> </span>to<span class=\"w\"> </span>be<span class=\"w\"> </span>upgraded.<span class=\"w\"> </span>See<span class=\"w\"> </span>https://openssh.com/pq.html\nremote:<span class=\"w\"> </span>Enumerating<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">3</span>,<span class=\"w\"> </span><span class=\"k\">done</span>.\nremote:<span class=\"w\"> </span>Total<span class=\"w\"> </span><span class=\"m\">3</span><span class=\"w\"> </span><span class=\"o\">(</span>delta<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"o\">)</span>,<span class=\"w\"> </span>reused<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"o\">(</span>delta<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"o\">)</span>,<span class=\"w\"> </span>pack-reused<span class=\"w\"> </span><span class=\"m\">3</span><span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">)</span>\nReceiving<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">3</span>/3<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"k\">done</span>.\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span><span class=\"nb\">cd</span><span class=\"w\"> </span>fineweb-gpt2-tokens\n</code></pre>\n</div>\n\n<p>...then the same stuff to copy in the files and create the metadata file, then:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uvx<span class=\"w\"> </span>hf<span class=\"w\"> </span>lfs-enable-largefiles<span class=\"w\"> </span>.\nLocal<span class=\"w\"> </span>repo<span class=\"w\"> </span><span class=\"nb\">set</span><span class=\"w\"> </span>up<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>largefiles\ngiles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>git<span class=\"w\"> </span>add<span class=\"w\"> </span>.\ngiles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>git<span class=\"w\"> </span>commit<span class=\"w\"> </span>-am<span class=\"s2\">&quot;First cut of code to prepare datasets&quot;</span>\n<span class=\"o\">[</span>main<span class=\"w\"> </span>44df15c<span class=\"o\">]</span><span class=\"w\"> </span>First<span class=\"w\"> </span>cut<span class=\"w\"> </span>of<span class=\"w\"> </span>code<span class=\"w\"> </span>to<span class=\"w\"> </span>prepare<span class=\"w\"> </span>datasets\n<span class=\"w\"> </span><span class=\"m\">3</span><span class=\"w\"> </span>files<span class=\"w\"> </span>changed,<span class=\"w\"> </span><span class=\"m\">14</span><span class=\"w\"> </span>insertions<span class=\"o\">(</span>+<span class=\"o\">)</span>\n<span class=\"w\"> </span>create<span class=\"w\"> </span>mode<span class=\"w\"> </span><span class=\"m\">100644</span><span class=\"w\"> </span>meta.json\n<span class=\"w\"> </span>create<span class=\"w\"> </span>mode<span class=\"w\"> </span><span class=\"m\">100644</span><span class=\"w\"> </span>train.safetensors\n<span class=\"w\"> </span>create<span class=\"w\"> </span>mode<span class=\"w\"> </span><span class=\"m\">100644</span><span class=\"w\"> </span>validation.safetensors\ngiles@perry:~/Dev/ddp-base-model-from-scratch/fineweb-gpt2-tokens<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>git<span class=\"w\"> </span>push\n**<span class=\"w\"> </span>WARNING:<span class=\"w\"> </span>connection<span class=\"w\"> </span>is<span class=\"w\"> </span>not<span class=\"w\"> </span>using<span class=\"w\"> </span>a<span class=\"w\"> </span>post-quantum<span class=\"w\"> </span>key<span class=\"w\"> </span>exchange<span class=\"w\"> </span>algorithm.\n**<span class=\"w\"> </span>This<span class=\"w\"> </span>session<span class=\"w\"> </span>may<span class=\"w\"> </span>be<span class=\"w\"> </span>vulnerable<span class=\"w\"> </span>to<span class=\"w\"> </span><span class=\"s2\">&quot;store now, decrypt later&quot;</span><span class=\"w\"> </span>attacks.\n**<span class=\"w\"> </span>The<span class=\"w\"> </span>server<span class=\"w\"> </span>may<span class=\"w\"> </span>need<span class=\"w\"> </span>to<span class=\"w\"> </span>be<span class=\"w\"> </span>upgraded.<span class=\"w\"> </span>See<span class=\"w\"> </span>https://openssh.com/pq.html\nEOFoading<span class=\"w\"> </span>LFS<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">2</span>/2<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"m\">21</span><span class=\"w\"> </span>GB<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span>B/s\nerror:<span class=\"w\"> </span>failed<span class=\"w\"> </span>to<span class=\"w\"> </span>push<span class=\"w\"> </span>some<span class=\"w\"> </span>refs<span class=\"w\"> </span>to<span class=\"w\"> </span><span class=\"s1\">'hf.co:datasets/gpjt/fineweb-gpt2-tokens'</span>\n</code></pre>\n</div>\n\n<p>Looks like the same error.  Odd.</p>\n\n<p>Let's try using HF's upload tools rather than Git -- feels like a bit of a cop-out,\nbut maybe it'll work better.</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uvx<span class=\"w\"> </span>hf<span class=\"w\"> </span>upload<span class=\"w\"> </span>gpjt/fineweb-gpt2-tokens<span class=\"w\"> </span>./fineweb-prepared/train.safetensors<span class=\"w\"> </span>train.safetensors<span class=\"w\"> </span>--repo-type<span class=\"w\"> </span>dataset\nProcessing<span class=\"w\"> </span>Files<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">1</span><span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">)</span><span class=\"w\">      </span>:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">20</span>.5GB<span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">20</span>.5GB,<span class=\"w\"> </span><span class=\"m\">2</span>.76MB/s\nNew<span class=\"w\"> </span>Data<span class=\"w\"> </span>Upload<span class=\"w\">               </span>:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">2</span>.95GB<span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">2</span>.95GB,<span class=\"w\"> </span><span class=\"m\">2</span>.76MB/s\n<span class=\"w\">  </span>...repared/train.safetensors:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">20</span>.5GB<span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">20</span>.5GB\nhttps://huggingface.co/datasets/gpjt/fineweb-gpt2-tokens/commit/69085f941ba3e8f0750929a1f8cd451fba761bff\n</code></pre>\n</div>\n\n<p>That did indeed take about 200 seconds to run, but the upload speed was only about\n10 MiB/s -- from the output, I think it must have been compressing it.  Anyway, it looks like\nit succeeded, so let's upload the others!</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uvx<span class=\"w\"> </span>hf<span class=\"w\"> </span>upload<span class=\"w\"> </span>gpjt/fineweb-gpt2-tokens<span class=\"w\"> </span>./fineweb-prepared/validation.safetensors<span class=\"w\"> </span>validation.safetensors<span class=\"w\"> </span>--repo-type<span class=\"w\"> </span>dataset\nProcessing<span class=\"w\"> </span>Files<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">1</span><span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">)</span><span class=\"w\">      </span>:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\">  </span>205MB<span class=\"w\"> </span>/<span class=\"w\">  </span>205MB,<span class=\"w\"> </span><span class=\"m\">78</span>.7MB/s\nNew<span class=\"w\"> </span>Data<span class=\"w\"> </span>Upload<span class=\"w\">               </span>:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\">  </span>235kB<span class=\"w\"> </span>/<span class=\"w\">  </span>235kB,<span class=\"w\"> </span><span class=\"m\">90</span>.6kB/s\n<span class=\"w\">  </span>...ed/validation.safetensors:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\">  </span>205MB<span class=\"w\"> </span>/<span class=\"w\">  </span>205MB\nhttps://huggingface.co/datasets/gpjt/fineweb-gpt2-tokens/commit/885777d5211383cc7990004f99a8823fad53be66\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uvx<span class=\"w\"> </span>hf<span class=\"w\"> </span>upload<span class=\"w\"> </span>gpjt/fineweb-edu-gpt2-tokens<span class=\"w\"> </span>./fineweb-edu-prepared/train.safetensors<span class=\"w\"> </span>train.safetensors<span class=\"w\"> </span>--repo-type<span class=\"w\"> </span>dataset\nProcessing<span class=\"w\"> </span>Files<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">1</span><span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">)</span><span class=\"w\">      </span>:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">19</span>.7GB<span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">19</span>.7GB,<span class=\"w\"> </span><span class=\"m\">4</span>.53MB/s\nNew<span class=\"w\"> </span>Data<span class=\"w\"> </span>Upload<span class=\"w\">               </span>:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3</span>.09GB<span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">3</span>.09GB,<span class=\"w\"> </span><span class=\"m\">4</span>.53MB/s\n<span class=\"w\">  </span>...repared/train.safetensors:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">19</span>.7GB<span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">19</span>.7GB\nhttps://huggingface.co/datasets/gpjt/fineweb-edu-gpt2-tokens/commit/55baacd6812ac085df0c91ea573c8ccd89015341\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uvx<span class=\"w\"> </span>hf<span class=\"w\"> </span>upload<span class=\"w\"> </span>gpjt/fineweb-edu-gpt2-tokens<span class=\"w\"> </span>./fineweb-edu-prepared/validation.safetensors<span class=\"w\"> </span>validation.safetensors<span class=\"w\"> </span>--repo-type<span class=\"w\"> </span>dataset\nProcessing<span class=\"w\"> </span>Files<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">1</span><span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">)</span><span class=\"w\">      </span>:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\">  </span>201MB<span class=\"w\"> </span>/<span class=\"w\">  </span>201MB,<span class=\"w\"> </span><span class=\"m\">62</span>.8MB/s\nNew<span class=\"w\"> </span>Data<span class=\"w\"> </span>Upload<span class=\"w\">               </span>:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\">  </span>104kB<span class=\"w\"> </span>/<span class=\"w\">  </span>104kB,<span class=\"w\"> </span><span class=\"m\">32</span>.5kB/s\n<span class=\"w\">  </span>...ed/validation.safetensors:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\">  </span>201MB<span class=\"w\"> </span>/<span class=\"w\">  </span>201MB\nhttps://huggingface.co/datasets/gpjt/fineweb-edu-gpt2-tokens/commit/8bc548d681476ecc79444779746d6dc1a852cca2\n</code></pre>\n</div>\n\n<p>...and that's done :-)</p>\n\n<p>Next, a bit of manual editing of the dataset cards on the Hugging Face website,\nand we have our two new public datasets:</p>\n\n<ul>\n<li><a href=\"https://huggingface.co/datasets/gpjt/fineweb-gpt2-tokens\"><code>gpjt/fineweb-gpt2-tokens</code></a></li>\n<li><a href=\"https://huggingface.co/datasets/gpjt/fineweb-edu-gpt2-tokens\"><code>gpjt/fineweb-edu-gpt2-tokens</code></a></li>\n</ul>\n\n<p>That looks solid.  So, the next thing: change our codebase so that we have some\nquick and easy way to download them (I'm feeling a little wary of using Git for\nthat after the upload issue), and then to use the downloaded files in our training\ncode.</p>\n\n<h3 id=\"downloading-the-datasets-from-hugging-face\">Downloading the datasets from Hugging Face</h3>\n\n<p>We already have the code to download a dataset; the stuff that I wrote to\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/aede6fbca2bf5ca518995ef39e58103e4c2dae15/download-fineweb-10b.py\">download FineWeb and FineWeb-Edu</a>\noriginally.  Here's the important bit:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">huggingface_hub</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">snapshot_download</span>\n\n<span class=\"o\">...</span>\n\n    <span class=\"n\">folder</span> <span class=\"o\">=</span> <span class=\"n\">snapshot_download</span><span class=\"p\">(</span>\n        <span class=\"sa\">f</span><span class=\"s2\">&quot;HuggingFaceFW/</span><span class=\"si\">{</span><span class=\"n\">name</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">repo_type</span><span class=\"o\">=</span><span class=\"s2\">&quot;dataset&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">local_dir</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">&quot;./</span><span class=\"si\">{</span><span class=\"n\">name</span><span class=\"si\">}</span><span class=\"s2\">/&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">allow_patterns</span><span class=\"o\">=</span><span class=\"s2\">&quot;sample/10BT/*&quot;</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...so we can adapt that to download all files in an arbitrary dataset:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">download_dataset</span><span class=\"p\">(</span><span class=\"n\">datasets_dir</span><span class=\"p\">,</span> <span class=\"n\">dataset_name</span><span class=\"p\">):</span>\n    <span class=\"n\">download_path</span> <span class=\"o\">=</span> <span class=\"n\">snapshot_download</span><span class=\"p\">(</span>\n        <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">dataset_name</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">repo_type</span><span class=\"o\">=</span><span class=\"s2\">&quot;dataset&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">local_dir</span><span class=\"o\">=</span><span class=\"n\">datasets_dir</span> <span class=\"o\">/</span> <span class=\"n\">dataset_name</span><span class=\"p\">,</span>\n        <span class=\"n\">allow_patterns</span><span class=\"o\">=</span><span class=\"s2\">&quot;*&quot;</span>\n    <span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">Path</span><span class=\"p\">(</span><span class=\"n\">download_path</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...and call that from our <code>main</code>, using a new command-line argument <code>datasets_dir_path</code>,\nand a new <code>dataset</code> element in our train config JSON file:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">datasets_dir</span> <span class=\"o\">=</span> <span class=\"n\">Path</span><span class=\"p\">(</span><span class=\"n\">datasets_dir_path</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">datasets_dir</span><span class=\"o\">.</span><span class=\"n\">is_dir</span><span class=\"p\">():</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">Exception</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">datasets_dir_path</span><span class=\"si\">}</span><span class=\"s2\"> is not a directory&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">dataset_dir</span> <span class=\"o\">=</span> <span class=\"n\">download_dataset</span><span class=\"p\">(</span><span class=\"n\">datasets_dir</span><span class=\"p\">,</span> <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;dataset&quot;</span><span class=\"p\">])</span>\n</code></pre>\n</div>\n\n<p>I was thinking that we'd need extra guard code to not download the dataset again\nif it's already there, but it looks like <code>snapshot_download</code> handles that all nicely for us.</p>\n\n<p>So we have a way to specify which dataset we should use for a training run, and\ncode to download it.  Now we just need to adjust the code that loads our datasets\nso that instead of looking in the <code>run_dir</code>, it looks in the directory returned\nby <code>download_dataset</code>:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train_ds</span> <span class=\"o\">=</span> <span class=\"n\">load_dataset</span><span class=\"p\">(</span>\n        <span class=\"n\">dataset_dir</span><span class=\"p\">,</span> <span class=\"s2\">&quot;train&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">model_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;context_length&quot;</span><span class=\"p\">],</span> <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;microbatch_size&quot;</span><span class=\"p\">]</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">val_ds</span> <span class=\"o\">=</span> <span class=\"n\">load_dataset</span><span class=\"p\">(</span>\n        <span class=\"n\">dataset_dir</span><span class=\"p\">,</span> <span class=\"s2\">&quot;validation&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">model_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;context_length&quot;</span><span class=\"p\">],</span> <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;microbatch_size&quot;</span><span class=\"p\">]</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...and update the <code>load_dataset</code> directory so that if just blindly uses the\ndirectory provided rather than trying to look in a <code>datasets</code> subdirectory:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">load_dataset</span><span class=\"p\">(</span><span class=\"n\">dataset_dir</span><span class=\"p\">,</span> <span class=\"n\">split</span><span class=\"p\">,</span> <span class=\"n\">seq_length</span><span class=\"p\">,</span> <span class=\"n\">microbatch_size</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">BigTrainDataset</span><span class=\"p\">(</span>\n        <span class=\"n\">load_file</span><span class=\"p\">(</span><span class=\"n\">dataset_dir</span> <span class=\"o\">/</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">split</span><span class=\"si\">}</span><span class=\"s2\">.safetensors&quot;</span><span class=\"p\">)[</span><span class=\"s2\">&quot;tokens&quot;</span><span class=\"p\">],</span>\n        <span class=\"n\">seq_length</span><span class=\"p\">,</span> <span class=\"n\">microbatch_size</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>That all works!  We successfully download the datasets and try to use them.\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/441e3564e2dabc9e06dbc7d1783d086312a11960/ddp_train.py\">Here's the code</a>.</p>\n\n<p>But now we have a problem; when the <code>BigTrainDataset</code> tries to\nreshape the huge tensor that we have as our inputs:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">xs</span> <span class=\"o\">=</span> <span class=\"n\">all_tokens</span><span class=\"p\">[:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">microbatch_size</span><span class=\"p\">,</span> <span class=\"n\">seq_length</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...it craps out:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">RuntimeError: shape '[-1, 6, 1024]' is invalid for input of size 10248871836</span>\n</code></pre>\n</div>\n\n<p>That makes perfect sense.  Our original <code>safetensors</code> files were carefully sized\nfor a batch size of six, and 1024-token sequences.  We need some way to work out\nan appropriate slice of both the training and the validation data.</p>\n\n<h3 id=\"slicing-the-datasets\">Slicing the datasets</h3>\n\n<p>Most of the trains are likely to be Chinchilla-optimal, or at least use a Chinchilla-optimal\nnumber of tokens -- rounded up appropriately to match our micro-batch size, sequence\nlength, and world size.</p>\n\n<p>But I'd like it to be more configurable.  What I'll do is add a <code>min_train_tokens</code>\nkey to the training config dictionary, along with a <code>start_train_token</code> so that we\ncan (for example) train on the first Chinchilla-optimal tokens, then do an\nextended train continuing on from there.  The idea is that we can use\n<code>min_train_tokens</code> as a base, and train on the smallest number of full batches that contains\nat least that many tokens.</p>\n\n<p>For validation, I think that the <code>validation_batches</code> key that we already have is\nactually quite nice.  Validation is time-bound, and the number of batches is the\neasiest lever to pull to handle that. However, a <code>start_val_token</code> would be\nnice for symmetry.</p>\n\n<p>So, here are some numbers for debugging:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;microbatch_size&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">6</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;validation_interval&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;dataset&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;gpjt/fineweb-gpt2-tokens&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;min_train_tokens&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">3260190720</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;start_train_token&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;validation_batches&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;start_val_token&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<p>Now let's use them.  Initially, we have this to load the train dataset:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train_ds</span> <span class=\"o\">=</span> <span class=\"n\">load_dataset</span><span class=\"p\">(</span>\n        <span class=\"n\">dataset_dir</span><span class=\"p\">,</span> <span class=\"s2\">&quot;train&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">model_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;context_length&quot;</span><span class=\"p\">],</span> <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;microbatch_size&quot;</span><span class=\"p\">]</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Let's work through that one first then make appropriate changes to the validation\none.  The pieces of information we need to work out which tokens to use are:</p>\n\n<ul>\n<li>The <code>min_train_tokens</code></li>\n<li>The <code>start_train_token</code></li>\n<li>The world size -- that is, how many per-GPU processes are we running?</li>\n<li>The micro-batch size</li>\n<li>The sequence length</li>\n</ul>\n\n<p>Let's update our <code>load_dataset</code> function so that it takes those parameters in that\norder:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">train_ds</span> <span class=\"o\">=</span> <span class=\"n\">load_dataset</span><span class=\"p\">(</span>\n        <span class=\"n\">dataset_dir</span><span class=\"p\">,</span> <span class=\"s2\">&quot;train&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;min_train_tokens&quot;</span><span class=\"p\">],</span> <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;start_train_token&quot;</span><span class=\"p\">],</span>\n        <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">get_world_size</span><span class=\"p\">(),</span> <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;microbatch_size&quot;</span><span class=\"p\">],</span>\n        <span class=\"n\">model_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;context_length&quot;</span><span class=\"p\">]</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...and now we can write an updated <code>load_dataset</code> that uses those numbers to get\nthe right number of tokens:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">load_dataset</span><span class=\"p\">(</span>\n    <span class=\"n\">dataset_dir</span><span class=\"p\">,</span> <span class=\"n\">split</span><span class=\"p\">,</span>\n    <span class=\"n\">min_tokens</span><span class=\"p\">,</span> <span class=\"n\">start_token</span><span class=\"p\">,</span>\n    <span class=\"n\">world_size</span><span class=\"p\">,</span> <span class=\"n\">microbatch_size</span><span class=\"p\">,</span>\n    <span class=\"n\">seq_length</span>\n<span class=\"p\">):</span>\n    <span class=\"n\">full_dataset</span> <span class=\"o\">=</span> <span class=\"n\">load_file</span><span class=\"p\">(</span><span class=\"n\">dataset_dir</span> <span class=\"o\">/</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">split</span><span class=\"si\">}</span><span class=\"s2\">.safetensors&quot;</span><span class=\"p\">)[</span><span class=\"s2\">&quot;tokens&quot;</span><span class=\"p\">]</span>\n\n    <span class=\"n\">one_full_batch_tokens</span> <span class=\"o\">=</span> <span class=\"n\">world_size</span> <span class=\"o\">*</span> <span class=\"n\">microbatch_size</span> <span class=\"o\">*</span> <span class=\"n\">seq_length</span>\n    <span class=\"n\">batches_for_just_over_min</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">min_tokens</span> <span class=\"o\">//</span> <span class=\"n\">one_full_batch_tokens</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n\n    <span class=\"c1\"># Note that we need one extra token for our Ys.</span>\n    <span class=\"n\">tokens_needed</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">batches_for_just_over_min</span> <span class=\"o\">*</span> <span class=\"n\">one_full_batch_tokens</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n\n    <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_dataset</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"n\">start_token</span> <span class=\"o\">+</span> <span class=\"n\">tokens_needed</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">Exception</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Not enough tokens (wanted </span><span class=\"si\">{</span><span class=\"n\">start_token</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">tokens_needed</span><span class=\"si\">}</span><span class=\"s2\">, got </span><span class=\"si\">{</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_dataset</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"s2\">)&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">BigTrainDataset</span><span class=\"p\">(</span>\n        <span class=\"n\">full_dataset</span><span class=\"p\">[</span><span class=\"n\">start_token</span><span class=\"p\">:</span><span class=\"n\">start_token</span> <span class=\"o\">+</span> <span class=\"n\">tokens_needed</span><span class=\"p\">],</span>\n        <span class=\"n\">seq_length</span><span class=\"p\">,</span> <span class=\"n\">microbatch_size</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Validation is less obvious; I think that the best way to do this (given that\nthe validation dataset is small) is just to have a \"magic\" <code>-1</code> value for\n<code>min_tokens</code>, which means \"just get a round number of full batches starting at\n<code>start_val_token</code>.  It's also worth remembering that we only do evals on the\nrank 0 process, so we could in theory pass in a world size of 1 -- but I think that\npassing in the real world size might be a good idea, because it gives us one fewer\nthing to change if, in the future, we move towards distributed evals.</p>\n\n<p>So:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">val_ds</span> <span class=\"o\">=</span> <span class=\"n\">load_dataset</span><span class=\"p\">(</span>\n        <span class=\"n\">dataset_dir</span><span class=\"p\">,</span> <span class=\"s2\">&quot;validation&quot;</span><span class=\"p\">,</span>\n        <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;start_val_token&quot;</span><span class=\"p\">],</span>\n        <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">get_world_size</span><span class=\"p\">(),</span> <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;microbatch_size&quot;</span><span class=\"p\">],</span>\n        <span class=\"n\">model_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;context_length&quot;</span><span class=\"p\">]</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...and we change <code>load_dataset</code> to be able to handle the magic <code>-1</code>:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">load_dataset</span><span class=\"p\">(</span>\n    <span class=\"n\">dataset_dir</span><span class=\"p\">,</span> <span class=\"n\">split</span><span class=\"p\">,</span>\n    <span class=\"n\">min_tokens</span><span class=\"p\">,</span> <span class=\"n\">start_token</span><span class=\"p\">,</span>\n    <span class=\"n\">world_size</span><span class=\"p\">,</span> <span class=\"n\">microbatch_size</span><span class=\"p\">,</span>\n    <span class=\"n\">seq_length</span>\n<span class=\"p\">):</span>\n    <span class=\"n\">full_dataset</span> <span class=\"o\">=</span> <span class=\"n\">load_file</span><span class=\"p\">(</span><span class=\"n\">dataset_dir</span> <span class=\"o\">/</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">split</span><span class=\"si\">}</span><span class=\"s2\">.safetensors&quot;</span><span class=\"p\">)[</span><span class=\"s2\">&quot;tokens&quot;</span><span class=\"p\">]</span>\n    <span class=\"k\">if</span> <span class=\"n\">start_token</span> <span class=\"o\">&gt;</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_dataset</span><span class=\"p\">):</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">Exception</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;start_token </span><span class=\"si\">{</span><span class=\"n\">start_token</span><span class=\"si\">}</span><span class=\"s2\"> is past the end of the dataset&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"n\">one_full_batch_tokens</span> <span class=\"o\">=</span> <span class=\"n\">world_size</span> <span class=\"o\">*</span> <span class=\"n\">microbatch_size</span> <span class=\"o\">*</span> <span class=\"n\">seq_length</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">min_tokens</span> <span class=\"o\">==</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">:</span>\n        <span class=\"n\">available_tokens</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_dataset</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">start_token</span>\n        <span class=\"n\">available_batches</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">available_tokens</span> <span class=\"o\">//</span> <span class=\"n\">one_full_batch_tokens</span><span class=\"p\">)</span>\n        <span class=\"n\">tokens_needed</span> <span class=\"o\">=</span> <span class=\"n\">available_batches</span> <span class=\"o\">*</span> <span class=\"n\">one_full_batch_tokens</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">batches_for_just_over_min</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">min_tokens</span> <span class=\"o\">//</span> <span class=\"n\">one_full_batch_tokens</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n        <span class=\"n\">tokens_needed</span> <span class=\"o\">=</span> <span class=\"n\">batches_for_just_over_min</span> <span class=\"o\">*</span> <span class=\"n\">one_full_batch_tokens</span>\n\n    <span class=\"c1\"># Note that we need one extra token for our Ys.</span>\n    <span class=\"n\">tokens_needed</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n\n    <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_dataset</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"n\">start_token</span> <span class=\"o\">+</span> <span class=\"n\">tokens_needed</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">Exception</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Not enough tokens (wanted </span><span class=\"si\">{</span><span class=\"n\">start_token</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">tokens_needed</span><span class=\"si\">}</span><span class=\"s2\">, got </span><span class=\"si\">{</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">full_dataset</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"s2\">)&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">BigTrainDataset</span><span class=\"p\">(</span>\n        <span class=\"n\">full_dataset</span><span class=\"p\">[</span><span class=\"n\">start_token</span><span class=\"p\">:</span><span class=\"n\">start_token</span> <span class=\"o\">+</span> <span class=\"n\">tokens_needed</span><span class=\"p\">],</span>\n        <span class=\"n\">seq_length</span><span class=\"p\">,</span> <span class=\"n\">microbatch_size</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>I also added in a quick sanity check to make sure that we don't get weird behaviour\nif the <code>start_token</code> is past the end of the original dataset.</p>\n\n<p>That all looks good!  Running it kicks off training, and validation is running\nhappily every ten global steps, but just with three samples, as configured in the\nJSON file.  <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/36e475bcae5ef36757ddd3cff4da8692d2434213/ddp_train.py\">Here's the code</a>.</p>\n\n<h3 id=\"qol-features\">QoL features</h3>\n\n<p>One thing that hasn't shown up while running this code locally is that our training loop\nhas this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">for</span> <span class=\"n\">global_step</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">start_global_step</span><span class=\"p\">,</span> <span class=\"n\">total_global_steps</span><span class=\"p\">)):</span>\n</code></pre>\n</div>\n\n<p>With one GPU, that's fine, but on a multi-GPU machine, that <code>tqdm</code> is going to happen in all of our per-GPU processes\n-- so they'll all be spamming out progress bars, which will be ugly.  So, as a first cut:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">for</span> <span class=\"n\">global_step</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">start_global_step</span><span class=\"p\">,</span> <span class=\"n\">total_global_steps</span><span class=\"p\">),</span> <span class=\"n\">disable</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">rank</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">)):</span>\n</code></pre>\n</div>\n\n<p>Now, in order to compare different machines (say, an 8x H100 vs an 8x A100) it would\nbe nice to get tokens-per-second numbers while training.  We can do that in the <code>tqdm</code>\nprogress bar too!   It has a <code>set_postfix</code> method that adds stuff to the end of the\nbar, just after the elapsed time and iterations/second numbers.  For that, we'll need\nto have the <code>tqdm</code> object available in a variable:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">progress_bar</span> <span class=\"o\">=</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span>\n        <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">start_global_step</span><span class=\"p\">,</span> <span class=\"n\">total_global_steps</span><span class=\"p\">),</span>\n        <span class=\"n\">disable</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">rank</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">global_step</span> <span class=\"ow\">in</span> <span class=\"n\">progress_bar</span><span class=\"p\">:</span>\n</code></pre>\n</div>\n\n<p>...and now we can count the total tokens seen in the training run, plus keep track\nof the start time -- just before the start of the training loop:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">start_time</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n    <span class=\"n\">tokens_seen_this_rank</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n</code></pre>\n</div>\n\n<p>...then inside, after the training step:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>        <span class=\"n\">microbatch_size</span><span class=\"p\">,</span> <span class=\"n\">sequence_length</span> <span class=\"o\">=</span> <span class=\"n\">inputs</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n        <span class=\"n\">tokens_seen_this_rank</span> <span class=\"o\">+=</span> <span class=\"n\">microbatch_size</span> <span class=\"o\">*</span> <span class=\"n\">sequence_length</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">elapsed_time</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">start_time</span>\n            <span class=\"n\">tokens_per_sec</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">tokens_seen_this_rank</span> <span class=\"o\">*</span> <span class=\"n\">world_size</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">elapsed_time</span>\n            <span class=\"n\">progress_bar</span><span class=\"o\">.</span><span class=\"n\">set_postfix</span><span class=\"p\">(</span>\n                <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">train_loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span><span class=\"si\">:</span><span class=\"s2\">.3f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">,</span>\n                <span class=\"n\">tps</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">tokens_per_sec</span><span class=\"si\">:</span><span class=\"s2\">,.0f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span>\n            <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>That will give us a running average of tokens per second over the train as a whole since\nthe start.</p>\n\n<p>Running that, we get a nice progress bar like this (you'll need to scroll to the right):</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">  0%|                                                                                                      | 10/530631 [00:04&lt;47:31:23,  3.10it/s, loss=8.094, tps=14,362]</span>\n</code></pre>\n</div>\n\n<p>Note that the tokens per second is worse than the just less than 20k that we got when running\nthe single-GPU test previously,\nbut that's due to the testing setup I have -- I'm doing an eval every 10 global steps.\nChanging that to 1,000,000 so that we just get a single eval when we start, then\nletting it run for a while to settle down from the initial eval, we get this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">  0%|                                                                                                     | 631/530631 [03:17&lt;46:02:23,  3.20it/s, loss=6.890, tps=19,678]</span>\n</code></pre>\n</div>\n\n<p>...which is close enough to what we had before.</p>\n\n<p>Finally, let's print out some summary information at the end:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">end_time</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n    <span class=\"n\">elapsed_time</span> <span class=\"o\">=</span> <span class=\"n\">end_time</span> <span class=\"o\">-</span> <span class=\"n\">start_time</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"se\">\\n\\n\\n</span><span class=\"s2\">Training complete in </span><span class=\"si\">{</span><span class=\"n\">elapsed_time</span><span class=\"si\">:</span><span class=\"s2\">,.3f</span><span class=\"si\">}</span><span class=\"s2\"> seconds&quot;</span><span class=\"p\">)</span>\n        <span class=\"n\">total_tokens_seen</span> <span class=\"o\">=</span> <span class=\"n\">tokens_seen_this_rank</span> <span class=\"o\">*</span> <span class=\"n\">world_size</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Tokens seen: </span><span class=\"si\">{</span><span class=\"n\">total_tokens_seen</span><span class=\"si\">:</span><span class=\"s2\">,.0f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Throughput: </span><span class=\"si\">{</span><span class=\"n\">total_tokens_seen</span><span class=\"w\"> </span><span class=\"o\">/</span><span class=\"w\"> </span><span class=\"n\">elapsed_time</span><span class=\"si\">:</span><span class=\"s2\">,.0f</span><span class=\"si\">}</span><span class=\"s2\"> tokens/second&quot;</span><span class=\"p\">)</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Final train loss: </span><span class=\"si\">{</span><span class=\"n\">avg_train_loss</span><span class=\"si\">:</span><span class=\"s2\">.3f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Final val loss: </span><span class=\"si\">{</span><span class=\"n\">val_loss</span><span class=\"si\">:</span><span class=\"s2\">.3f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Ran that on a super-short train with about 50 iterations-worth of tokens, and:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 26.520 seconds</span>\n<span class=\"go\">Tokens seen: 331,776</span>\n<span class=\"go\">Throughput: 12,510 tokens/second</span>\n<span class=\"go\">Final train loss: 7.817</span>\n<span class=\"go\">Final val loss: 8.093</span>\n</code></pre>\n</div>\n\n<p>Looking good.  <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/d6c0baaf7081828614beda68d47620084d99b805/ddp_train.py\">Here's the code</a>.</p>\n\n<p>I think we now have something where it's worth spinning up a Lambda Labs\nmachine to run it.</p>\n\n<h3 id=\"a-first-run-on-lambda-labs\">A first run on Lambda Labs</h3>\n\n<p>Let's kick off a training run on the cheapest two-GPU machine that they have\navailable right now.  That's actually not all that cheap, it's a $6.38/hour\n2x H100 80 GiB SXM5.  But I'm not planning to do a full train on it yet, this\nis just a sanity test.</p>\n\n<p>I won't attach a filesystem this time, either -- let's see how things go without\nthe caching of the datasets that I was considering.</p>\n\n<p>First thing: do we have <code>uv</code>?</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@192-222-53-181:~$<span class=\"w\"> </span>uv\nuv:<span class=\"w\"> </span><span class=\"nb\">command</span><span class=\"w\"> </span>not<span class=\"w\"> </span>found\n</code></pre>\n</div>\n\n<p>Nope.  OK, let's install it:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@192-222-53-181:~$<span class=\"w\"> </span>curl<span class=\"w\"> </span>-LsSf<span class=\"w\"> </span>https://astral.sh/uv/install.sh<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>sh\ndownloading<span class=\"w\"> </span>uv<span class=\"w\"> </span><span class=\"m\">0</span>.9.15<span class=\"w\"> </span>x86_64-unknown-linux-gnu\nno<span class=\"w\"> </span>checksums<span class=\"w\"> </span>to<span class=\"w\"> </span>verify\ninstalling<span class=\"w\"> </span>to<span class=\"w\"> </span>/home/ubuntu/.local/bin\n<span class=\"w\">  </span>uv\n<span class=\"w\">  </span>uvx\neverything<span class=\"err\">'</span>s<span class=\"w\"> </span>installed!\n</code></pre>\n</div>\n\n<p>Right, now let's clone our repo and set up our environment:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@192-222-53-181:~$<span class=\"w\"> </span>git<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://github.com/gpjt/ddp-base-model-from-scratch.git\nCloning<span class=\"w\"> </span>into<span class=\"w\"> </span><span class=\"s1\">'ddp-base-model-from-scratch'</span>...\nremote:<span class=\"w\"> </span>Enumerating<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">123</span>,<span class=\"w\"> </span><span class=\"k\">done</span>.\nremote:<span class=\"w\"> </span>Counting<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">123</span>/123<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"k\">done</span>.\nremote:<span class=\"w\"> </span>Compressing<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">82</span>/82<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"k\">done</span>.\nremote:<span class=\"w\"> </span>Total<span class=\"w\"> </span><span class=\"m\">123</span><span class=\"w\"> </span><span class=\"o\">(</span>delta<span class=\"w\"> </span><span class=\"m\">66</span><span class=\"o\">)</span>,<span class=\"w\"> </span>reused<span class=\"w\"> </span><span class=\"m\">88</span><span class=\"w\"> </span><span class=\"o\">(</span>delta<span class=\"w\"> </span><span class=\"m\">34</span><span class=\"o\">)</span>,<span class=\"w\"> </span>pack-reused<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"o\">)</span>\nReceiving<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">123</span>/123<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"m\">124</span>.76<span class=\"w\"> </span>KiB<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3</span>.46<span class=\"w\"> </span>MiB/s,<span class=\"w\"> </span><span class=\"k\">done</span>.\nResolving<span class=\"w\"> </span>deltas:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">66</span>/66<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"k\">done</span>.\nubuntu@192-222-53-181:~$<span class=\"w\"> </span><span class=\"nb\">cd</span><span class=\"w\"> </span>ddp-base-model-from-scratch/\nubuntu@192-222-53-181:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>uv<span class=\"w\"> </span>sync\nUsing<span class=\"w\"> </span>CPython<span class=\"w\"> </span><span class=\"m\">3</span>.13.10\nCreating<span class=\"w\"> </span>virtual<span class=\"w\"> </span>environment<span class=\"w\"> </span>at:<span class=\"w\"> </span>.venv\nResolved<span class=\"w\"> </span><span class=\"m\">90</span><span class=\"w\"> </span>packages<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">0</span>.58ms\nPrepared<span class=\"w\"> </span><span class=\"m\">88</span><span class=\"w\"> </span>packages<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">34</span>.05s\nInstalled<span class=\"w\"> </span><span class=\"m\">88</span><span class=\"w\"> </span>packages<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>133ms\n...\n</code></pre>\n</div>\n\n<p>And now I think we can just try running it!</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@192-222-53-181:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>torchrun<span class=\"w\"> </span>ddp_train.py<span class=\"w\"> </span>original<span class=\"w\"> </span>datasets\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:18&lt;<span class=\"m\">00</span>:00,<span class=\"w\">  </span><span class=\"m\">4</span>.51s/it<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">20</span>.7GB<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:18,<span class=\"w\"> </span>724MB/s<span class=\"o\">]</span><span class=\"w\">                                                                                                                              </span>Starting<span class=\"w\"> </span>rank<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span>training<span class=\"w\"> </span>at<span class=\"w\"> </span>global<span class=\"w\"> </span>step<span class=\"w\"> </span><span class=\"m\">0</span>\n\n<span class=\"w\">  </span><span class=\"m\">0</span>%<span class=\"p\">|</span><span class=\"w\">                                                                                                                 </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/530631<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s,<span class=\"w\"> </span><span class=\"nv\">loss</span><span class=\"o\">=</span><span class=\"m\">10</span>.972,<span class=\"w\"> </span><span class=\"nv\">tps</span><span class=\"o\">=</span><span class=\"m\">12</span>,439<span class=\"o\">]</span>\n\nValidation/checkpoint\n<span class=\"m\">100</span>%<span class=\"p\">|</span>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">2</span>/2<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">33</span>.12it/s<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">20</span>.7GB<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:19,<span class=\"w\"> </span><span class=\"m\">1</span>.06GB/s<span class=\"o\">]</span>\nfindfont:<span class=\"w\"> </span>Font<span class=\"w\"> </span>family<span class=\"w\"> </span><span class=\"s1\">'xkcd'</span><span class=\"w\"> </span>not<span class=\"w\"> </span>found.<span class=\"w\">                                                                                                             </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/2<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>\nfindfont:<span class=\"w\"> </span>Font<span class=\"w\"> </span>family<span class=\"w\"> </span><span class=\"s1\">'xkcd'</span><span class=\"w\"> </span>not<span class=\"w\"> </span>found.\nfindfont:<span class=\"w\"> </span>Font<span class=\"w\"> </span>family<span class=\"w\"> </span><span class=\"s1\">'xkcd'</span><span class=\"w\"> </span>not<span class=\"w\"> </span>found.\n</code></pre>\n</div>\n\n<p>It took 18 seconds to download the dataset!  I don't think we need to worry about\nthe caching thing with persistent disks, at least at this point.</p>\n\n<p>But there are a couple of issues here.  I didn't put the number of processes in the command line\n-- I should be using</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">uv run torchrun --nproc_per_node=2 ddp_train.py original datasets</span>\n</code></pre>\n</div>\n\n<p>Also, we don't have the XKCD font family.  I'll ignore that for now.</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@192-222-53-181:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>torchrun<span class=\"w\"> </span>--nproc_per_node<span class=\"o\">=</span><span class=\"m\">2</span><span class=\"w\"> </span>ddp_train.py<span class=\"w\"> </span>original<span class=\"w\"> </span>datasets\nW1205<span class=\"w\"> </span><span class=\"m\">20</span>:16:58.548000<span class=\"w\"> </span><span class=\"m\">6491</span><span class=\"w\"> </span>torch/distributed/run.py:803<span class=\"o\">]</span>\nW1205<span class=\"w\"> </span><span class=\"m\">20</span>:16:58.548000<span class=\"w\"> </span><span class=\"m\">6491</span><span class=\"w\"> </span>torch/distributed/run.py:803<span class=\"o\">]</span><span class=\"w\"> </span>*****************************************\nW1205<span class=\"w\"> </span><span class=\"m\">20</span>:16:58.548000<span class=\"w\"> </span><span class=\"m\">6491</span><span class=\"w\"> </span>torch/distributed/run.py:803<span class=\"o\">]</span><span class=\"w\"> </span>Setting<span class=\"w\"> </span>OMP_NUM_THREADS<span class=\"w\"> </span>environment<span class=\"w\"> </span>variable<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>each<span class=\"w\"> </span>process<span class=\"w\"> </span>to<span class=\"w\"> </span>be<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>default,<span class=\"w\"> </span>to<span class=\"w\"> </span>avoid<span class=\"w\"> </span>your<span class=\"w\"> </span>system<span class=\"w\"> </span>being<span class=\"w\"> </span>overloaded,<span class=\"w\"> </span>please<span class=\"w\"> </span>further<span class=\"w\"> </span>tune<span class=\"w\"> </span>the<span class=\"w\"> </span>variable<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>optimal<span class=\"w\"> </span>performance<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>your<span class=\"w\"> </span>application<span class=\"w\"> </span>as<span class=\"w\"> </span>needed.\nW1205<span class=\"w\"> </span><span class=\"m\">20</span>:16:58.548000<span class=\"w\"> </span><span class=\"m\">6491</span><span class=\"w\"> </span>torch/distributed/run.py:803<span class=\"o\">]</span><span class=\"w\"> </span>*****************************************\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">8260</span>.57it/s<span class=\"o\">]</span>\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">19418</span>.07it/s<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">0</span>.00B<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00,<span class=\"w\"> </span>?B/s<span class=\"o\">]</span><span class=\"w\">              </span>Starting<span class=\"w\"> </span>rank<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span>training<span class=\"w\"> </span>at<span class=\"w\"> </span>global<span class=\"w\"> </span>step<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\">                                                     </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>\nStarting<span class=\"w\"> </span>rank<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>training<span class=\"w\"> </span>at<span class=\"w\"> </span>global<span class=\"w\"> </span>step<span class=\"w\"> </span><span class=\"m\">0</span>\n\n<span class=\"w\">  </span><span class=\"m\">0</span>%<span class=\"p\">|</span><span class=\"w\">                                                                                                                 </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/265316<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s,<span class=\"w\"> </span><span class=\"nv\">loss</span><span class=\"o\">=</span><span class=\"m\">10</span>.982,<span class=\"w\"> </span><span class=\"nv\">tps</span><span class=\"o\">=</span><span class=\"m\">26</span>,590<span class=\"o\">]</span>\n\nValidation/checkpoint\n<span class=\"m\">100</span>%<span class=\"p\">|</span>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">2</span>/2<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">33</span>.23it/s<span class=\"o\">]</span>\nfindfont:<span class=\"w\"> </span>Font<span class=\"w\"> </span>family<span class=\"w\"> </span><span class=\"s1\">'xkcd'</span><span class=\"w\"> </span>not<span class=\"w\"> </span>found.\n...\n\nContinuing<span class=\"w\"> </span>training\n\n<span class=\"w\">  </span><span class=\"m\">0</span>%<span class=\"p\">|</span><span class=\"w\">                                                                                                      </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">10</span>/265316<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:03&lt;<span class=\"m\">13</span>:29:38,<span class=\"w\">  </span><span class=\"m\">5</span>.46it/s,<span class=\"w\"> </span><span class=\"nv\">loss</span><span class=\"o\">=</span><span class=\"m\">8</span>.391,<span class=\"w\"> </span><span class=\"nv\">tps</span><span class=\"o\">=</span><span class=\"m\">35</span>,279<span class=\"o\">]</span>\n</code></pre>\n</div>\n\n<p>OK, that's looking good!  Let's make our validations happen less often, and see\nhow high we can get the micro-batches with the 80 GiB VRAM we have on each of our\ntwo GPUs.</p>\n\n<p>Doing a binary chop, I set the micro-batch size to 100 (OOM), then to 50 (OOM),\nthen to 25 (worked), then to 37 (OOM), then 31 (OOM), then 28 (worked), and finally\n29 (OOM).</p>\n\n<p>So we have a batch size of 28 for our 80 GiB machines.  Leaving it for a little\nwhile to settle down, and we get to about 142,000 tokens/second.</p>\n\n<p>Now, on the 3090, we were training at 20,000 tokens/second.  That means that this\nmachine is running at about 7 times the speed.  Given that our original train finished in\n48 hours, we'd expect the train to finish in about 6, which indeed is the estimated time on\nthe tqdm progress bar.</p>\n\n<p>At $6.38 per hour, that comes\nto $38.28.  Not bad!  And this instance is actually\nquite pricey on a per-GPU basis -- it's $3.19 per GPU/hour, whereas there is an 8x H100\nthat costs $2.99 per GPU/hour.</p>\n\n<p>I'm almost tempted to let it run.  But the purpose of this run was to work out the\nbugs.</p>\n\n<p>We're going to want to track the training chart -- remember that after every validation\nrun, our training code generates a chart showing the training and validation loss\nso far, <a href=\"/post-assets/llm-from-scratch-28-training-a-base-model-from-scratch/big-training-run-chart-fineweb.png\">like this one</a>.\nI ran the normal quick-and-dirty\nPython webserver command on the instance, inside the directory containing the\ntraining chart:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@192-222-53-181:~/ddp-base-model-from-scratch/runs/original$<span class=\"w\"> </span>python<span class=\"w\"> </span>-m<span class=\"w\"> </span>http.server<span class=\"w\"> </span><span class=\"m\">8000</span>\n</code></pre>\n</div>\n\n<p>My browser didn't connect to it, but looking at the Lambda Labs interface, there's a\nnew \"Firewall\" section, where you configure rules for allowing incoming connections\nto your instances.  That's sensible, and the default rules are just \"allow SSH from\nany IP\" and \"allow ping from any IP\".  Adding one letting anyone access port 8000\nfixed the problem, and I saw a directory listing; clicking on the chart showed exactly\nwhat I'd expect, but without the XKCD fonts.  Nice.</p>\n\n<p>Let's work out how to fix that XKCD font thing.  Looking around, it seems like there\nare approximately twenty thousand ways to do it.  Here's one that seems to work;\nfirstly, install the font on the system:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>mkdir<span class=\"w\"> </span>-p<span class=\"w\"> </span>~/.local/share/fonts\ncurl<span class=\"w\"> </span>-sL<span class=\"w\"> </span>https://github.com/ipython/xkcd-font/raw/master/xkcd-script/font/xkcd-script.ttf<span class=\"w\"> </span>-o<span class=\"w\"> </span>~/.local/share/fonts/xkcd-script.ttf\nfc-cache<span class=\"w\"> </span>-f<span class=\"w\"> </span>-v\n</code></pre>\n</div>\n\n<p>Now, that installs a font that has the family name 'xkcd Script` (with that erratic\ncapitalisation).  So we need to change the code to pick up pretty much anything that\nlooks like it's XKCD, so instead of this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">rcParams</span><span class=\"p\">[</span><span class=\"s1\">'font.family'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;xkcd&quot;</span>\n</code></pre>\n</div>\n\n<p>...we can do this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">matplotlib</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">font_manager</span>\n\n<span class=\"o\">...</span>\n\n    <span class=\"n\">font_family</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"k\">for</span> <span class=\"n\">f</span> <span class=\"ow\">in</span> <span class=\"n\">font_manager</span><span class=\"o\">.</span><span class=\"n\">fontManager</span><span class=\"o\">.</span><span class=\"n\">ttflist</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"s2\">&quot;xkcd&quot;</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">():</span>\n            <span class=\"n\">font_family</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">name</span>\n            <span class=\"k\">break</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">font_family</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">rcParams</span><span class=\"p\">[</span><span class=\"s1\">'font.family'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">font_family</span>\n</code></pre>\n</div>\n\n<p>That seems to work OK.</p>\n\n<p>So, now, I think we have the beginnings of a script to set up a Lambda Labs\nmachine so that we can use it.  Let's write a\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/30a3fd75f1eab96cd598a6c0a0f14465ddee6ae4/setup_lambda.sh\"><code>setup_lambda.sh</code></a>\nwith this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"ch\">#!/bin/bash</span>\n<span class=\"nb\">set</span><span class=\"w\"> </span>-a\ncurl<span class=\"w\"> </span>-LsSf<span class=\"w\"> </span>https://astral.sh/uv/install.sh<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>sh\nmkdir<span class=\"w\"> </span>-p<span class=\"w\"> </span>~/.local/share/fonts\ncurl<span class=\"w\"> </span>-sL<span class=\"w\"> </span>https://github.com/ipython/xkcd-font/raw/master/xkcd-script/font/xkcd-script.ttf<span class=\"w\"> </span>-o<span class=\"w\"> </span>~/.local/share/fonts/xkcd-script.ttf\nfc-cache<span class=\"w\"> </span>-f<span class=\"w\"> </span>-v\n</code></pre>\n</div>\n\n<p>...and give it another go on a fresh machine.  Shut this one down -- total cost\nso far $7.28.</p>\n\n<h3 id=\"a-second-run-on-lambda-labs-as-a-sanity-check\">A second run on Lambda Labs, as a sanity check</h3>\n\n<p>Now there are no 2-GPU instances available.  There is a super-cheap 1x A10 (basically\nthe datacenter version of a 3090), though, so let's use that -- we're as certain as\nwe can be that the multi-GPU stuff works, and the proof of the pudding will\nbe whether we can train a model that works.</p>\n\n<p>After spinning up our 1x A10 machine:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@150-136-154-247:~$<span class=\"w\"> </span>git<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://github.com/gpjt/ddp-base-model-from-scratch.git\nCloning<span class=\"w\"> </span>into<span class=\"w\"> </span><span class=\"s1\">'ddp-base-model-from-scratch'</span>...\nremote:<span class=\"w\"> </span>Enumerating<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">134</span>,<span class=\"w\"> </span><span class=\"k\">done</span>.\nremote:<span class=\"w\"> </span>Counting<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">134</span>/134<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"k\">done</span>.\nremote:<span class=\"w\"> </span>Compressing<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">89</span>/89<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"k\">done</span>.\nremote:<span class=\"w\"> </span>Total<span class=\"w\"> </span><span class=\"m\">134</span><span class=\"w\"> </span><span class=\"o\">(</span>delta<span class=\"w\"> </span><span class=\"m\">70</span><span class=\"o\">)</span>,<span class=\"w\"> </span>reused<span class=\"w\"> </span><span class=\"m\">98</span><span class=\"w\"> </span><span class=\"o\">(</span>delta<span class=\"w\"> </span><span class=\"m\">37</span><span class=\"o\">)</span>,<span class=\"w\"> </span>pack-reused<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"o\">)</span>\nReceiving<span class=\"w\"> </span>objects:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">134</span>/134<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"m\">127</span>.03<span class=\"w\"> </span>KiB<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">31</span>.76<span class=\"w\"> </span>MiB/s,<span class=\"w\"> </span><span class=\"k\">done</span>.\nResolving<span class=\"w\"> </span>deltas:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">70</span>/70<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"k\">done</span>.\nubuntu@150-136-154-247:~$<span class=\"w\"> </span><span class=\"nb\">cd</span><span class=\"w\"> </span>ddp-base-model-from-scratch\nubuntu@150-136-154-247:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>./setup_lambda.sh\ndownloading<span class=\"w\"> </span>uv<span class=\"w\"> </span><span class=\"m\">0</span>.9.15<span class=\"w\"> </span>x86_64-unknown-linux-gnu\nno<span class=\"w\"> </span>checksums<span class=\"w\"> </span>to<span class=\"w\"> </span>verify\ninstalling<span class=\"w\"> </span>to<span class=\"w\"> </span>/home/ubuntu/.local/bin\n<span class=\"w\">  </span>uv\n<span class=\"w\">  </span>uvx\neverything<span class=\"err\">'</span>s<span class=\"w\"> </span>installed!\n/usr/share/fonts:<span class=\"w\"> </span>caching,<span class=\"w\"> </span>new<span class=\"w\"> </span>cache<span class=\"w\"> </span>contents:<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span>fonts,<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span><span class=\"nb\">dirs</span>\n/usr/share/fonts/opentype:<span class=\"w\"> </span>caching,<span class=\"w\"> </span>new<span class=\"w\"> </span>cache<span class=\"w\"> </span>contents:<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span>fonts,<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span><span class=\"nb\">dirs</span>\n/usr/share/fonts/opentype/font-awesome:<span class=\"w\"> </span>caching,<span class=\"w\"> </span>new<span class=\"w\"> </span>cache<span class=\"w\"> </span>contents:<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>fonts,<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"nb\">dirs</span>\n/usr/share/fonts/truetype:<span class=\"w\"> </span>caching,<span class=\"w\"> </span>new<span class=\"w\"> </span>cache<span class=\"w\"> </span>contents:<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span>fonts,<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span><span class=\"nb\">dirs</span>\n/usr/share/fonts/truetype/dejavu:<span class=\"w\"> </span>caching,<span class=\"w\"> </span>new<span class=\"w\"> </span>cache<span class=\"w\"> </span>contents:<span class=\"w\"> </span><span class=\"m\">6</span><span class=\"w\"> </span>fonts,<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"nb\">dirs</span>\n/usr/share/fonts/truetype/font-awesome:<span class=\"w\"> </span>caching,<span class=\"w\"> </span>new<span class=\"w\"> </span>cache<span class=\"w\"> </span>contents:<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>fonts,<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"nb\">dirs</span>\n/usr/share/fonts/truetype/lato:<span class=\"w\"> </span>caching,<span class=\"w\"> </span>new<span class=\"w\"> </span>cache<span class=\"w\"> </span>contents:<span class=\"w\"> </span><span class=\"m\">18</span><span class=\"w\"> </span>fonts,<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"nb\">dirs</span>\n/usr/share/fonts/truetype/lyx:<span class=\"w\"> </span>caching,<span class=\"w\"> </span>new<span class=\"w\"> </span>cache<span class=\"w\"> </span>contents:<span class=\"w\"> </span><span class=\"m\">12</span><span class=\"w\"> </span>fonts,<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"nb\">dirs</span>\n/usr/local/share/fonts:<span class=\"w\"> </span>caching,<span class=\"w\"> </span>new<span class=\"w\"> </span>cache<span class=\"w\"> </span>contents:<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span>fonts,<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"nb\">dirs</span>\n/home/ubuntu/.local/share/fonts:<span class=\"w\"> </span>caching,<span class=\"w\"> </span>new<span class=\"w\"> </span>cache<span class=\"w\"> </span>contents:<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>fonts,<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"nb\">dirs</span>\n/home/ubuntu/.fonts:<span class=\"w\"> </span>skipping,<span class=\"w\"> </span>no<span class=\"w\"> </span>such<span class=\"w\"> </span>directory\n/usr/share/fonts/opentype:<span class=\"w\"> </span>skipping,<span class=\"w\"> </span>looped<span class=\"w\"> </span>directory<span class=\"w\"> </span>detected\n/usr/share/fonts/truetype:<span class=\"w\"> </span>skipping,<span class=\"w\"> </span>looped<span class=\"w\"> </span>directory<span class=\"w\"> </span>detected\n/usr/share/fonts/opentype/font-awesome:<span class=\"w\"> </span>skipping,<span class=\"w\"> </span>looped<span class=\"w\"> </span>directory<span class=\"w\"> </span>detected\n/usr/share/fonts/truetype/dejavu:<span class=\"w\"> </span>skipping,<span class=\"w\"> </span>looped<span class=\"w\"> </span>directory<span class=\"w\"> </span>detected\n/usr/share/fonts/truetype/font-awesome:<span class=\"w\"> </span>skipping,<span class=\"w\"> </span>looped<span class=\"w\"> </span>directory<span class=\"w\"> </span>detected\n/usr/share/fonts/truetype/lato:<span class=\"w\"> </span>skipping,<span class=\"w\"> </span>looped<span class=\"w\"> </span>directory<span class=\"w\"> </span>detected\n/usr/share/fonts/truetype/lyx:<span class=\"w\"> </span>skipping,<span class=\"w\"> </span>looped<span class=\"w\"> </span>directory<span class=\"w\"> </span>detected\n/var/cache/fontconfig:<span class=\"w\"> </span>not<span class=\"w\"> </span>cleaning<span class=\"w\"> </span>unwritable<span class=\"w\"> </span>cache<span class=\"w\"> </span>directory\n/home/ubuntu/.cache/fontconfig:<span class=\"w\"> </span>cleaning<span class=\"w\"> </span>cache<span class=\"w\"> </span>directory\n/home/ubuntu/.fontconfig:<span class=\"w\"> </span>not<span class=\"w\"> </span>cleaning<span class=\"w\"> </span>non-existent<span class=\"w\"> </span>cache<span class=\"w\"> </span>directory\nfc-cache:<span class=\"w\"> </span>succeeded\nubuntu@150-136-154-247:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>mkdir<span class=\"w\"> </span>datasets\nubuntu@150-136-154-247:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>torchrun<span class=\"w\"> </span>--nproc_per_node<span class=\"o\">=</span><span class=\"m\">1</span><span class=\"w\"> </span>ddp_train.py<span class=\"w\"> </span>original<span class=\"w\"> </span>datasets\nUsing<span class=\"w\"> </span>CPython<span class=\"w\"> </span><span class=\"m\">3</span>.13.10\nCreating<span class=\"w\"> </span>virtual<span class=\"w\"> </span>environment<span class=\"w\"> </span>at:<span class=\"w\"> </span>.venv\nInstalled<span class=\"w\"> </span><span class=\"m\">88</span><span class=\"w\"> </span>packages<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">1</span>.30s\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:14&lt;<span class=\"m\">00</span>:00,<span class=\"w\">  </span><span class=\"m\">3</span>.60s/it<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">20</span>.7GB<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:14,<span class=\"w\"> </span><span class=\"m\">1</span>.20GB/s<span class=\"o\">]</span><span class=\"w\">                                                                                                                             </span>Starting<span class=\"w\"> </span>rank<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span>training<span class=\"w\"> </span>at<span class=\"w\"> </span>global<span class=\"w\"> </span>step<span class=\"w\"> </span><span class=\"m\">0</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">20</span>.7GB<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:15,<span class=\"w\"> </span><span class=\"m\">1</span>.36GB/s<span class=\"o\">]</span>\n<span class=\"w\">  </span><span class=\"m\">0</span>%<span class=\"p\">|</span><span class=\"w\">                                                                                                                                          </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/530631<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>\n<span class=\"w\">  </span><span class=\"m\">0</span>%<span class=\"p\">|</span><span class=\"w\">                                                                                                                  </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/530631<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s,<span class=\"w\"> </span><span class=\"nv\">loss</span><span class=\"o\">=</span><span class=\"m\">10</span>.981,<span class=\"w\"> </span><span class=\"nv\">tps</span><span class=\"o\">=</span><span class=\"m\">6</span>,533<span class=\"o\">]</span>\n\nValidation/checkpoint\n<span class=\"m\">100</span>%<span class=\"p\">|</span>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">2</span>/2<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\">  </span><span class=\"m\">7</span>.27it/s<span class=\"o\">]</span>\n</code></pre>\n</div>\n\n<p>Looking good!  I think we have something that (in theory) should work.  That cost $0.05.</p>\n\n<p>I think it's time to do our first train on a big instance.</p>\n\n<h3 id=\"first-train-on-a-big-instance-8x-a100-40-gibgpu-sxm4\">First train on a big instance: 8x A100, 40 GiB/GPU, SXM4</h3>\n\n<p>There are four 8x instances available on Lambda Labs for me right now:</p>\n\n<ul>\n<li>An 8x B200, with 160 GiB per GPU, at $39.92/hour</li>\n<li>An 8x H100, with 80 GiB per GPU, at $23.92/hour</li>\n<li>An 8x A100, with 80 GiB per GPU, at $14.32/hour</li>\n<li>An 8x A100, with 40 GiB per GPU, at $10.32/hour</li>\n</ul>\n\n<p>I think I'm going to want to train on all of those, to try to work out some kind\nof metric (dollars per megatoken?) to compare them.  But let's start with something reasonably\nlow-end -- in fact, let's try the cheapest, and see what happens.</p>\n\n<p>Spin one up, and first thing; after the setup, we need to work out the micro-batch size.  Last time we used\n28, but this machine has GPUs with half as much VRAM.  I did a binary chop again... it turns out\nto be 13.</p>\n\n<p>Now let's think about validation frequency.  Let's try to get a feel for how long it will take.\nWe can set the eval batches to (say) 100, so that we can see how fast evals are,\nbut also set the interval to 10,000,000 so that it never does one after the first.</p>\n\n<p>It took 11 seconds to run 100 validation batches, and after a few minutes, it\nsettles down at 254,000 tokens/second or so, and is estimating 3h15m to completion.\nNice!  The cards are an earlier generation to the H100s we used in the two-GPU test,\nso they're slower, and they have half the VRAM.  So eight of them are, working together,\nabout twice as fast as two H100s.  Doesn't sound completely crazy.</p>\n\n<p>So, in our local train, we spent 5 minutes evaluating every 30 minutes.  So our eval\ntime was 16% of our train time.  Probably a bit high, but let's run with it.</p>\n\n<p>If we're going to take 3 hours training time, then 16% of that is about 28 minutes.\nPreviously we did about 88 evals (44 hours train time, with an eval after each\nhalf hour).  That seems a bit too high.  So let's say that we want to do 50 evals.</p>\n\n<p>28 minutes eval time in total, with 50 of them, means about 30 seconds per eval.  If\n100 eval batches take 11 seconds, let's approximate it to 300 eval batches.</p>\n\n<p>As to the interval between them -- if we want to do 50 over 3h15m, or 195 minutes,\nthen that's one every (let's approximate) 4 minutes.\nWe seem to have settled down to 2.57 iterations per second, so that's about every\n617 iterations.</p>\n\n<p>Let's bake those in and let it rip.</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@129-213-131-52:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>cat<span class=\"w\"> </span>runs/8xa100m40/train.json\n<span class=\"o\">{</span>\n<span class=\"w\">    </span><span class=\"s2\">&quot;microbatch_size&quot;</span>:<span class=\"w\"> </span><span class=\"m\">13</span>,\n<span class=\"w\">    </span><span class=\"s2\">&quot;validation_interval&quot;</span>:<span class=\"w\"> </span><span class=\"m\">617</span>,\n<span class=\"w\">    </span><span class=\"s2\">&quot;dataset&quot;</span>:<span class=\"w\"> </span><span class=\"s2\">&quot;gpjt/fineweb-gpt2-tokens&quot;</span>,\n<span class=\"w\">    </span><span class=\"s2\">&quot;min_train_tokens&quot;</span>:<span class=\"w\"> </span><span class=\"m\">3260190720</span>,\n<span class=\"w\">    </span><span class=\"s2\">&quot;start_train_token&quot;</span>:<span class=\"w\"> </span><span class=\"m\">0</span>,\n<span class=\"w\">    </span><span class=\"s2\">&quot;validation_batches&quot;</span>:<span class=\"w\"> </span><span class=\"m\">300</span>,\n<span class=\"w\">    </span><span class=\"s2\">&quot;start_val_token&quot;</span>:<span class=\"w\"> </span><span class=\"m\">0</span>\n<span class=\"o\">}</span>\n</code></pre>\n</div>\n\n<p>After the run:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 13,904.270 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,268,544</span>\n<span class=\"go\">Throughput: 234,480 tokens/second</span>\n<span class=\"go\">Final train loss: 3.720</span>\n<span class=\"go\">Final val loss: 3.675</span>\n</code></pre>\n</div>\n\n<p>OK, let's download everything.</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch/first-cloud-train<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>scp<span class=\"w\"> </span>ubuntu@129.213.131.52:/home/ubuntu/ddp-base-model-from-scratch/runs/8xa100m40/big-training-run-chart.png<span class=\"w\"> </span>.\nbig-training-run-chart.png\n</code></pre>\n</div>\n\n<p>Looking at the checkpoints, the latest (that is, the last one at the end of the training)\nand best (the checkpoint that had the lowest validation loss) are the same one, meaning\nthat validation loss kept falling consistently:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">drwxrwxr-x 2 ubuntu ubuntu 4096 Dec  6 01:49 20251206Z014912-iteration-29616</span>\n<span class=\"go\">drwxrwxr-x 2 ubuntu ubuntu 4096 Dec  6 01:53 20251206Z015351-iteration-30233</span>\n<span class=\"go\">lrwxrwxrwx 1 ubuntu ubuntu   31 Dec  6 01:57 latest -&gt; 20251206Z015658-iteration-30613</span>\n<span class=\"go\">lrwxrwxrwx 1 ubuntu ubuntu   31 Dec  6 01:57 best -&gt; 20251206Z015658-iteration-30613</span>\n<span class=\"go\">drwxrwxr-x 2 ubuntu ubuntu 4096 Dec  6 01:57 20251206Z015658-iteration-30613</span>\n</code></pre>\n</div>\n\n<p>So let's just download using the \"best\" symlink to get the weights for that checkpoint:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch/runs/8xa100m40 (main)$ </span>scp<span class=\"w\"> </span>-r<span class=\"w\"> </span>ubuntu@129.213.131.52:/home/ubuntu/ddp-base-model-from-scratch/runs/8xa100m40/big-training-run-chart.png<span class=\"w\"> </span>.\n<span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch/runs/8xa100m40 (main)$ </span>mkdir<span class=\"w\"> </span>checkpoints<span class=\"p\">;</span><span class=\"w\"> </span><span class=\"nb\">cd</span><span class=\"w\"> </span>checkpoints\n<span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch/runs/8xa100m40/checkpoints (main)$ </span>scp<span class=\"w\"> </span>-r<span class=\"w\"> </span>ubuntu@129.213.131.52:/home/ubuntu/ddp-base-model-from-scratch/runs/8xa100m40/checkpoints/best/<span class=\"w\"> </span>.\n<span class=\"go\">scaler.pt                                                                                                                               100% 1383     6.8KB/s   00:00</span>\n<span class=\"go\">optimizer.pt                                                                                                                            100% 1244MB   7.7MB/s   02:41</span>\n<span class=\"go\">model.safetensors                                                                                                                       100%  670MB   7.4MB/s   01:29</span>\n<span class=\"go\">meta.json                                                                                                                               100%  104     0.5KB/s   00:00</span>\n<span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch/runs/8xa100m40/checkpoints (main)$ </span>ls<span class=\"w\"> </span>-lrt\n<span class=\"go\">total 84</span>\n<span class=\"go\">drwxr-xr-x 2 giles giles  4096 Dec  6 02:05 best</span>\n<span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch/runs/8xa100m40/checkpoints (main)$ </span>ls<span class=\"w\"> </span>best\n<span class=\"go\">meta.json  model.safetensors  optimizer.pt  scaler.pt</span>\n<span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch/runs/8xa100m40/checkpoints (main)$ </span>ls<span class=\"w\"> </span>-h<span class=\"w\"> </span>best\n<span class=\"go\">meta.json  model.safetensors  optimizer.pt  scaler.pt</span>\n<span class=\"gp\">giles@perry:~/Dev/ddp-base-model-from-scratch/runs/8xa100m40/checkpoints (main)$ </span>ls<span class=\"w\"> </span>-lh<span class=\"w\"> </span>best\n<span class=\"go\">total 1.9G</span>\n<span class=\"go\">-rw-r--r-- 1 giles giles  104 Dec  6 02:05 meta.json</span>\n<span class=\"go\">-rw-r--r-- 1 giles giles 670M Dec  6 02:05 model.safetensors</span>\n<span class=\"go\">-rw-r--r-- 1 giles giles 1.3G Dec  6 02:04 optimizer.pt</span>\n<span class=\"go\">-rw-r--r-- 1 giles giles 1.4K Dec  6 02:01 scaler.pt</span>\n</code></pre>\n</div>\n\n<p>And now we can shut the cloud machine down.</p>\n\n<p>Now that the clock is no longer ticking and we aren't spending money on an unused\nmachine, here's the training chart:</p>\n\n<p><img alt=\"Training run on an 8x A100 with 40 GiB/GPU\" src=\"/post-assets/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud/training-run-8xa100m40.png\" title=\"Training run on an 8x A100 with 40 GiB/GPU\" /></p>\n\n<p>It looks like we had a couple of gradient spikes there.  I'm going to add some gradient\nclipping code at some point, but I think I'll hold off for a little bit -- I want\nto do a few cloud trains first to work out the best instance sizes to use, and only\nthen start exploring the possibilities for making the models better.</p>\n\n<p>Apart from that, it looks pretty normal.</p>\n\n<p>Looking at the billing page on Lambda Labs, that machine was up for about 4 hours and 35 minutes, costing US$10.32 per hour,\nfor a total cost of US$47.35.</p>\n\n<p>Of that 4h35m, 13,904 seconds, or 3h52 was the actual training run -- somewhat more\nthan the 3h15m that was predicted at the start of the run.  The validation will have\naccounted for most of that -- we did 50 evals, at 30 seconds each, so that's 25 minutes.\nThat means that 3h40m is accounted for, and the remainder can just be chalked up to\nnoise, I guess.</p>\n\n<p>That leads to one question: do we actually need to be doing validation for these\ntrains?</p>\n\n<h3 id=\"to-validate-or-not-to-validate\">To validate or not to validate?</h3>\n\n<p>I've been doing validation loops in these trains largely out of habit -- when you're\ntraining an ML model, it's just \"what you do\".</p>\n\n<p>The reason you'd normally hold out a validation set is simple: if you're training\nover multiple epochs, then eventually your model is going to start overfitting to the training\ndata <sup class=\"footnote-ref\" id=\"fnref-2\"><a href=\"#fn-2\">2</a></sup>.  You validate as you go along so that you can spot any points where,\nwhile the training loss continues to drop, the validation loss -- which is loss on\ndata that the model hasn't been trained on -- starts rising.  That's the classic\nindicator of overfitting.</p>\n\n<p>But for these models we're not doing multiple epochs -- we're just training through\na stream of constantly new tokens.  So, in fact, there's no real difference between\nthe training data and the validation data, apart from the fact that the validation\ndata is constant.  From the model's perspective, it's all new stuff (modulo any\nrepetitions in the dataset, which is possible but I think not likely to be\nsuper-common in something as curated as FineWeb).</p>\n\n<p>Now, in this post I'm aiming to identify the best options for training in the cloud --\ncost in terms of dollars and time.  I don't want to change the model itself or the\ntraining strategy because I want whatever I come up with to be roughly equivalent\nto the models I trained on my own machine.  Exploring enhancements is for the\nnext post. (Of course, given that the batch size is one of the levers I want to\nexperiment with, and training on larger machines is already meaning that I'm doing\nmicro-batches larger than the batch size of 6 that I used locally, and then the overall batches are 8 times\nlarger, that's not quite true.)</p>\n\n<p>Validation, however, doesn't actually affect the training runs in any direct way.\nI could in theory remove it.</p>\n\n<p>However, that is a relatively large change to the code, as I've kind of linked\nit in with my checkpointing code.</p>\n\n<p>I think that what I'll do for now is leave it in.  Validation will scale at the same\nrate as training (so long as I leave the eval batches constant) so it leaving it\nthere will give me a clean comparison between machine types.  And I can keep notes\non how much time was spent on validation for each train so that I can subtract it from\nthe total time if that proves useful.</p>\n\n<p>However, when I start tweaking the training code with changes beyond the batch\nsize, I should probably try removing validation first.</p>\n\n<p>Anyway, while validation during the training run might not be important, evaluating\nthe model at the end and seeing how it compares to others is!  Let's do that next.</p>\n\n<h3 id=\"testing-the-first-model\">Testing the first model</h3>\n\n<p>There were two important post-train evals that I did on the models that I trained locally:</p>\n\n<ol>\n<li>The loss they got on the validation set from the first train.  Strictly speaking, I was kind of\ncheating and using that as a test set.</li>\n<li>The score given by the OpenAI GPT 5.1 model for an instruction-following dataset.\nThis was the one provided in the book -- an Alpaca-style Q&amp;A dataset, with a\nwell-defined train and test set.  Each model was fine-tuned on a training set of 85% of the data\nuntil loss on a validation set of 5% of the data started rising, and then tested on the\nremaining 10%.  Sebastian Raschka, being a pro, was splitting up the data properly :-)</li>\n</ol>\n\n<p>There was also a simple smoke test -- how does the model predict that the phrase</p>\n\n<pre><code>Every effort moves you\n</code></pre>\n\n<p>...should continue?</p>\n\n<p>I should do the same three tests here.</p>\n\n<h4 id=\"smoke-test\">Smoke test</h4>\n\n<p><a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/a9167188d4a64762bfb3d45624f6677cabff9efd/test_smoke.py\">A simple autoregressive generation script</a>\nis easy enough to knock together, and:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xa100m40/model.json<span class=\"w\"> </span>runs/8xa100m40/checkpoints/best/model.safetensors\nEvery<span class=\"w\"> </span>effort<span class=\"w\"> </span>moves<span class=\"w\"> </span>you<span class=\"w\"> </span>toward<span class=\"w\"> </span>finding<span class=\"w\"> </span>more<span class=\"w\"> </span>fun<span class=\"w\"> </span>stuff,”<span class=\"w\"> </span>Geller<span class=\"w\"> </span>explains.\n“We<span class=\"w\"> </span>love<span class=\"w\"> </span>the<span class=\"w\"> </span>music<span class=\"w\"> </span>because\n</code></pre>\n</div>\n\n<p>All we're looking for here is basic coherency, and I think this is good enough to\npass that filter.</p>\n\n<h4 id=\"loss-tests\">Loss tests</h4>\n\n<p>Next, the loss-style testing.  What I think I want to be able to do here is\njust take a <code>model.safetensors</code> file and run an eval against a standard dataset.</p>\n\n<p>I did not generate my own test set, but I did generate a much-larger-than-necessary\neval set, 1% of both FineWeb and FineWeb-Edu -- that's 100 million tokens or so in\nboth cases.</p>\n\n<p>In the validation that I was doing during the train just now, I did 300 batches\nof 1,024 tokens with a micro-batch size of 13.  That only ran on the rank 0 process,\nso that's</p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>300</mn><mi>&#x000d7;</mi><mn>13</mn><mi>&#x000d7;</mi><mn>1</mn><mo>&#x0002c;</mo><mn>024</mn><mo>&#x0003d;</mo><mn>3</mn><mo>&#x0002c;</mo><mn>993</mn><mo>&#x0002c;</mo><mn>600</mn><mtext>tokens</mtext></mrow></math>\n\n<p>Not even 4% of the validation data.</p>\n\n<p>Now, for the local eval, I think it makes sense to make it run for about five minutes\n-- that's just for my own convenience, I don't want to spend very long -- and\nI know from the previous local train that I can do 3,200 batches of six 1,024-token\nsequences in that time:</p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mn>3</mn><mo>&#x0002c;</mo><mn>200</mn><mi>&#x000d7;</mi><mn>6</mn><mi>&#x000d7;</mi><mn>1</mn><mo>&#x0002c;</mo><mn>024</mn><mo>&#x0003d;</mo><mn>19</mn><mo>&#x0002c;</mo><mn>660</mn><mo>&#x0002c;</mo><mn>800</mn><mtext>tokens</mtext></mrow></math>\n\n<p>So, somewhat arbitrarily, let's use the 19,660,800 tokens starting at position 50,000,000 in the FineWeb\nvalidation dataset for our tests -- they'll never be used for training or validation during\nthe training loop.  It's kind of a hack, but it'll do for now.</p>\n\n<p><a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/80fb517ef91785644264b5eddb14d88455fa29da/test_loss.py\">Here's the code</a>.\nIt should be easy enough to understand; it did require one tweak to our existing <code>load_dataset</code> function,\nthough:</p>\n\n<p>Originally, that function worked out out the actual number of tokens to use by\nworking out the size of each global batch, dividing our requested minimum number of\ntokens by that size and taking the floor, adding on one, then multiplying that by\nthe global batch size.</p>\n\n<p>That works fine in cases where the <code>min_tokens</code> is not a multiple of the global\nbatch size -- it gives us a round number of batches that contains at least <code>min_tokens</code>.\nBut if <code>min_tokens</code> is already a multiple of the global batch size, it gives us\nan extra batch at the end.  So I added that as a special case in <code>load_dataset</code> to\navoid that.</p>\n\n<p>Anyway, running that gives us a loss:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets<span class=\"w\"> </span>runs/8xa100m40/model.json<span class=\"w\"> </span>runs/8xa100m40/checkpoints/best/model.safetensors\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">588</span>.84it/s<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">0</span>.00B<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00,<span class=\"w\"> </span>?B/s<span class=\"o\">]</span><span class=\"w\">                                                                                                            </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>\n<span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3200</span>/3200<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">05</span>:05&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.49it/s<span class=\"o\">]</span>\nLoss<span class=\"w\"> </span>against<span class=\"w\"> </span>our<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>dataset:<span class=\"w\"> </span><span class=\"m\">3</span>.674\n</code></pre>\n</div>\n\n<p>That's actually quite a lot lower than we were seeing with the locally-trained\nmodels on the test dataset I was using then -- but, of course, it's a different\ndataset so it's not strictly comparable.</p>\n\n<p>Let's run the same test against them:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets<span class=\"w\"> </span>~/Dev/llm-from-scratch/big-train-model-conf.json<span class=\"w\"> </span>~/Dev/llm-from-scratch/big-train-checkpoints-fw/best/model.safetensors\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">3069</span>.94it/s<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">0</span>.00B<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00,<span class=\"w\"> </span>?B/s<span class=\"o\">]</span><span class=\"w\">                                                                                                            </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>\n<span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3200</span>/3200<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">04</span>:56&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.79it/s<span class=\"o\">]</span>\nLoss<span class=\"w\"> </span>against<span class=\"w\"> </span>our<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>dataset:<span class=\"w\"> </span><span class=\"m\">3</span>.944\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets<span class=\"w\"> </span>~/Dev/llm-from-scratch/big-train-model-conf.json<span class=\"w\"> </span>~/Dev/llm-from-scratch/big-train-checkpoints-fw-edu/best/model.safetensors\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">979</span>.35it/s<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">0</span>.00B<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00,<span class=\"w\"> </span>?B/s<span class=\"o\">]</span><span class=\"w\">                                                                                                            </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>\n<span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3200</span>/3200<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">04</span>:55&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.83it/s<span class=\"o\">]</span>\nLoss<span class=\"w\"> </span>against<span class=\"w\"> </span>our<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>dataset:<span class=\"w\"> </span><span class=\"m\">4</span>.167\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets<span class=\"w\"> </span>~/Dev/llm-from-scratch/big-train-model-conf.json<span class=\"w\"> </span>~/Dev/llm-from-scratch/big-train-checkpoints-fw-edu-2x/best/model.safetensors\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">832</span>.12it/s<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">0</span>.00B<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00,<span class=\"w\"> </span>?B/s<span class=\"o\">]</span><span class=\"w\">                                                                                                            </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>\n<span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3200</span>/3200<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">04</span>:54&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.87it/s<span class=\"o\">]</span>\nLoss<span class=\"w\"> </span>against<span class=\"w\"> </span>our<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>dataset:<span class=\"w\"> </span><span class=\"m\">4</span>.135\n</code></pre>\n</div>\n\n<p>That's really interesting!   Those numbers are really close to the numbers I got in the\nlast post.   That does make some kind of sense, though -- while the numbers aren't\nstrictly comparable, as I said, both the dataset that I was using then and the one I'm\nusing now are essentially random stuff from FineWeb, so I guess they must be\nmore similar than I thought.</p>\n\n<p>But, importantly, the loss on the newly-trained model is much lower -- 3.674 rather\nthan &gt; 3.9 for all three of the older locally-trained models.</p>\n\n<p>Now, the only big difference between this training run and the ones that I did locally\nis the batch size.  As I said in the last post, while I felt that the difference\nbetween my batch size of six and the (reported) batch size of 512 for the original\nGPT-2 was the least-likely cause of the differences in the results, Gemini told me\nthat it thought it was the most likely cause.</p>\n\n<p>It looks like Gemini (and, I should note, <a href=\"https://news.ycombinator.com/item?id=46205589\"><code>spi</code> on Hacker News</a>)\nmight have been right!  Batch size is super-important.</p>\n\n<p>Let's do the same eval with the OpenAI weights.  I wrote a quick script (in my\nold 'LLM from scratch' repo, which has the code used in the book) to\n<a href=\"https://github.com/gpjt/llm-from-scratch/blob/c939774754f3b8ee42c1490f53ade2b77433e548/convert_openai_weights_to_safetensors.py\">load up the GPT-2 weights and save them as a safetensors file</a>.</p>\n\n<p>When I ran that, I got an interesting error:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">ValueError: You are trying to save a non contiguous tensor:</span>\n<span class=\"go\">``trf_blocks.0.att.W_query.weight`` which is not allowed. It either means you are</span>\n<span class=\"go\">trying to save tensors which are reference of each other in which case it's</span>\n<span class=\"go\">recommended to save only the full tensors, and reslice at load time, or simply</span>\n<span class=\"go\">call ``.contiguous()`` on your tensor to pack it before saving.</span>\n</code></pre>\n</div>\n\n<p>That was easy enough to fix; in the book's code we assign the weights that have\nbeen loaded from the OpenAI TensorFlow checkpoint files with a function called\n<code>assign</code> that looks like this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">assign</span><span class=\"p\">(</span><span class=\"n\">left</span><span class=\"p\">,</span> <span class=\"n\">right</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">left</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">!=</span> <span class=\"n\">right</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span>\n            <span class=\"sa\">f</span><span class=\"s2\">&quot;Shape mismatch.  Left: </span><span class=\"si\">{</span><span class=\"n\">left</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"si\">}</span><span class=\"s2\">, Right: </span><span class=\"si\">{</span><span class=\"n\">right</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span>\n        <span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Parameter</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">right</span><span class=\"p\">))</span>\n</code></pre>\n</div>\n\n<p>Just adding a call to <code>contiguous</code> to the last line fixed the error:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Parameter</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">right</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">contiguous</span><span class=\"p\">())</span>\n</code></pre>\n</div>\n\n<p>...and as a result, I had safetensors files for the original OpenAI models:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"o\">(</span>llm-from-scratch<span class=\"o\">)</span><span class=\"w\"> </span>giles@perry:~/Dev/llm-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>ls<span class=\"w\"> </span>-lrt\n...\n-rw-r--r--<span class=\"w\">  </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\">        </span><span class=\"m\">731</span><span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">9</span><span class=\"w\"> </span><span class=\"m\">18</span>:57<span class=\"w\"> </span>convert_openai_weights_to_safetensors.py\n-rw-r--r--<span class=\"w\">  </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\">        </span><span class=\"m\">160</span><span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">9</span><span class=\"w\"> </span><span class=\"m\">19</span>:00<span class=\"w\"> </span>openai-weights-gpt-medium.json\n-rw-r--r--<span class=\"w\">  </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\">        </span><span class=\"m\">159</span><span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">9</span><span class=\"w\"> </span><span class=\"m\">19</span>:01<span class=\"w\"> </span>openai-weights-gpt-small.json\n-rw-r--r--<span class=\"w\">  </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\">       </span><span class=\"m\">4452</span><span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">9</span><span class=\"w\"> </span><span class=\"m\">19</span>:10<span class=\"w\"> </span>download_and_use_gpt2.py\ndrwxr-xr-x<span class=\"w\">  </span><span class=\"m\">2</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\">       </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">9</span><span class=\"w\"> </span><span class=\"m\">19</span>:10<span class=\"w\"> </span>__pycache__\n-rw-r--r--<span class=\"w\">  </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\"> </span><span class=\"m\">1725850968</span><span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">9</span><span class=\"w\"> </span><span class=\"m\">19</span>:10<span class=\"w\"> </span>gpt-2-medium.safetensors\n-rw-r--r--<span class=\"w\">  </span><span class=\"m\">1</span><span class=\"w\"> </span>giles<span class=\"w\"> </span>giles<span class=\"w\">  </span><span class=\"m\">702501224</span><span class=\"w\"> </span>Dec<span class=\"w\">  </span><span class=\"m\">9</span><span class=\"w\"> </span><span class=\"m\">19</span>:14<span class=\"w\"> </span>gpt-2-small.safetensors\n</code></pre>\n</div>\n\n<p>So now we can run our test against them:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets<span class=\"w\"> </span>~/Dev/llm-from-scratch/openai-weights-gpt-medium.json<span class=\"w\"> </span>~/Dev/llm-from-scratch/gpt-2-medium.safetensors\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">804</span>.24it/s<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">0</span>.00B<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00,<span class=\"w\"> </span>?B/s<span class=\"o\">]</span><span class=\"w\">                                                                                                            </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>\n<span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3200</span>/3200<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">12</span>:41&lt;<span class=\"m\">00</span>:00,<span class=\"w\">  </span><span class=\"m\">4</span>.20it/s<span class=\"o\">]</span>\nLoss<span class=\"w\"> </span>against<span class=\"w\"> </span>our<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>dataset:<span class=\"w\"> </span><span class=\"m\">3</span>.231\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets<span class=\"w\"> </span>~/Dev/llm-from-scratch/openai-weights-gpt-small.json<span class=\"w\"> </span>~/Dev/llm-from-scratch/gpt-2-small.safetensors\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">687</span>.84it/s<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">0</span>.00B<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00,<span class=\"w\"> </span>?B/s<span class=\"o\">]</span><span class=\"w\">                                                                                                            </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>\n<span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3200</span>/3200<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">04</span>:53&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.89it/s<span class=\"o\">]</span>\nLoss<span class=\"w\"> </span>against<span class=\"w\"> </span>our<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>dataset:<span class=\"w\"> </span><span class=\"m\">3</span>.500\n</code></pre>\n</div>\n\n<p>Excellent.  Let's start putting together a table of these results:</p>\n\n<table>\n<thead>\n<tr>\n  <th></th>\n  <th>Test loss</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>OpenAI weights: medium</td>\n  <td>3.231</td>\n</tr>\n<tr>\n  <td>OpenAI weights: small</td>\n  <td>3.500</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x A100 40 GiB</td>\n  <td>3.674</td>\n</tr>\n<tr>\n  <td>Local FineWeb train</td>\n  <td>3.944</td>\n</tr>\n<tr>\n  <td>Local FineWeb-Edu extended train</td>\n  <td>4.135</td>\n</tr>\n<tr>\n  <td>Local FineWeb-Edu train</td>\n  <td>4.167</td>\n</tr>\n</tbody>\n</table>\n\n<p>That's pretty amazing.  Having a batch size of 13 micro-batches over eight GPUs, or\n104 in total, seems to have massively improved the model -- it's much closer to the\noriginal weights.  It will be interesting to see whether I get further improvements\nwhen I move to the larger machines, which (due to having more VRAM) will have larger\npossible micro-batches, so we'll get larger global batch sizes.</p>\n\n<p>It certainly makes me think that I could have got much better results locally by using\ngradient accumulation, which would mimic the effects of a larger batch size by running\nmultiple smaller batches through, without doing an optimiser step each time, then doing\none big update once enough has gone through.</p>\n\n<p>But all of that is for another day.  Let's try the instruction fine-tuning test now.</p>\n\n<h4 id=\"instruction-fine-tuning\">Instruction fine-tuning</h4>\n\n<p>I decided to pretty much re-use my adapted version of the code from the book;\nthat meant that I was borrowing quite a lot of Raschka's code, which he has\n<a href=\"https://github.com/rasbt/LLMs-from-scratch/blob/main/LICENSE.txt\">released under the Apache 2 license</a>.\nI normally use the MIT license for my code, but I'm not married to it, so I\nrelicensed the whole repo as Apache 2 with some specific headers to say which parts\ncame from \"Build a Large Language Model (from Scratch)\", and added\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/77ab971c5830a8f4827c1e485267915690871b0f/test_ift.py\">this code</a>.</p>\n\n<p>It downloads the Alpaca dataset from the site for the book, splits it into train/validation/test splits,\ntrains on the training set, evaluating each epoch and bailing out (and restoring the\nprevious epoch's weights) when validation loss starts rising, and then runs through the\ntest set generating responses, and then sends them all off to the OpenAI API for\nGPT-5.1 to judge them.</p>\n\n<p>Running it against our new model gets a score of 17.09.  Let's try the various other models\nand build out our table:</p>\n\n<table>\n<thead>\n<tr>\n  <th></th>\n  <th>Test loss</th>\n  <th>Instruction fine-tune score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>OpenAI weights: medium</td>\n  <td>3.231</td>\n  <td>38.53</td>\n</tr>\n<tr>\n  <td>OpenAI weights: small</td>\n  <td>3.500</td>\n  <td>22.98</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x A100 40 GiB</td>\n  <td>3.674</td>\n  <td>17.09</td>\n</tr>\n<tr>\n  <td>Local FineWeb train</td>\n  <td>3.944</td>\n  <td>16.01</td>\n</tr>\n<tr>\n  <td>Local FineWeb-Edu extended train</td>\n  <td>4.135</td>\n  <td>14.55</td>\n</tr>\n<tr>\n  <td>Local FineWeb-Edu train</td>\n  <td>4.167</td>\n  <td>16.86</td>\n</tr>\n</tbody>\n</table>\n\n<p>Interesting!  In the last run, <a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch#but-why-is-our-model-worse-than-openais\">I found</a>\nthe instruction fine-tune numbers came out as FineWeb-Edu extended &gt; FineWeb &gt; FineWeb-Edu,\nbut here we have FineWeb-Edu &gt; FineWeb &gt; FineWeb-Edu extended -- exactly the opposite!</p>\n\n<p>I do have to wonder, though, how precise a measure this is.  While the training should\nbe fairly consistent (though I don't have a random seed in there to enforce it),\nthe fact that we're using an LLM as a judge means that there is an element of\nrandomness coming in here.  Indeed, I re-ran the FineWeb-Edu extended train test again,\njust to see what I got, and it came up with an even-worse 12.12.</p>\n\n<p>So I don't think we can read a huge amount into these numbers -- well, unless we can get\nthe numbers significantly up.  While it looks like a 2.5-point difference might\njust be randomness, I doubt that a 10-point difference could be.</p>\n\n<p>I think we've done the tests that we need for this model now, and we have a testing\nprocedure in place.  So let's train some further models on different instance\nsizes, and gather numbers.</p>\n\n<h3 id=\"training-on-an-8x-b200-with-160-gib-per-gpu-using-sxm6\">Training on an 8x B200 with 160 GiB per GPU, using SXM6</h3>\n\n<p>This is the biggest machine available on Lambda Labs right now, and is only\nsporadically available; one happens to be there now, so let's\nto give it a go.  First, we need to create the <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/tree/main/runs/8xb200m160\">runs/8xb200m160</a>\ndirectory, initially with a <code>train.json</code> that is a clone of the one I did for\nthe last train, <code>8xa100m40</code>, then spin up the machine.</p>\n\n<h4 id=\"the-train\">The train</h4>\n\n<p>As before, we need to log in, clone the repo, then in it run the\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/30a3fd75f1eab96cd598a6c0a0f14465ddee6ae4/setup_lambda.sh\"><code>setup_lambda.sh</code></a>\nscript, run <code>uv sync</code>, and try to run the script:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">uv run torchrun --nproc_per_node=8 ddp_train.py 8xb200m160 datasets</span>\n</code></pre>\n</div>\n\n<p>It crapped out because there was no datasets directory, which is an annoyance.\nWe should create it if it doesn't exist.</p>\n\n<p>Create the directory, and run it again.  It took a while to download the\ndataset, because every per-GPU process downloads it separately.  That only took a minute or two,\nbut it was a waste of time; I think we should only download it from the rank 0 process\nwith some barriers to make the other processes pause.</p>\n\n<p>Next, we need to do a binary chop on the micro-batch size, starting with a low of 13 (which I\nknow will be fine because it worked on the 40 GiB GPUs that we used last time),\nand a high of 100 (fairly random, just something I'm pretty sure will fail).</p>\n\n<p>While doing that, a few things are standing out, both to do with validation.\nWhen the script starts, it does one training iteration, then goes straight into\nvalidation.  Then it starts the training run proper.  However:</p>\n\n<ul>\n<li>If we're going to do validation then it does make some sense to do one at the start --\nbut doing one training iteration first seems kind of arbitrary (though it's clear how\nthat drops out of the existing code).</li>\n<li>The validation runs on this machine are taking longer than they were on the less-powerful\nA100 GPUs!  That confused me for a bit, until I realised that I didn't notice that it was slower\nwith the batch-size 13 test, only with the larger ones later in in the binary\nchop.  If we're using larger batches, then there's more work to do for the validation.</li>\n<li>Doing this binary chop by hand is annoying and error-prone, and worse, we have to wait\nfor one of those (long) validation runs before we get into proper training.  The initial\ntraining iteration can succeed, while later ones hit memory limits -- it seems like we need to wait\nfor three or four training iterations before we can be sure that we have a workable\nbatch size.  Not quite sure why that is, perhaps it's something in the optimiser or\nthe scaler?</li>\n</ul>\n\n<p>We're going to need to work out some kind of fix for that, because it's taken me\n17 minutes from spinning up the machine to getting a size for our micro-batches --\nwhich happens to be 64.  On a machine that costs US$39.92/hour, that's an expensive test!\nWe'll look into that later.</p>\n\n<p>Anyway, a batch size of 64 is pretty neat, as with 8 GPUs, that means we have a global\nbatch size of 512 -- exactly the same as in the original GPT-2 paper!</p>\n\n<p>So, let's kick off the train.  It takes about 7 minutes to get to the first checkpoint,\nat which point it's averaging 801,221 tokens/second.  That pattern repeats, and with about one minute to\ndo the validation, we're spending about 12.5% of the time on this machine validating.\nHmm.  A further indication that we might want to remove the validation stuff if it's\nnot adding on any value.</p>\n\n<p>Eventually, it finishes:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Training complete in 4,190.357 seconds</span>\n<span class=\"go\">Tokens seen: 3,260,547,072</span>\n<span class=\"go\">Throughput: 778,107 tokens/second</span>\n<span class=\"go\">Final train loss: 3.865</span>\n<span class=\"go\">Final val loss: 3.770</span>\n</code></pre>\n</div>\n\n<p>So, that's 1h9m50s.  The final validation loss is not\nas good as the previous run on the 8x A100 40 GiB machine, where we got down to 3.675.  Given that we're using the\nsame validation dataset as the previous, that's meaningful: this is not as good a\nmodel, it seems.</p>\n\n<p>Again, latest and best checkpoints are the same one:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@129-213-85-212:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>ls<span class=\"w\"> </span>-lrt<span class=\"w\"> </span>runs/8xb200m160/checkpoints/\ntotal<span class=\"w\"> </span><span class=\"m\">64</span>\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:05<span class=\"w\"> </span>20251210Z170527-iteration-0\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:07<span class=\"w\"> </span>20251210Z170712-iteration-0\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:08<span class=\"w\"> </span>20251210Z170848-iteration-0\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:10<span class=\"w\"> </span>20251210Z171043-iteration-0\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:12<span class=\"w\"> </span>20251210Z171231-iteration-0\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:19<span class=\"w\"> </span>20251210Z171914-iteration-617\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:26<span class=\"w\"> </span>20251210Z172557-iteration-1234\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:32<span class=\"w\"> </span>20251210Z173241-iteration-1851\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:39<span class=\"w\"> </span>20251210Z173924-iteration-2468\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:46<span class=\"w\"> </span>20251210Z174608-iteration-3085\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:52<span class=\"w\"> </span>20251210Z175251-iteration-3702\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">17</span>:59<span class=\"w\"> </span>20251210Z175935-iteration-4319\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">18</span>:06<span class=\"w\"> </span>20251210Z180619-iteration-4936\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">18</span>:13<span class=\"w\"> </span>20251210Z181302-iteration-5553\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">18</span>:19<span class=\"w\"> </span>20251210Z181945-iteration-6170\nlrwxrwxrwx<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\">   </span><span class=\"m\">30</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">18</span>:21<span class=\"w\"> </span>latest<span class=\"w\"> </span>-&gt;<span class=\"w\"> </span>20251210Z182116-iteration-6218\nlrwxrwxrwx<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\">   </span><span class=\"m\">30</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">18</span>:21<span class=\"w\"> </span>best<span class=\"w\"> </span>-&gt;<span class=\"w\"> </span>20251210Z182116-iteration-6218\ndrwxrwxr-x<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>ubuntu<span class=\"w\"> </span>ubuntu<span class=\"w\"> </span><span class=\"m\">4096</span><span class=\"w\"> </span>Dec<span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"m\">18</span>:21<span class=\"w\"> </span>20251210Z182116-iteration-6218\n</code></pre>\n</div>\n\n<p>So we can download everything:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch/runs/8xb200m160<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>scp<span class=\"w\"> </span>ubuntu@129.213.85.212:/home/ubuntu/ddp-base-model-from-scratch/runs/8xb200m160/big-training-run-chart.png<span class=\"w\"> </span>.\nbig-training-run-chart.png<span class=\"w\">                                                                                                              </span><span class=\"m\">100</span>%<span class=\"w\">   </span>75KB<span class=\"w\"> </span><span class=\"m\">149</span>.0KB/s<span class=\"w\">   </span><span class=\"m\">00</span>:00\ngiles@perry:~/Dev/ddp-base-model-from-scratch/runs/8xb200m160<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>scp<span class=\"w\"> </span>-r<span class=\"w\"> </span>ubuntu@129.213.85.212:/home/ubuntu/ddp-base-model-from-scratch/runs/8xb200m160/checkpoints/best<span class=\"w\"> </span>./\nbig-training-run-chart.html<span class=\"w\">  </span>big-training-run-chart.png<span class=\"w\">   </span>model.json<span class=\"w\">                   </span>train.json\ngiles@perry:~/Dev/ddp-base-model-from-scratch/runs/8xb200m160<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>scp<span class=\"w\"> </span>-r<span class=\"w\"> </span>ubuntu@129.213.85.212:/home/ubuntu/ddp-base-model-from-scratch/runs/8xb200m160/checkpoints/best<span class=\"w\"> </span>./\nmeta.json<span class=\"w\">                                                                                                                               </span><span class=\"m\">100</span>%<span class=\"w\">  </span><span class=\"m\">100</span><span class=\"w\">     </span><span class=\"m\">0</span>.5KB/s<span class=\"w\">   </span><span class=\"m\">00</span>:00\noptimizer.pt<span class=\"w\">                                                                                                                            </span><span class=\"m\">100</span>%<span class=\"w\"> </span>1244MB<span class=\"w\">  </span><span class=\"m\">12</span>.2MB/s<span class=\"w\">   </span><span class=\"m\">01</span>:42\nscaler.pt<span class=\"w\">                                                                                                                               </span><span class=\"m\">100</span>%<span class=\"w\"> </span><span class=\"m\">1383</span><span class=\"w\">     </span><span class=\"m\">4</span>.9KB/s<span class=\"w\">   </span><span class=\"m\">00</span>:00\nmodel.safetensors<span class=\"w\">                                                                                                                       </span><span class=\"m\">100</span>%<span class=\"w\">  </span>670MB<span class=\"w\">  </span><span class=\"m\">12</span>.7MB/s<span class=\"w\">   </span><span class=\"m\">00</span>:52\n</code></pre>\n</div>\n\n<p>...and here's the training chart:</p>\n\n<p><img alt=\"Training run on an 8x B200 with 160 GiB/GPU\" src=\"/post-assets/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud/training-run-8xb200m160.png\" title=\"Training run on an 8x B200 with 160 GiB/GPU\" /></p>\n\n<p>OK, so that's smoother than the last one -- no loss spikes.  Maybe the larger\nbatch size smoothed them?</p>\n\n<p>Let's think a bit about the cost of this train.</p>\n\n<h4 id=\"cost\">Cost</h4>\n\n<p>From Lambda Labs, we had that machine running for a little over 1h30m.  At US$39.92/hour,\nthe total cost was US$60.25.</p>\n\n<p>Yikes.  So, knocking off the 1h10 or so for the train, we have 20m to allow for --\nwhich matches up quite well to the 17 minutes of fiddling with batch sizes, and\nthen 3 minutes to download all of the files.</p>\n\n<p>If this blog post isn't going to cost significantly more than it needs to, we need\nto get that down.  Of the US$60.25, just\nover US$13 was spent on identifying the\nbatch size.  Only US$46.57 was spent on the train itself.</p>\n\n<p>We also did 11 validation runs as part of that; at a minute each, those cost US$7.32.\nSo, excluding validation, we're below US$40 for the train.</p>\n\n<h4 id=\"evals\">Evals</h4>\n\n<p>Now, let's run our tests.  First, the smoke test: we get this:</p>\n\n<pre><code>Every effort moves you to give something back. You will only make sure to check what you find on all other website for\n</code></pre>\n\n<p>\"...on all other website for...\" is a bit rubbish.  Still, on to the loss:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"go\">Loss against our test dataset: 3.771</span>\n</code></pre>\n</div>\n\n<p>That's in line with the training loss -- worse than the loss I got with the one trained on\nthe smaller machine, with its corresponding smaller batch size, but still better than\nany of our local trains.  Still interesting, though -- larger batches are not guaranteed\nto get bigger results.   More investigation needed there!</p>\n\n<p>On to the instruction fine-tuning test.  That gives us a score of 13.89 -- the\nworst that we've seen yet!</p>\n\n<p>I think I'll put together a full table including these results later; I want to try training on some\nother, differently sized machines first, and we can aggregate the results at the end.</p>\n\n<p>But before we do that, let's make some changes to the scripts to fix some of those\nQoL issues we encountered in that last train.</p>\n\n<h3 id=\"qol-fixes-to-the-script\">QoL fixes to the script</h3>\n\n<p>The first irritation was that it errored out saying that <code>datasets</code> was not\na directory when it didn't exist.  The script takes a datasets directory as one of\nits command-line options, and it's reasonable that it checks that it really is a\ndirectory (rather than, say, a file or a symlink):</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">datasets_dir</span> <span class=\"o\">=</span> <span class=\"n\">Path</span><span class=\"p\">(</span><span class=\"n\">datasets_dir_path</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">datasets_dir</span><span class=\"o\">.</span><span class=\"n\">is_dir</span><span class=\"p\">():</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">Exception</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">datasets_dir_path</span><span class=\"si\">}</span><span class=\"s2\"> is not a directory&quot;</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>...but if it doesn't exist, it might as well create it first.  Now, I could just put this\nbefore the <code>is_dir</code> check:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">datasets_dir</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">():</span>\n        <span class=\"n\">datasets_dir</span><span class=\"o\">.</span><span class=\"n\">mkdir</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>...but remember, this code is run by multiple processes -- so they could easily\ntrip over a race condition here.</p>\n\n<p>What I want is to have just one of them do this; I've deemed the rank 0 process\nthe \"special\" one for validation, printing the progress bar, and so on, so we may as\nwell treat it that way here.</p>\n\n<p>But -- there's a difference!  Rank zero is the one that should be printing stuff\nout, it's true.  And right now, we only have one node participating in this train.\nBut I do want to avoid simple errors that would make it hard to run multi-node in\nthe future.</p>\n\n<p>Now, if we have multiple nodes, then each one will have its own filesytem (unless we're\nusing NFS or something like that), so we'll\nneed a separate \"datasets\" directory for all of them.  What we want is to do these\nchecks on one process on each node.</p>\n\n<p>Usefully, we have the <code>local_rank</code> variable that is defined earlier in <code>main</code>,\nwhich is per-node.  Again, let's imagine we have two nodes with two GPUs each.\nNode 0 might be runnning the processes with global rank 0 and 1, and node 1 might\nhave global ranks 2 and 3.  On node 0, the processes would have local ranks 0 and 1\nrespectively, but on node 1, they'd also be local ranks 0 and 1.</p>\n\n<p>So, the full code becomes this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">datasets_dir</span> <span class=\"o\">=</span> <span class=\"n\">Path</span><span class=\"p\">(</span><span class=\"n\">datasets_dir_path</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">local_rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">datasets_dir</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">():</span>\n            <span class=\"n\">datasets_dir</span><span class=\"o\">.</span><span class=\"n\">mkdir</span><span class=\"p\">()</span>\n    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">barrier</span><span class=\"p\">()</span>\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">datasets_dir</span><span class=\"o\">.</span><span class=\"n\">is_dir</span><span class=\"p\">():</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">Exception</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">datasets_dir_path</span><span class=\"si\">}</span><span class=\"s2\"> is not a directory&quot;</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Note the barrier; we don't want the other processes to check whether <code>datasets_dir</code> is a\ndirectory until the local rank 0 process has had a chance to create it.</p>\n\n<p>(Of course, if we were running this on a setup where all of the nodes shared a filesystem,\nit wouldn't work -- in that case we'd want to use the global rank that we can get from\n<code>dist.get_rank()</code> instead.  But we can burn that bridge if we ever come to it ;-)</p>\n\n<p>Phew, that was a bit more work than I expected!  But it sets us up nicely for the\nnext QoL fix on my to-do list.</p>\n\n<p>I don't like the fact that every process downloaded the whole dataset.  The\n<code>huggingface_hub.snapshot_download</code> actually handled it pretty gracefully -- none\nof the processes tripped over any of the others.  Indeed, it looks like there was\nsome kind of global queueing going on, so they downloaded it one after the other.</p>\n\n<p>But it did take time -- maybe a minute or two in total, and with the clock ticking\non that ~US$40/hour machine, that felt a bit stress-inducing.</p>\n\n<p>So: I think it would be best to only do that from the rank 0 process as well.</p>\n\n<p>The code that downloads the dataset is just after the bit we've been looking at:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">dataset_dir</span> <span class=\"o\">=</span> <span class=\"n\">download_dataset</span><span class=\"p\">(</span><span class=\"n\">datasets_dir</span><span class=\"p\">,</span> <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;dataset&quot;</span><span class=\"p\">])</span>\n</code></pre>\n</div>\n\n<p>...and <code>download_dataset</code> looks like this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">download_dataset</span><span class=\"p\">(</span><span class=\"n\">datasets_dir</span><span class=\"p\">,</span> <span class=\"n\">dataset_name</span><span class=\"p\">):</span>\n    <span class=\"n\">download_path</span> <span class=\"o\">=</span> <span class=\"n\">snapshot_download</span><span class=\"p\">(</span>\n        <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">dataset_name</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">repo_type</span><span class=\"o\">=</span><span class=\"s2\">&quot;dataset&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">local_dir</span><span class=\"o\">=</span><span class=\"n\">datasets_dir</span> <span class=\"o\">/</span> <span class=\"n\">dataset_name</span><span class=\"p\">,</span>\n        <span class=\"n\">allow_patterns</span><span class=\"o\">=</span><span class=\"s2\">&quot;*&quot;</span>\n    <span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">Path</span><span class=\"p\">(</span><span class=\"n\">download_path</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Now, the docs for <a href=\"https://huggingface.co/docs/huggingface_hub/v1.2.2/en/package_reference/file_download#huggingface_hub.snapshot_download\"><code>huggingface_hub.snapshot_download</code></a>\nsay that the <code>local_dir</code> parameter is:</p>\n\n<blockquote>\n  <p>If provided, the downloaded files will be placed under this directory.</p>\n</blockquote>\n\n<p>...and the return value is this:</p>\n\n<blockquote>\n  <p>Returns</p>\n  \n  <p><code>str</code> or list of <code>DryRunFileInfo</code></p>\n  \n  <ul>\n  <li>If <code>dry_run=False</code>: Local snapshot path.</li>\n  <li>If <code>dry_run=True</code>: A list of DryRunFileInfo objects containing download information.</li>\n  </ul>\n</blockquote>\n\n<p>We happen to be passing in a <code>Path</code> object for <code>local_dir</code>, and we're not in <code>dry_run</code> mode -- it\ndefaults to <code>False</code>.  So all we're doing by returning that <code>download_path</code> wrapped in\na <code>Path</code> object is a slightly indirect way of returning the path <code>datasets_dir / dataset_name</code>\nthat we're passing in as <code>local_dir</code>.</p>\n\n<p>For tidiness, I really want to gate the call to <code>download_dataset</code> in <code>main</code> with\nthe same rank stuff as we did for the directory creation.  So, let's change the\nsetup so that <code>download_dataset</code> takes the path to the directory where we want this\nspecific dataset to be, not the generic \"all datasets\" directory.  And given that\nwe're now passing this specific path into the function, we don't need to return it:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">download_dataset</span><span class=\"p\">(</span><span class=\"n\">dataset_dir</span><span class=\"p\">,</span> <span class=\"n\">dataset_name</span><span class=\"p\">):</span>\n    <span class=\"n\">snapshot_download</span><span class=\"p\">(</span>\n        <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">dataset_name</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">repo_type</span><span class=\"o\">=</span><span class=\"s2\">&quot;dataset&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">local_dir</span><span class=\"o\">=</span><span class=\"n\">dataset_dir</span><span class=\"p\">,</span>\n        <span class=\"n\">allow_patterns</span><span class=\"o\">=</span><span class=\"s2\">&quot;*&quot;</span>\n    <span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Now it's just a wrapper around a single call to <code>snapshot_download</code>, which I'm not\nentirely sure about (it's a code smell that I'm probably creating an unnecessary\nlevel of abstraction) but I think I'm happiest leaving it that way for now, as it does\nhide away a bit of messiness in the HF hub API. <sup class=\"footnote-ref\" id=\"fnref-3\"><a href=\"#fn-3\">3</a></sup></p>\n\n<p>That means that we can now combine the directory-checking logic that we fixed above\nwith download-on-local-rank-zero-only code like this:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>    <span class=\"n\">datasets_dir</span> <span class=\"o\">=</span> <span class=\"n\">Path</span><span class=\"p\">(</span><span class=\"n\">datasets_dir_path</span><span class=\"p\">)</span>\n    <span class=\"n\">dataset_name</span> <span class=\"o\">=</span> <span class=\"n\">train_conf</span><span class=\"p\">[</span><span class=\"s2\">&quot;dataset&quot;</span><span class=\"p\">]</span>\n    <span class=\"n\">dataset_dir</span> <span class=\"o\">=</span> <span class=\"n\">datasets_dir</span> <span class=\"o\">/</span> <span class=\"n\">dataset_name</span>\n    <span class=\"k\">if</span> <span class=\"n\">local_rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">datasets_dir</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">():</span>\n            <span class=\"n\">datasets_dir</span><span class=\"o\">.</span><span class=\"n\">mkdir</span><span class=\"p\">()</span>\n        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">datasets_dir</span><span class=\"o\">.</span><span class=\"n\">is_dir</span><span class=\"p\">():</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">Exception</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">datasets_dir_path</span><span class=\"si\">}</span><span class=\"s2\"> is not a directory&quot;</span><span class=\"p\">)</span>\n        <span class=\"n\">download_dataset</span><span class=\"p\">(</span><span class=\"n\">dataset_dir</span><span class=\"p\">,</span> <span class=\"n\">dataset_name</span><span class=\"p\">)</span>\n    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">barrier</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p><a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/6851ce9b403204477758faa2e14f99061fc25504/ddp_train.py\">Here's the updated code</a> with those fixes.</p>\n\n<p>Now, let's move on to validation.  I'm increasingly of the opinion that the validation\nsteps are just adding on to the cost without much in the way of benefit.</p>\n\n<p>Additionally,\nthe validation is taking a different amount of time for each batch size, and\nhappen a different number of times in each train -- remember,\nit's <code>validation_batches</code> batches every <code>validation_interval</code> global steps, and\nthe batch size varies based on the micro-batch size, which is different for different\namounts of GPU VRAM, and the total number of global steps in a train <em>also</em> varies\nbased on the size of each batch.</p>\n\n<p>So that means that if we want to compare apples to apples in any final comparison\nof the time and money cost of training models on different kinds of Lambda Labs machines,\nwe'll want to exclude the validation cost -- once we've settled on a machine type,\nwe're going to want to fine-tune the validation size for that in much more detail than I\nhave to date, assuming we don't drop it entirely.</p>\n\n<p>However: I'm loath to make such a fundamental change halfway through this comparison.\nIt's tightly coupled to the checkpointing code, and the charting code, and so on.\nSo I think that for this post, I'm just going to keep it there, and keep track of how\nmuch time (roughly) we're spending on each validation step for each train, so that\nwe can remove it and get a \"pure\" train-time only comparison between the different\nkinds of machines.</p>\n\n<p>It's not pretty, but I think it's better than changing horses mid-stream.</p>\n\n<p>On the other hand, the validation is a real pain when doing the binary chop to find out\nthe maximum micro-batch size for our VRAM before\nwe start the training run.  That's because we have to wait for one validation to run before\nwe get into the full training loop, which makes it slower.   On top of that, having\nto do a manual binary chop is a PITA.</p>\n\n<p>What I think would be a true QoL improvement for the future trains is something that\ndoes the binary chop for us, using a dummy training loop.  We run it once on each\nnew machine type, get a micro-batch size to plug into our training parameters, and\nthen let it rip,</p>\n\n<p>This will re-use so much of the code from the training script <code>ddp_train</code> that I\nthink it actually is just an alternative way of running it.</p>\n\n<p>After a bit of hacking, I came up with <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/d2a7b694ff217a4ca0ade8267776a87816c58825/ddp_train.py\">this updated code</a>\n-- the <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/d2a7b694ff217a4ca0ade8267776a87816c58825\">diff</a> is a bit\nhairy, but essentially:</p>\n\n<ul>\n<li>I updated the <code>train</code> function so that it takes flags to\ntell it whether or not to do validation (default true) and an optional maximum\nnumber of steps, which is <code>None</code> by default.  With those default values, it does\nexactly the same as before, of course.</li>\n<li>I created a <code>load_datasets_and_train</code> function, which does all of the dataset-loading\nstuff that the original <code>main</code> function did, and then calls <code>train</code> with a\n<code>DDP</code>-wrapped model.  So that maintains the current flow.</li>\n<li>Next, I added a <code>--find-max-microbatch-size</code> flag to the script; if that's not\nset, it just calls <code>load_datasets_and_train</code>.</li>\n<li>However, if it is set, it instead calls a new <code>binary_chop_batch_sizes</code> function,\nwhich determines the largest batch size we can fit onto the current hardware\nfor the current run, and (on the rank 0 process only, to avoid log spam),\nprints it out.</li>\n<li><code>binary_chop_batch_sizes</code> does what it says on the tin; it confirms that we can\ntrain with batch size of 1, and that we can't with batch size 70 (chosen because\nthe limit was 64 on that massive B200 machine), then chops between them to find\nthe largest batch size that doesn't OOM.</li>\n<li>It uses <code>check_batch_size_works</code> for that -- that just constructs a dataset with\nthe appropriate batch size, then runs a three-step train with no validation\nto see if it raises an OOM.  PyTorch rather messily just raises a generic\n<code>RuntimeError</code> for those, but we can look inside the exception's message to\nsee if it is an OOM.</li>\n</ul>\n\n<p>That takes just over six seconds to find the correct batch size on my local machine;\nwith multiple GPUs, I expect it will be slower (there's a spinup overhead to start\nall of the per-GPU processes), but I'm sure it won't be as bad\nas the manual binary chops with validation that I was doing, and will be less\nerror-prone.</p>\n\n<p>Right!  We've done some QoL stuff, let's try another machine size on Lambda Labs :-)</p>\n\n<h3 id=\"training-on-an-8x-h100-with-80-gib-per-gpu-using-sxm5\">Training on an 8x H100 with 80 GiB per GPU, using SXM5</h3>\n\n<p>These are the machines that Andrej Karpathy is recommending for training nanochat,\nso let's see how we do with them.  They cost US$23.92/hour; let's see how it works\nout.</p>\n\n<h4 id=\"the-train-2\">The train</h4>\n\n<p>Here are the steps:</p>\n\n<ol>\n<li>Create the <code>8xh100m80</code> run file, commit and push.</li>\n<li>Spin up the machine.  On it:</li>\n<li>Clone the repo</li>\n<li><code>setup_lambda.sh</code></li>\n<li><code>uv sync</code></li>\n</ol>\n\n<p>Now let's download our dataset and find our micro-batch size:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@192-222-52-220:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>torchrun<span class=\"w\"> </span>--nproc_per_node<span class=\"o\">=</span><span class=\"m\">8</span><span class=\"w\"> </span>ddp_train.py<span class=\"w\"> </span>8xh100m80<span class=\"w\"> </span>datasets<span class=\"w\"> </span>-f\n...\nMax<span class=\"w\"> </span>microbatch<span class=\"w\"> </span>size<span class=\"w\"> </span>was<span class=\"w\"> </span><span class=\"m\">27</span>\n</code></pre>\n</div>\n\n<p>That took less than a minute to run -- nice!  Now we can put that micro-batch\nsize in <code>train.json</code>.  It does seem a little small -- after all, we could fit\na batch of 64 into 160 GiB -- but I'll do some analysis later.</p>\n\n<p>Actually, before we kick off the train, let's see how long all of the preparatory\nsteps took to run before we can do that -- not just the micro-batch-size script, but\nalso the installation of the dependencies, the clone, and any overhead from boot\ntime etc:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@192-222-52-220:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>uptime\n<span class=\"w\"> </span><span class=\"m\">22</span>:37:19<span class=\"w\"> </span>up<span class=\"w\"> </span><span class=\"m\">5</span><span class=\"w\"> </span>min,<span class=\"w\">  </span><span class=\"m\">2</span><span class=\"w\"> </span>users,<span class=\"w\">  </span>load<span class=\"w\"> </span>average:<span class=\"w\"> </span><span class=\"m\">1</span>.38,<span class=\"w\"> </span><span class=\"m\">1</span>.51,<span class=\"w\"> </span><span class=\"m\">0</span>.69\n</code></pre>\n</div>\n\n<p>Five minutes total.  Not bad.</p>\n\n<p>Let's start the train:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@192-222-52-220:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>torchrun<span class=\"w\"> </span>--nproc_per_node<span class=\"o\">=</span><span class=\"m\">8</span><span class=\"w\"> </span>ddp_train.py<span class=\"w\"> </span>8xh100m80<span class=\"w\"> </span>datasets\n</code></pre>\n</div>\n\n<p>The initial validation run took 38 seconds, and then we started off.  At 4m37s in,\nwe get the first real validation run; at that point, it's running at 493k tokens/second.</p>\n\n<p>Eventually, it finishes, having taken about 1h50 including all of the validations.</p>\n\n<pre><code>Training complete in 6,650.197 seconds\nTokens seen: 3,260,252,160\nThroughput: 490,249 tokens/second\nFinal train loss: 4.091\nFinal val loss: 3.729\n</code></pre>\n\n<p>Here's the training chart:</p>\n\n<p><img alt=\"Training run on an 8x H100 with 80 GiB/GPU\" src=\"/post-assets/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud/training-run-8xh100m80.png\" title=\"Training run on an 8x H100 with 80 GiB/GPU\" /></p>\n\n<p>Two things stand out here:</p>\n\n<ol>\n<li>We had two nasty loss spikes.</li>\n<li>As a result of the second of those, the best iteration as per validation\nloss is not the last one.</li>\n</ol>\n\n<p>Further evidence that gradient clipping is likely to be an excellent addition to\nour training loop!  It's also worth noting that the train loss spikes at the same\ntime as the validation loss, so getting rid of the latter would still allow us\nto get a \"best\" checkpoint to compare with the latest at the end of the train.</p>\n\n<h4 id=\"cost-2\">Cost</h4>\n\n<p>The machine was up and running for 2h9m, costing US$23.92/hour, for a total cost of\nUS$51.47.</p>\n\n<p>The train took 6,650.197 seconds, so about 1h50m.  Allowing for five minutes setup\ntime, that's 1h55m accounted for.  There's an extra 14m there -- that was because\ndownloading those two checkpoints to my machine took quite a long time due to local\nnetwork issues.  Might want to look into ways to avoid that later.</p>\n\n<p>And for later cost-accounting purposes, we should note that it took 38 seconds or\nso for each validation run, and we can see on the chart that there were 24 of them.</p>\n\n<h4 id=\"evals-2\">Evals</h4>\n\n<p>So, firstly, let's give our two models -- the best one and the latest one -- a smoke test:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xh100m80/model.json<span class=\"w\"> </span>runs/8xh100m80/checkpoints/best/model.safetensors\nEvery<span class=\"w\"> </span>effort<span class=\"w\"> </span>moves<span class=\"w\"> </span>you<span class=\"w\"> </span>forward,<span class=\"w\"> </span>and<span class=\"w\"> </span>you<span class=\"w\"> </span>will<span class=\"w\"> </span>not<span class=\"w\"> </span>regret<span class=\"w\"> </span>it.\nBut<span class=\"w\"> </span><span class=\"k\">if</span><span class=\"w\"> </span>something<span class=\"w\"> </span>comes<span class=\"w\"> </span><span class=\"k\">in</span>,<span class=\"w\"> </span>you<span class=\"w\"> </span>may<span class=\"w\"> </span>ask<span class=\"w\"> </span>the\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_smoke.py<span class=\"w\"> </span>runs/8xh100m80/model.json<span class=\"w\"> </span>runs/8xh100m80/checkpoints/latest/model.safetensors\nEvery<span class=\"w\"> </span>effort<span class=\"w\"> </span>moves<span class=\"w\"> </span>you<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>the<span class=\"w\"> </span>future<span class=\"p\">;</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>many<span class=\"w\"> </span>cases<span class=\"w\"> </span>you<span class=\"w\"> </span>can<span class=\"err\">'</span>t<span class=\"w\"> </span>afford<span class=\"w\"> </span>the<span class=\"w\"> </span>most<span class=\"w\"> </span>costly<span class=\"w\"> </span>replacement.&lt;<span class=\"p\">|</span>endoftext<span class=\"p\">|</span>&gt;The<span class=\"w\"> </span>following<span class=\"w\"> </span>list\n</code></pre>\n</div>\n\n<p>Both of those look OK!  Now let's try the loss test.  I started running it, but\nwhen it started downloading the dataset, I realised that it needed\n<a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/bd4f14cd1e323434561b35edebe4bce71c5e4b37\">updating to allow for the changes I made to <code>download_dataset</code></a> -- ooops!\nThat done, let's give it a run for both of our models:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>giles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets<span class=\"w\"> </span>runs/8xh100m80/model.json<span class=\"w\"> </span>runs/8xh100m80/checkpoints/best/model.safetensors\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">810</span>.61it/s<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">0</span>.00B<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00,<span class=\"w\"> </span>?B/s<span class=\"o\">]</span><span class=\"w\">                                                                                                            </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>\n<span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3200</span>/3200<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">04</span>:54&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.88it/s<span class=\"o\">]</span>\nLoss<span class=\"w\"> </span>against<span class=\"w\"> </span>our<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>dataset:<span class=\"w\"> </span><span class=\"m\">3</span>.725\ngiles@perry:~/Dev/ddp-base-model-from-scratch<span class=\"w\"> </span><span class=\"o\">(</span>main<span class=\"o\">)</span>$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>test_loss.py<span class=\"w\"> </span>datasets<span class=\"w\"> </span>runs/8xh100m80/model.json<span class=\"w\"> </span>runs/8xh100m80/checkpoints/latest/model.safetensors\nFetching<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"w\"> </span>files:<span class=\"w\"> </span><span class=\"m\">100</span>%<span class=\"p\">|</span>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">4</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">1719</span>.15it/s<span class=\"o\">]</span>\nDownload<span class=\"w\"> </span>complete:<span class=\"w\"> </span>:<span class=\"w\"> </span><span class=\"m\">0</span>.00B<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00,<span class=\"w\"> </span>?B/s<span class=\"o\">]</span><span class=\"w\">                                                                                                            </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>/4<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;?,<span class=\"w\"> </span>?it/s<span class=\"o\">]</span>\n<span class=\"m\">100</span>%<span class=\"p\">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">3200</span>/3200<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">04</span>:53&lt;<span class=\"m\">00</span>:00,<span class=\"w\"> </span><span class=\"m\">10</span>.89it/s<span class=\"o\">]</span>\nLoss<span class=\"w\"> </span>against<span class=\"w\"> </span>our<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>dataset:<span class=\"w\"> </span><span class=\"m\">3</span>.734\n</code></pre>\n</div>\n\n<p>As you'd expect, the best checkpoint has somewhat better loss, at 3.725, than the\nlast one, with 3.734.  Once again, better than our local trains, but not quite as good\nas the result with the first cloud train on that 8x A100 40 GiB machine, which was 3.674.\nAgain, I'll put together a table comparing all of these results at the end.</p>\n\n<p>Does that make any real difference with the instruction fine-tune test?\nThe test prints a lot out, but the headline numbers:</p>\n\n<ul>\n<li>Best checkpoint: 4 epochs of fine-tuning, and a score of 11.98 -- another record low!  Amusingly, it confidently\nsaid \"The author of 'Pride and Prejudice' is Sarah Palin\".</li>\n<li>Latest checkpoint: 5 epochs of fine-tuning, and a rather good score of 17.91.</li>\n</ul>\n\n<p>So that was interesting!  However, I am getting ever less convinced that the IFT\ntest is a useful one; the randomness of the LLM-as-a-judge responses means that I\ndon't think it can be consistent.</p>\n\n<p>Perhaps a better way to do this would be to batch up all of the models, and then\ngive GPT5.1 answers from \"model A\", \"model B\", and so on all in one query, and then\nto ask it to give them scores all at the same time.  That would hopefully make things at least a bit more\nconsistent.  Something to ponder later, I think.</p>\n\n<p>In the meantime, one extra thing I wanted to dig into before going on to the last\ntrain for this post:</p>\n\n<h3 id=\"batch-size-scaling\">Batch size scaling</h3>\n\n<p>I mentioned that I thought that the batch size for that last run, 27, was a bit\nsmall considering that we'd managed to fit a size of 64 into the 160 GiB/GPU machine.\nBut after thinking about it for a bit, it occurs to me that during my\nexperiments doing fine-tuning, I came to the conclusion that\n<a href=\"/2024/08/fine-tuning-8\">memory use scaled linearly with batch size</a>, with a\nfixed amount per element in the batch (the activations for the model for that batch element),\nplus an overhead (the model itself, the optimiser, and perhaps other stuff).</p>\n\n<p>We have batch sizes for:</p>\n\n<ul>\n<li>24 GiB locally, which was 6</li>\n<li>40 GiB in the first train in this series, which was 13</li>\n<li>80 GiB in the last one, giving us 27</li>\n<li>160 GiB in the one on the huge machine, giving us 64</li>\n</ul>\n\n<p>Now, that is slightly messy data because each memory \"measurement\" is the size of\nthe card's VRAM, not the amount of VRAM we actually used -- there might have been anything from zero to\njust less than one extra batch element's worth of \"spare\" space -- but we can see\nwhat we get with a simple linear regression:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">xs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">13</span><span class=\"p\">,</span> <span class=\"mi\">27</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">])</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">ys</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">24</span><span class=\"p\">,</span> <span class=\"mi\">40</span><span class=\"p\">,</span> <span class=\"mi\">80</span><span class=\"p\">,</span> <span class=\"mi\">160</span><span class=\"p\">])</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">polyfit</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">,</span> <span class=\"n\">ys</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n<span class=\"mf\">2.3461346633416458</span> <span class=\"mf\">11.481296758104722</span>\n</code></pre>\n</div>\n\n<p>And if we plot that, we get this:</p>\n\n<p><img alt=\"Batch size vs GPU VRAM linear regression\" src=\"/post-assets/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud/micro-batch-size-scaling-chart.png\" title=\"Batch size vs GPU VRAM linear regression\" /></p>\n\n<p>Nice!  That fits really well.  So we have an overhead of about 11.5 GiB, then\nabout 2.35 GiB per batch element on top of that.</p>\n\n<p>That is, of course, somewhat sad news for anyone trying to repro this on a GPU with\n12 GiB -- looks like it would be <em>just</em> too small to even fit in a single-element\nbatch after the overhead :-(</p>\n\n<p>Anyway, that's been a bit of a side quest.   Let's try our last machine size for\nwhat has (once again) turned into a bit of a monster of a blog post...</p>\n\n<h3 id=\"training-on-an-8x-a100-with-80-gib-per-gpu-using-sxm4\">Training on an 8x A100 with 80 GiB per GPU, using SXM4</h3>\n\n<p>This is the same kind of instance as the first train in this post, except that\nit has double the VRAM per GPU.  Let's see what we can do with it.</p>\n\n<h4 id=\"the-train-3\">The train</h4>\n\n<p>Once again, we create the <code>8xa100m80</code> run file, commit and push, then spin up the machine.\nOn it, we clone the repo, run <code>setup_lambda.sh</code> then <code>uv sync</code>.</p>\n\n<p>Next, we can find our micro-batch size:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code>ubuntu@192-222-52-220:~/ddp-base-model-from-scratch$<span class=\"w\"> </span>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>torchrun<span class=\"w\"> </span>--nproc_per_node<span class=\"o\">=</span><span class=\"m\">8</span><span class=\"w\"> </span>ddp_train.py<span class=\"w\"> </span>8xa100m80<span class=\"w\"> </span>datasets<span class=\"w\"> </span>-f\n...\nMax<span class=\"w\"> </span>microbatch<span class=\"w\"> </span>size<span class=\"w\"> </span>was<span class=\"w\"> </span><span class=\"m\">28</span>\n</code></pre>\n</div>\n\n<p>Interesting, we managed to squeeze an extra one in compared to the H100's batch size of 27, despite having\nexactly the same amount of VRAM!   Not sure what might have caused that.</p>\n\n<p>It took 4 minutes to get to this point, so let's get that batch size into the config and kick off the run.  The initial validation takes 1m06s,\nwhich is consistent throughout the train.  The first real val run at 8m15s in, and the\nestimated train time is 2h35m, with a tokens-per-second of 286,188.</p>\n\n<p>At the end:</p>\n\n<pre><code>Training complete in 11,532.620 seconds\nTokens seen: 3,260,350,464\nThroughput: 282,707 tokens/second\nFinal train loss: 3.771\nFinal val loss: 3.723\n</code></pre>\n\n<p>Again, the latest and the best global steps are the same (despite some loss spikes):</p>\n\n<p><img alt=\"Training run on an 8x A100 with 80 GiB/GPU\" src=\"/post-assets/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud/training-run-8xa100m80.png\" title=\"Training run on an 8x A100 with 80 GiB/GPU\" /></p>\n\n<p>...so we just need to download that and shut down the machine.</p>\n\n<p>How much did that cost us?</p>\n\n<h4 id=\"cost-3\">Cost</h4>\n\n<p>The machine was running for 3h25m, costing US$14.32 / hour, for a total of\nUS$48.76.</p>\n\n<p>Our train took 11,532 seconds, which is 3h12m, and our setup took about 4 minutes --\nmaybe five including the time required to update the train config with the micro-batch size,\nso we have 7 minutes on top of that, which is about the amount of time it took to\ndownload the model.</p>\n\n<p>Let's run some evals!</p>\n\n<h4 id=\"evals-3\">Evals</h4>\n\n<p>Our smoke test gives us this:</p>\n\n<pre><code>Every effort moves you up the hill for a full day.\n“We don’t know anyone who looks\n</code></pre>\n\n<p>Coherent enough, I think!  Now the loss on our test dataset; it comes out as 3.730,\nso pretty similar to our other cloud trains, apart from the oddly-low one on the 40 GiB GPUs.</p>\n\n<p>Now let's see what GPT-5.1 thinks of the instruction fine-tuned version.  It only needs\ntwo epochs of fine-tuning, and believes that \"The author of 'Pride and Prejudice' is 'Pride and Prejudice'\",\nwhich is not promising, and gets a score in the same kind of range as the other models,\n11.71.</p>\n\n<p>So: we've trained four models on four different machine sizes.  Let's see how\nthey stack up against each other, against our locally-trained models, and the original\nOpenAI GPT-2 weights.</p>\n\n<h3 id=\"the-results\">The results</h3>\n\n<p>So, I've trained four of my 163M-parameter GPT-2 models, using almost exactly the same dataset\n-- the Chinchilla-optimal number of tokens, rounded up to make an even number of batches.\nI did this on four different multi-GPU machines on Lambda Labs:</p>\n\n<ul>\n<li>An 8x A100 40 GiB</li>\n<li>An 8x A100 80 GiB</li>\n<li>An 8x H100 80 GiB</li>\n<li>An 8x B200 160 GiB</li>\n</ul>\n\n<h4 id=\"evals-4\">Evals</h4>\n\n<p>I've done some evals on each of the models, so let's put those results together\nin one table -- results for the\ntrains in this blog post, alongside those for the original OpenAI GPT-2 weights, both\nsmall and medium, and for the models I got when training locally.</p>\n\n<p>For all models, I've provided:</p>\n\n<ul>\n<li>The loss on my test set.</li>\n<li>The results it got on an instruction fine-tune test based on Sebastian Raschka's.</li>\n<li>The global batch size (that is, for single GPU runs, just the batch size, but for\nthe multi-GPU ones, where each batch is made up of per-GPU micro-batches, the\nper-GPU batch size times the number of GPUs). <sup class=\"footnote-ref\" id=\"fnref-4\"><a href=\"#fn-4\">4</a></sup></li>\n</ul>\n\n<p>I've sorted the models\nin order of increasing loss on the test set -- so, the best model by that measure is first.</p>\n\n<table>\n<thead>\n<tr>\n  <th></th>\n  <th>Test loss</th>\n  <th>IFT score</th>\n  <th>Batch size</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>OpenAI weights: medium</td>\n  <td>3.231</td>\n  <td>38.52</td>\n  <td>512</td>\n</tr>\n<tr>\n  <td>OpenAI weights: small</td>\n  <td>3.500</td>\n  <td>22.98</td>\n  <td>512</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x A100 40 GiB</td>\n  <td>3.674</td>\n  <td>17.09</td>\n  <td>104</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x H100 80 GiB</td>\n  <td>3.725</td>\n  <td>11.98</td>\n  <td>216</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x A100 80 GiB</td>\n  <td>3.734</td>\n  <td>11.71</td>\n  <td>224</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x B200 160 GiB</td>\n  <td>3.771</td>\n  <td>13.89</td>\n  <td>512</td>\n</tr>\n<tr>\n  <td>Local FineWeb train</td>\n  <td>3.944</td>\n  <td>16.01</td>\n  <td>6</td>\n</tr>\n<tr>\n  <td>Local FineWeb-Edu extended train</td>\n  <td>4.135</td>\n  <td>14.44</td>\n  <td>6</td>\n</tr>\n<tr>\n  <td>Local FineWeb-Edu train</td>\n  <td>4.167</td>\n  <td>16.86</td>\n  <td>6</td>\n</tr>\n</tbody>\n</table>\n\n<p>The instruction fine-tune results are kind of all over the place, and I'll look into\nthat later <sup class=\"footnote-ref\" id=\"fnref-5\"><a href=\"#fn-5\">5</a></sup>.  For now, let's focus on the test loss.  We have a pretty clear pattern,\nwhere the local trains are grouped together at around 4.0, and the cloud trains at\naround 3.7.  For the local trains, as I noticed last time around, FineWeb is\ncounter-intuitively <em>better</em> than FineWeb-Edu.</p>\n\n<p>There are two interesting things about the cloud trains:</p>\n\n<ol>\n<li>They're all consistently better than the local ones.</li>\n<li>The one on the <em>smaller</em> machine is better than the ones on the larger ones;\nindeed, it looks like the larger the machine, the worse.</li>\n</ol>\n\n<p>I think that what we're seeing here is that larger batches are better, but only\nup to a point.  It's as if there's some kind of curve like this:</p>\n\n<p><img alt=\"Log batch size vs loss quadratic polynomial regression\" src=\"/post-assets/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud/batch-size-fit.png\" title=\"Log batch size vs loss quadratic polynomial regression\" /></p>\n\n<p>I got that by taking the log of the batch size, then asking NumPy to do a polynomial\nregression -- that is, work out <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>a</mi></mrow></math>, <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>b</mi></mrow></math> and <math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>c</mi></mrow></math> so that the formula</p>\n\n<math display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>y</mi><mo>&#x0003d;</mo><mi>a</mi><msup><mi>x</mi><mn>2</mn></msup><mo>&#x0002b;</mo><mi>b</mi><mi>x</mi><mo>&#x0002b;</mo><mi>c</mi></mrow></math>\n\n<p>...fits it as well as possible:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">xs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">104</span><span class=\"p\">,</span> <span class=\"mi\">216</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">])</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">ys</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">3.674</span><span class=\"p\">,</span> <span class=\"mf\">3.725</span><span class=\"p\">,</span> <span class=\"mf\">3.73</span><span class=\"p\">,</span> <span class=\"mf\">3.771</span><span class=\"p\">,</span> <span class=\"mf\">3.944</span><span class=\"p\">])</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">log_xs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">polyfit</span><span class=\"p\">(</span><span class=\"n\">log_xs</span><span class=\"p\">,</span> <span class=\"n\">ys</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">c</span>\n<span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float64</span><span class=\"p\">(</span><span class=\"mf\">0.03231264430524897</span><span class=\"p\">),</span>\n <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float64</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">0.2957154034594081</span><span class=\"p\">),</span>\n <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float64</span><span class=\"p\">(</span><span class=\"mf\">4.368745850428664</span><span class=\"p\">))</span>\n</code></pre>\n</div>\n\n<p>It's kind of interesting that it's such a good\nfit with such an ad-hoc formula!  We have a nice smooth curve hitting almost all\nof the points, and our optimal batch size looks like it's just a little below that\n104 we managed with the smaller cloud machine, at about 97.\nBut it's certainly not something that I'd like to\nread too much into.  Best to treat it as purely illustrative: \"it <em>might</em>\nbe something like this\".</p>\n\n<p>I think digging into that might be an interesting experiment at some later point.\nA bit of checking around the Internet (and a chat with ChatGPT) suggests that it's\nsomething people have looked into in some detail, unsurprisingly.  An\ninteresting point ChatGPT raised is that with our pretty much fixed \"budget\" of tokens --\nwe're always training on something close to the Chinchilla-optimal number -- then a larger\nbatch size means that we're doing fewer optimiser steps.</p>\n\n<p>Intuitively, that sounds like a problem.\nThe larger batches mean that each move across the loss landscape\nis \"better\", or at least more stable.  But we're doing fewer of those moves over the course\nof the train.  There's obviously a tension between those two.  You can imagine\na degenerate case where the batch is so large you can fit the entire run into\none iteration, so you do just one update of the parameters; that obviously wouldn’t work very well.</p>\n\n<p>Anyway, for the purposes of this post, let's flag it as interesting and move on.\nLet's take a look at costs.</p>\n\n<h4 id=\"costs-of-training-in-the-cloud\">Costs of training in the cloud</h4>\n\n<p>Here's another table for those -- for each cloud model, I've listed:</p>\n\n<ul>\n<li>How long the training run took.</li>\n<li>How much the machine cost per hour.</li>\n<li>How much the training run cost.</li>\n<li>How much of that was doing validation (which I'm now thinking is pointless on single-epoch trains like this).</li>\n<li>How much it would have cost, and how long it would have taken if it had been run without validation.</li>\n</ul>\n\n<table>\n<thead>\n<tr>\n  <th></th>\n  <th>Train time (s)</th>\n  <th>Cost/hour (USD)</th>\n  <th>Train cost (USD)</th>\n  <th>Val runs</th>\n  <th>Per-val time (s)</th>\n  <th>Total val time (s)</th>\n  <th>Val cost (USD)</th>\n  <th>Cost ex val (USD)</th>\n  <th>Time ex val (s)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>Cloud FineWeb, 8x A100 40 GiB</td>\n  <td>13,904</td>\n  <td>10.32</td>\n  <td>39.86</td>\n  <td>50</td>\n  <td>30</td>\n  <td>1,500</td>\n  <td>4.30</td>\n  <td>35.56</td>\n  <td>12,404</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x H100 80 GiB</td>\n  <td>6,650</td>\n  <td>23.92</td>\n  <td>44.19</td>\n  <td>24</td>\n  <td>38</td>\n  <td>912</td>\n  <td>6.06</td>\n  <td>38.13</td>\n  <td>5,738</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x A100 80 GiB</td>\n  <td>11,532</td>\n  <td>14.32</td>\n  <td>45.87</td>\n  <td>24</td>\n  <td>66</td>\n  <td>1,584</td>\n  <td>6.30</td>\n  <td>39.57</td>\n  <td>9,948</td>\n</tr>\n<tr>\n  <td>Cloud FineWeb, 8x B200 160 GiB</td>\n  <td>4,190</td>\n  <td>39.92</td>\n  <td>46.46</td>\n  <td>11</td>\n  <td>60</td>\n  <td>660</td>\n  <td>7.32</td>\n  <td>39.14</td>\n  <td>3,530</td>\n</tr>\n</tbody>\n</table>\n\n<p>What do these numbers tell us, given what we were trying to do here?</p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>Like I said at the start, this was a pretty expensive learning experience: I wound\nup spending US$215.16 on Lambda Labs instances over the course of putting this all together.\nBut it was worth it!</p>\n\n<p>At the start of this post (if you can remember so far back), I said I wanted to\nachieve two things:</p>\n\n<ol>\n<li>I wanted to learn how to change a simple single-GPU training loop to make it\nmulti-GPU.</li>\n</ol>\n\n<p>Success!</p>\n\n<ol start=\"2\">\n<li>Could I get the training time for a full base model down from 48 hours\nto something more manageable -- and, hopefully, not too expensive?</li>\n</ol>\n\n<p>Yes, absolutely.  The trains I did, if we exclude the validation time, each cost\nbetween US$35.56\nand US$39.14.  In time, also excluding validation, the slowest ran for about 3h25m,\nand the fastest just less than an hour.</p>\n\n<p>Now, in a future post I want to try making the changes that I listed\n<a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch#but-why-is-our-model-worse-than-openais\">at the end of my last post</a>\nto see if I can get the loss lower:</p>\n\n<ul>\n<li>Removing dropout</li>\n<li>Tweaking the learning rate (and maybe adding the warmup and cosine learning-rate decay stuff I've read about).</li>\n<li>Reverting the architectural differences between our model and the original GPT-2:\nreintroducing weight tying between the token embeddings and the final linear layer, and also bias in the attention weights.</li>\n<li>Trying full-fat 32-bit precision.</li>\n<li>Fixing the exploding gradients issue with gradient clipping.</li>\n</ul>\n\n<p>If I'm to do those, what I'll need to do is start with a baseline train on one particular\nsize of machine, and then try introducing each change separately to see what happens to loss.\nI'll want to use a fixed seed for random number generation, so that I start with the\nsame initial weights each time.</p>\n\n<p>Given what these experiments have already shown about loss -- that the smallest,\ncheapest machine has better loss than the other more expensive ones due to what I assume\nis the batch size -- then that\nactually feels like exactly the right machine to choose for this.  It does take a\nwhile to train anything, but three and a half hours is pretty acceptable, I think\n-- I can do a train or two per day.  An 8x\nA100 with 40 GiB VRAM per GPU is the way forward.</p>\n\n<p>So: next steps.  I want to:</p>\n\n<ul>\n<li>Dig in to the instruction fine-tuning tests a little more -- as I've\nsaid above, I'm not 100% happy with how comparable it really is between models,\nat least given how I've been running it so far.</li>\n<li>Upload the models we have to Hugging Face.  I have a new motherboard ready for my\nPC, and replacing the old one has a risk that I might mess up and break the NVMe drive\nI have them stored on.  I was holding off on this because it would mean sharing\nRaschka's GPT code, but having noticed that he's already licensed it all under\nthe Apache license, I can release them under the same one.</li>\n<li>Strip out the validation stuff.  We can use training loss to track our progress,\nand losing evals during the train will help keep the cost down.</li>\n<li>Finally, do the trains to see how each of the levers above affects loss.</li>\n</ul>\n\n<p>This is going to be fun.  Stay tuned!</p>\n\n<p><a href=\"/2026/01/llm-from-scratch-30-digging-into-llm-as-a-judge\">Here's a link to the next post in this series</a>.</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>I erroneously called this a \"mini-batch\" in earlier versions of this post and\nin the code -- fixed in <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/commit/7f50aa4d86ff81699bbf4bb7fbb1931e0461cc8a\">this commit</a>.\nThe code in this post reflects the correct terminology, but if you follow the\nlinks to the earlier versions you will, of course, see the mistaken name.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-1\" title=\"Jump back to footnote 1 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-2\">\n<p>Disregarding the \"grokking\" phenomenon where continued training after overfitting,\nin some cases, can apparently make it start generalising again.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-2\" title=\"Jump back to footnote 2 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-3\">\n<p>Of course, people <em>always</em> say that when they add on unnecessary levels of\nabstraction...&#160;<a class=\"footnoteBackLink\" href=\"#fnref-3\" title=\"Jump back to footnote 3 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-4\">\n<p>The <a href=\"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">GPT-2 paper</a> is\nannoyingly short on concrete numbers, but they do at least explicitly state\nthat they used a batch size of 512.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-4\" title=\"Jump back to footnote 4 in the text.\">&#8617;</a></p>\n</li>\n\n<li id=\"fn-5\">\n<p>To be strictly honest here, I've already dug into it, but adding a writeup of\nthat to this already absurdly long blog post felt like something adjacent to sadism.\nUpdate shortly.&#160;<a class=\"footnoteBackLink\" href=\"#fnref-5\" title=\"Jump back to footnote 5 in the text.\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
  "id": "/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud"
}