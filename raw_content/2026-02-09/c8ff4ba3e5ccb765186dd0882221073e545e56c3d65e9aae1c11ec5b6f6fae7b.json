{
  "title": "Writing an LLM from scratch, part 31 -- the models are now on Hugging Face",
  "link": "https://www.gilesthomas.com/2026/01/llm-from-scratch-31-models-on-hugging-face",
  "published": "Sat, 17 Jan 2026 19:45:00 +0000",
  "summary": "<p>As part of my <a href=\"/2025/11/llm-from-scratch-27-whats-left-and-whats-next\">\"extra credit\" projects</a> after finishing the main body of\n<a href=\"https://sebastianraschka.com/\">Sebastian Raschka</a>'s book\n\"<a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">Build a Large Language Model (from Scratch)</a>\",\nI've trained seven base models completely from scratch based on the book's GPT-2 code -- <a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">three locally</a>,\nand <a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud\">four in the cloud</a>.\nI plan to train more as I work on ways to improve the quality of the trained models,\nin the hope that I can get to something closer to the original OpenAI weights' loss\non my own hardware, or at least on something I can rent without breaking the bank.</p>\n\n<p>It makes sense to share these models somewhere, both so that other people can take\na look if they like, and also to build the knowledge of how to do it so that if\nI produce something more interesting in the future, I'll know how to share that\ntoo.</p>\n\n<p>Raschka's code is all released under the Apache v2 open source license, so I\ncan share my stuff under the same license without worrying about triggering any\nlegal issues.  So: I've put all of the models I've trained so far on Hugging Face\nunder that license,\nand made them reasonably HF-native (I'll explain what I mean by that later).</p>\n\n<p>From <a href=\"/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch\">the post where I trained the models locally</a>, we have:</p>\n\n<ul>\n<li><a href=\"https://huggingface.co/gpjt/1xrtx3090m24-fineweb\"><code>gpjt/1xrtx3090m24-fineweb</code></a> --\nthe first model in that post, trained on a roughly Chinchilla-optimal number of tokens\n(20x the number of parameters) from <a href=\"https://huggingface.co/datasets/HuggingFaceFW/fineweb\">FineWeb</a>.</li>\n<li><a href=\"https://huggingface.co/gpjt/1xrtx3090m24-fineweb-edu\"><code>gpjt/1xrtx3090m24-fineweb-edu</code></a> --\nthe second model, trained on the same number of tokens from <a href=\"https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\">FineWeb-Edu</a>.</li>\n<li><a href=\"https://huggingface.co/gpjt/1xrtx3090m24-fineweb-edu-2x\"><code>gpjt/1xrtx3090m24-fineweb-edu-2x</code></a> --\nthe third one, which is the <code>gpjt/1xrtx3090m24-fineweb-edu</code> model trained further on another\nroughly Chinchilla-optimal number of tokens from the same dataset.</li>\n</ul>\n\n<p>Then, from <a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud\">the post where I trained on a bunch of different kinds of machines on Lambda Labs</a>,\nfour models (with two checkpoints from one of them):</p>\n\n<ul>\n<li><a href=\"https://huggingface.co/gpjt/8xa100m40\"><code>gpjt/8xa100m40</code></a>\n-- trained on a 8x A100, 40 GiB/GPU machine.</li>\n<li><a href=\"https://huggingface.co/gpjt/8xb200m160\"><code>gpjt/8xb200m160</code></a>\n-- trained on a 8x B200, 160 GiB/GPU machine.</li>\n<li><a href=\"https://huggingface.co/gpjt/8xh100m80-best\"><code>gpjt/8xh100m80-best</code></a>\n-- trained on a 8x H100, 80 GiB/GPU machine.  The best validation loss for this train was\nnot in the last iteration, so this is the checkpoint with the best loss.</li>\n<li><a href=\"https://huggingface.co/gpjt/8xh100m80-latest\"><code>gpjt/8xh100m80-latest</code></a>\n-- this one is the final checkpoint from the one above.</li>\n<li><a href=\"https://huggingface.co/gpjt/8xa100m80\"><code>gpjt/8xa100m80</code></a>\n-- trained on a 8x A100, 80 GiB/GPU machine.</li>\n</ul>\n\n<p>You can see how they compare on my evals at the bottom of <a href=\"/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud\">this post</a>.</p>\n\n<p>I wanted to make them all usable within the Hugging Face ecosystem -- that is, I didn't\nwant to just dump a bunch of weights and code into repos there, but rather to have\nsomething that someone coming to them without much context could make sense of.\nLet's dig into that.</p>\n<p>Here's the code I've been using as a smoke test after training a model\nto make sure it's not complete garbage.  There's quite a lot of it.</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">json</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">math</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pathlib</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Path</span>\n\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">click</span>\n\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">tiktoken</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch</span>\n\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">safetensors.torch</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">load_file</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">gpt</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">GPTModel</span>\n\n\n<span class=\"nd\">@click</span><span class=\"o\">.</span><span class=\"n\">command</span><span class=\"p\">()</span>\n<span class=\"nd\">@click</span><span class=\"o\">.</span><span class=\"n\">argument</span><span class=\"p\">(</span><span class=\"s2\">&quot;model_config_path&quot;</span><span class=\"p\">)</span>\n<span class=\"nd\">@click</span><span class=\"o\">.</span><span class=\"n\">argument</span><span class=\"p\">(</span><span class=\"s2\">&quot;model_safetensors_path&quot;</span><span class=\"p\">)</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">main</span><span class=\"p\">(</span><span class=\"n\">model_config_path</span><span class=\"p\">,</span> <span class=\"n\">model_safetensors_path</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">Path</span><span class=\"p\">(</span><span class=\"n\">model_config_path</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">is_file</span><span class=\"p\">():</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">Exception</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Could not find model config at </span><span class=\"si\">{</span><span class=\"n\">model_config_path</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">model_config_path</span><span class=\"p\">,</span> <span class=\"s2\">&quot;r&quot;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">model_config</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">Path</span><span class=\"p\">(</span><span class=\"n\">model_safetensors_path</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">is_file</span><span class=\"p\">():</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">Exception</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Could not find model safetensors at </span><span class=\"si\">{</span><span class=\"n\">model_safetensors_path</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda&quot;</span> <span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">is_available</span><span class=\"p\">()</span> <span class=\"k\">else</span> <span class=\"s2\">&quot;cpu&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">GPTModel</span><span class=\"p\">(</span><span class=\"n\">model_config</span><span class=\"p\">)</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">load_state_dict</span><span class=\"p\">(</span><span class=\"n\">load_file</span><span class=\"p\">(</span><span class=\"n\">model_safetensors_path</span><span class=\"p\">))</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n\n    <span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">tiktoken</span><span class=\"o\">.</span><span class=\"n\">get_encoding</span><span class=\"p\">(</span><span class=\"s2\">&quot;gpt2&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"n\">input_text</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;Every effort moves you&quot;</span>\n    <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">input_text</span><span class=\"p\">)</span>\n\n    <span class=\"n\">num_tokens</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>\n    <span class=\"n\">temperature</span> <span class=\"o\">=</span> <span class=\"mf\">1.4</span>\n    <span class=\"n\">top_k</span> <span class=\"o\">=</span> <span class=\"mi\">25</span>\n    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n        <span class=\"k\">for</span> <span class=\"n\">ix</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_tokens</span><span class=\"p\">):</span>\n            <span class=\"n\">input_tensor</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span>\n                <span class=\"n\">tokens</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">long</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span>\n            <span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n            <span class=\"n\">output_tensor</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">)</span>\n            <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">output_tensor</span><span class=\"p\">[:,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n            <span class=\"n\">top_logits</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">topk</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">top_k</span><span class=\"p\">)</span>\n            <span class=\"n\">min_val</span> <span class=\"o\">=</span> <span class=\"n\">top_logits</span><span class=\"p\">[:,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n            <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span>\n                <span class=\"n\">logits</span> <span class=\"o\">&lt;</span> <span class=\"n\">min_val</span><span class=\"p\">,</span>\n                <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">inf</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">),</span>\n                <span class=\"n\">logits</span>\n            <span class=\"p\">)</span>\n            <span class=\"n\">logits</span> <span class=\"o\">/=</span> <span class=\"n\">temperature</span>\n            <span class=\"n\">probs</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n            <span class=\"n\">next_token</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">multinomial</span><span class=\"p\">(</span><span class=\"n\">probs</span><span class=\"p\">,</span> <span class=\"n\">num_samples</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n            <span class=\"n\">tokens</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">next_token</span><span class=\"p\">)</span>\n\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">))</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">main</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>That's a lot of faffing about to generate a continuation of <code>Every effort moves you</code>!\nDisregarding the boilerplate with the argument parsing and validating, we have to load up the model,\nload up the tokeniser, encode our prompt, and then do a bunch of rather arcane stuff <sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"#fn-1\">1</a></sup> to sample from the\nmodel to generate some tokens before we finally print out the result.</p>\n\n<p>With the HF <a href=\"https://huggingface.co/docs/transformers/en/index\">Transformers</a> library,\nthere are extra levels of abstraction that allow you to do things much more simply:</p>\n\n<div class=\"codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">transformers</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">pipeline</span>\n<span class=\"n\">pipe</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">(</span><span class=\"n\">task</span><span class=\"o\">=</span><span class=\"s2\">&quot;text-generation&quot;</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;gpjt/some-model-name&quot;</span><span class=\"p\">,</span> <span class=\"n\">trust_remote_code</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">pipe</span><span class=\"p\">(</span>\n    <span class=\"s2\">&quot;Every effort moves you&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">max_new_tokens</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span>\n    <span class=\"n\">do_sample</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">1.4</span><span class=\"p\">,</span>\n    <span class=\"n\">top_k</span><span class=\"o\">=</span><span class=\"mi\">25</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">out</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s2\">&quot;generated_text&quot;</span><span class=\"p\">])</span>\n</code></pre>\n</div>\n\n<p>...and I wanted what I published to work with that -- and, indeed to be trainable further using the\nassociated training library, like I did during my <a href=\"/2024/12/fine-tuning-10\">fine-tuning experiments</a>.</p>\n\n<p>I managed to get that all to work, but it was quite a lot more effort than I expected.\nBut at the end, both the pipeline code above, and the training code that you can see\nin <a href=\"https://github.com/gpjt/ddp-base-model-from-scratch/blob/main/hf_train.ipynb\">this notebook</a>\nworked fine.</p>\n\n<p>I'll write a follow-up blog post shortly about how to write the code to make a vanilla\nPyTorch model work within the Hugging Face ecosystem (probably not as part of\nthis LLM from scratch series, as it's a bit of a tangent).\nBut in the meantime, if you're using HF and want to take a look, have\nfun :-)  I've put all of the models in <a href=\"https://huggingface.co/collections/gpjt/llm-from-scratch\">a collection</a>.</p>\n\n<p>And <a href=\"/2026/02/llm-from-scratch-32a-interventions-baseline-model\">here's a link to the next post in this series</a>.</p>\n\n<p>Update: here's the separate <a href=\"/2026/01/custom-automodelforcausallm-frompretrained-models-on-hugging-face\">follow-up on how to upload custom models to Hugging Face</a>.</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>Of course, if you've been reading the posts in this series carefully I'm sure\nit's all as clear as day ;-)&#160;<a class=\"footnoteBackLink\" href=\"#fnref-1\" title=\"Jump back to footnote 1 in the text.\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
  "id": "/2026/01/llm-from-scratch-31-models-on-hugging-face"
}